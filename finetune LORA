import os
import warnings
import requests.packages.urllib3 as urllib3

import pandas as pd
import numpy as np
import openpyxl

import torch
import torch.nn as nn
torch.manual_seed(0)

from datasets import Dataset
from peft import LoraConfig, get_peft_model
import evaluate
import sacrebleu

from nltk.translate.bleu_score import sentence_bleu
from concurrent.futures import ThreadPoolExecutor, as_completed

from transformers import (
    T5ForConditionalGeneration,
    T5Tokenizer,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq,
    logging,
)

# ─── Setup & Environment ────────────────────────────────────────────────────────

# suppress warnings
urllib3.disable_warnings()
warnings.filterwarnings("ignore")
logging.set_verbosity_error()

# choose GPU
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ─── Load Base Model & Tokenizer ────────────────────────────────────────────────

BASE_CHECKPOINT = "/appdata/cortex/dev1/aptaiModels/madlad400-3b-mt"
base_model = T5ForConditionalGeneration.from_pretrained(BASE_CHECKPOINT).to(device)
tokenizer = T5Tokenizer.from_pretrained(BASE_CHECKPOINT)
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=base_model)

# ─── Prepare Datasets ──────────────────────────────────────────────────────────

def load_and_lowercase(path):
    df = pd.read_excel(path)
    return df.applymap(lambda x: x.lower() if isinstance(x, str) else x)

train_df = load_and_lowercase("/appdata/cortex/dev1/shob/refined_datasets/refined_train_df.xlsx")
val_df   = load_and_lowercase("/appdata/cortex/dev1/shob/refined_datasets/refined_validation_df.xlsx")

train_ds = Dataset.from_pandas(train_df)
val_ds   = Dataset.from_pandas(val_df)

max_length = 128
def preprocess_function(batch):
    inputs = batch["es"]
    targets = batch["en"]
    model_inputs = tokenizer(inputs, text_target=targets,
                             max_length=max_length, truncation=True)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets,
                           max_length=max_length, truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_train = train_ds.map(preprocess_function, batched=True)
tokenized_val   = val_ds.map(preprocess_function, batched=True)

# ─── Freeze Base, Enable PEFT (LoRA) ────────────────────────────────────────────

# freeze all base parameters
for p in base_model.parameters():
    p.requires_grad = False
    if p.ndim == 1:  # keep layernorm in full precision
        p.data = p.data.to(torch.float32)

base_model.gradient_checkpointing_enable()
base_model.enable_input_require_grads()

# ensure lm_head outputs float32
class CastOutputToFloat(nn.Sequential):
    def forward(self, x):
        return super().forward(x).to(torch.float32)
base_model.lm_head = CastOutputToFloat(base_model.lm_head)

# LoRA configuration
lora_config = LoraConfig(
    use_rslora=True,
    r=64,
    lora_alpha=64,
    target_modules=["q", "k", "v"],
    lora_dropout=0.0,
    bias="lora_only",
    task_type="SEQ_2_SEQ_LM"
)
model = get_peft_model(base_model, lora_config)

# sanity check: how many parameters are trainable?
def print_trainable_parameters(m):
    trainable, total = 0, 0
    for _, p in m.named_parameters():
        total += p.numel()
        if p.requires_grad:
            trainable += p.numel()
    print(f"Trainable: {trainable:,} / {total:,} "
          f"({100*trainable/total:.2f}%)")
print_trainable_parameters(model)

# ─── Metrics ───────────────────────────────────────────────────────────────────

def postprocess_text(preds, labels):
    preds = [p.strip() for p in preds]
    labels = [[l.strip()] for l in labels]
    return preds, labels

def get_transmetrics_single(t, r):
    bleu = sentence_bleu([r.split()], t.split(), weights=(0.75, 0.25, 0, 0))
    return {"BLEU": round(bleu, 2)}

def get_transmetrics_batch(preds, refs, workers=25):
    # flatten refs
    flat_refs = ([item for sub in refs for item in sub]
                 if isinstance(refs[0], list) else refs)
    preds_l = [p.lower() for p in preds]
    refs_l  = [r.lower() for r in flat_refs]
    pairs = zip(preds_l, refs_l)
    with ThreadPoolExecutor(max_workers=workers) as ex:
        futures = [ex.submit(get_transmetrics_single, p, r)
                   for p, r in pairs]
        results = [f.result() for f in as_completed(futures)]
    df = pd.DataFrame(results)
    return df.mean().round(2).to_dict()

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    # replace -100 & decode
    preds  = np.where(preds != -100, preds, tokenizer.pad_token_id)
    labels = np.where(labels!= -100, labels, tokenizer.pad_token_id)
    dec_preds  = tokenizer.batch_decode(preds,  skip_special_tokens=True)
    dec_labels = tokenizer.batch_decode(labels,skip_special_tokens=True)
    dec_preds, dec_labels = postprocess_text(dec_preds, dec_labels)
    return get_transmetrics_batch(dec_preds, dec_labels)

# ─── Training ──────────────────────────────────────────────────────────────────

batch_size = 16
model.generation_config.max_new_tokens = max_length
model.generation_config.max_length = max_length

training_args = Seq2SeqTrainingArguments(
    output_dir="./models/checkpoint_madlad400_finetuned_LORA_rank64alpha64best",
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size =batch_size,
    learning_rate=2e-4,
    weight_decay=0.01,
    save_total_limit=1,
    num_train_epochs=10,
    predict_with_generate=True,
    label_smoothing_factor=0.1,
    eval_steps=100,
    #evaluation_strategy="epoch",
)

trainer = Seq2SeqTrainer(
    model            = model,
    args             = training_args,
    train_dataset    = tokenized_train,
    eval_dataset     = tokenized_val,
    data_collator    = data_collator,
    tokenizer        = tokenizer,
    compute_metrics  = compute_metrics,
)

# kick off training!
trainer.train()

# ─── Inference on Test Set ─────────────────────────────────────────────────────

# load your newly-saved checkpoint for generation
FT_CHECKPOINT = "./models/checkpoint_madlad400_finetuned_LORA_rank64alpha64best"
ft_model     = T5ForConditionalGeneration.from_pretrained(FT_CHECKPOINT).to(device)
ft_tokenizer = T5Tokenizer.from_pretrained(FT_CHECKPOINT)

def base_madlad_translate(text: str) -> str:
    """Translate one Spanish string to English."""
    prefix = "<2en> "
    inp = prefix + text
    ids = ft_tokenizer(inp, return_tensors="pt").input_ids.to(device)
    out = ft_model.generate(input_ids=ids)
    return ft_tokenizer.decode(out[0], skip_special_tokens=True)

# read your test file, translate, and save
test_df = pd.read_excel("/appdata/cortex/dev1/shob/refined_datasets/refined_test_df.xlsx")
test_df = test_df.applymap(lambda x: x.lower() if isinstance(x, str) else x)

test_df["base_madlad400_translation"] = test_df["es"].apply(base_madlad_translate)

OUT_PATH = "/appdata/cortex/dev1/shob/refined_datasets/refinedOutput_rank64alpha64best.xlsx"
test_df.to_excel(OUT_PATH, index=False)

print("✅ Done. Inference written to:", OUT_PATH)
