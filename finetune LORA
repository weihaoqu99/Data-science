from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    # ── I/O & frequency ───────────────────────────────────────────────
    output_dir                = "./models/checkpoint_optuna1",
    save_total_limit          = 2,              # keep only the 2 best checkpoints
    evaluation_strategy       = "steps",        # or "epoch"
    eval_steps                = 100,            # run eval every 100 steps
    save_strategy             = "steps",
    save_steps                = 100,
    logging_strategy          = "steps",
    logging_steps             = 50,             # log training loss/metrics every 50 steps
    load_best_model_at_end    = True,           # after training, load the best checkpoint
    metric_for_best_model     = "bleu",         # pick BLEU or "chrf"
    greater_is_better         = True,
    # ── Optimization hyperparams ──────────────────────────────────────
    per_device_train_batch_size = batch_size,
    per_device_eval_batch_size  = batch_size,
    learning_rate             = 0.00194839,     # from Optuna
    weight_decay              = 0.02294,        # from Optuna
    num_train_epochs          = 7,              # from Optuna
    gradient_accumulation_steps = 2,            # simulate larger batch
    warmup_steps              = 500,            # or warmup_ratio=0.03
    lr_scheduler_type         = "cosine",       # ["linear","cosine","cosine_with_restarts",…]
    optim                     = "adamw_torch",  # or "adafactor"
    fp16                      = True,           # mixed‐precision
    # ── Generation‐time parameters ────────────────────────────────────
    predict_with_generate     = True,
    generation_max_length     = 128,
    generation_num_beams      = 4,              # beam‐search
    length_penalty            = 0.8,
    no_repeat_ngram_size      = 2,
    early_stopping            = True,
    # ── Smoothing / label / … ─────────────────────────────────────────
    label_smoothing_factor    = 0.1,
)