# 1) Make sure the 'audio' column never decodes
from datasets import Audio
raw_datasets = raw_datasets.cast_column("audio", Audio(decode=False))

print(raw_datasets["train"].features)  # should show Audio(..., decode=False)
# Access will now return a dict with a 'path' key; no decoding happens.


from pathlib import Path
import shutil

# ===== settings =====
OUT = Path("/appdata/cortex/dev1/export_split")  # where to save train/ and test/
PRESERVE_TREE = True                              # keep source subfolders
COMMON_ROOT = Path("/appdata/cortex/dev1/OrigAudio")  # used only if PRESERVE_TREE
# ====================

def _src_path(row):
    # works whether your dataset has 'audio_path' OR 'audio' (Audio(decode=False))
    return row.get("audio_path") or row["audio"]["path"]

def _save_split(split_name: str):
    out_dir = OUT / split_name
    out_dir.mkdir(parents=True, exist_ok=True)

    for row in raw_datasets[split_name]:
        src = Path(_src_path(row))
        if PRESERVE_TREE and COMMON_ROOT in src.parents:
            dst = out_dir / src.relative_to(COMMON_ROOT)   # keep subfolder structure
            dst.parent.mkdir(parents=True, exist_ok=True)
        else:
            dst = out_dir / src.name                       # flat folder; keep filename
        if not dst.exists():
            shutil.copy2(src, dst)

# run
_save_split("train")
_save_split("test")
print(f"Saved: {OUT / 'train'} and {OUT / 'test'}")


FOLDER = "/appdata/cortex/dev1/OrigAudio/one_particular_folder"  # or leave as "" to list all

def get_path(row):
    return row["audio_path"] if "audio_path" in row else row["audio"]["path"]

train_paths = [p for p in (get_path(r) for r in raw_datasets["train"]) if p.startswith(FOLDER)]
test_paths  = [p for p in (get_path(r) for r in raw_datasets["test"])  if p.startswith(FOLDER)]

print("=== TRAIN ==="); print("\n".join(train_paths) or "(none)")
print("\n=== TEST ===");  print("\n".join(test_paths)  or "(none)")



import os

FOLDER = "/appdata/cortex/dev1/OrigAudio/one_particular_folder"  # <- change me

def get_path(row):
    # works for either schema: {'audio_path': ...} OR {'audio': {'path': ...}}
    return row["audio_path"] if "audio_path" in row else row["audio"]["path"]

train_list = [get_path(r) for r in raw_datasets["train"] if get_path(r).startswith(FOLDER)]
test_list  = [get_path(r) for r in raw_datasets["test"]  if get_path(r).startswith(FOLDER)]

print("=== TRAIN ===")
print("\n".join(train_list) or "(none)")
print("\n=== TEST ===")
print("\n".join(test_list) or "(none)")





# 0) Build the base dataset called `raw`
from pathlib import Path
from datasets import Dataset
import glob, os, re

SRC = "/appdata/cortex/dev1/OrigAudio"   # <- your mp3 root

# list all mp3 paths
audio_files = sorted(set(
    glob.glob(f"{SRC}/**/*.mp3", recursive=True) +
    glob.glob(f"{SRC}/**/*.MP3", recursive=True)
))

# if you already have `transcripts`, keep it; else derive from filename:
def retrieve_transcript(p: str) -> str:
    b = os.path.basename(p)
    if b.lower().endswith(".mp3"): b = b[:-4]
    d = b.find("-")
    s = b[d+1:] if d != -1 else b
    s = s.replace("_"," ").replace("."," ").replace("dont","don't").replace("shell","she'll")
    return s.capitalize().replace(" i ", " I ")

transcripts = [retrieve_transcript(p) for p in audio_files]

# raw = single Dataset with paths + text (NOT datasets.Audio)
raw = Dataset.from_dict({"audio_path": audio_files, "sentence": transcripts}).with_format("python")
print(raw)  # sanity check




import os, re
from datasets import DatasetDict

# parse leading number like "001-" / "01-" / "139-"
def _lead_num(path: str):
    m = re.match(r"^(\d+)-", os.path.basename(path))
    return int(m.group(1)) if m else None

def _is_test(ex):
    n = _lead_num(ex["audio_path"])
    return (n is not None) and (1 <= n <= 139)

# split with filters (like your photo)
test_ds  = raw.filter(_is_test)
train_ds = raw.filter(lambda ex: not _is_test(ex))

raw_datasets = DatasetDict({"train": train_ds, "test": test_ds})
print(raw_datasets)  # sanity check
# DatasetDict({
#   train: Dataset({ num_rows: ... })
#   test:  Dataset({ num_rows: ... })
# })

# (re)vectorize exactly like you do later
vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets["train"].column_names,  # ['audio_path','sentence']
    num_proc=1,                                         # keep single process
    desc="Preparing dataset",
)


from pathlib import Path
import re, shutil

# Edit these:
SRC  = Path("/appdata/cortex/dev1/OrigAudio")
DEST = Path("/appdata/cortex/dev1/split_mp3")
MOVE = False   # True = move, False = copy

# Collect all MP3s (case-insensitive)
files = [p for p in SRC.rglob("*") if p.suffix.lower() == ".mp3"]

# Helper: parse leading number like "01-" / "139-"
def lead_num(name: str):
    m = re.match(r"^(\d+)-", name)   # start-of-name, digits, dash
    return int(m.group(1)) if m else None

# Select test = names with leading number in [1..139]
test = [p for p in files if (lambda n: n is not None and 1 <= n <= 139)(lead_num(p.name))]
test_set = set(test)
train = [p for p in files if p not in test_set]

# Make dirs
(TRAIN := DEST / "train").mkdir(parents=True, exist_ok=True)
(TEST  := DEST / "test").mkdir(parents=True, exist_ok=True)

# Copy/move, preserving subfolders
op = shutil.move if MOVE else shutil.copy2
for name, subset in (("test", test), ("train", train)):
    out_root = DEST / name
    for src in subset:
        dst = out_root / src.relative_to(SRC)
        dst.parent.mkdir(parents=True, exist_ok=True)
        op(src, dst)

print(f"Done. train={len(train)}  test={len(test)}  -> {DEST}")




import os

TO_EXCLUDE = "03-shell-carp.mp3"

def keep_row(ex):
    return os.path.basename(ex["audio_path"]) != TO_EXCLUDE

# remove it from both splits
raw_datasets = raw_datasets.filter(keep_row)

# (re)vectorize
vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,
    desc="Preparing dataset",
)

# then:
trainer.train()

from datasets import Dataset

raw = Dataset.from_dict({
    "audio_path": audio_files,      # <-- string paths, not Audio(...)
    "sentence":   transcripts
})
raw_datasets = raw.train_test_split(test_size=0.15, seed=91).with_format("python")

# pip install miniaudio numpy
import numpy as np, miniaudio

def load_mp3(path: str, target_sr: int = 16000):
    sb = miniaudio.decode_file(path)                      # floats in [-1,1]
    sr_native = getattr(sb, "sample_rate", 16000)
    ch        = getattr(sb, "channels",    1)
    y = np.asarray(sb.samples, dtype=np.float32)
    if ch > 1:
        y = y.reshape(-1, ch).mean(axis=1)               # stereo -> mono
    if sr_native != target_sr:
        n_new = int(len(y) * target_sr / sr_native)
        y = np.interp(
            np.linspace(0, len(y), n_new, endpoint=False),
            np.arange(len(y), dtype=np.float32),
            y.astype(np.float32)
        ).astype(np.float32)
    return y, target_sr



from transformers import WhisperProcessor

# make sure whisper_loc is set to your model folder or HF id
processor = WhisperProcessor.from_pretrained(whisper_loc, language="English", task="transcribe")

def prepare_dataset(batch):
    path = batch["audio_path"]                      # <-- use string path
    y, _ = load_mp3(path, target_sr=16000)          # miniaudio decoder
    batch["input_features"] = processor.feature_extractor(
        y, sampling_rate=16000
    ).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,                                     # keep single process
    desc="Preparing dataset",
)

import numpy as np, miniaudio

def load_mp3_miniaudio(path: str, target_sr: int = 16000):
    sb = miniaudio.decode_file(path)   # decodes to floats in [-1, 1]
    # be robust to field names
    sr_native = getattr(sb, "sample_rate", getattr(sb, "samplerate", 16000))
    ch        = getattr(sb, "channels",    getattr(sb, "nchannels", 1))
    y = np.asarray(sb.samples, dtype=np.float32)

    # stereo -> mono
    if ch and ch > 1:
        y = y.reshape(-1, ch).mean(axis=1)

    # simple resample to 16k if needed
    if sr_native != target_sr:
        n_new = int(len(y) * target_sr / sr_native)
        y = np.interp(
            np.linspace(0, len(y), n_new, endpoint=False),
            np.arange(len(y), dtype=np.float32),
            y.astype(np.float32)
        ).astype(np.float32)

    return y, target_sr

from transformers import WhisperProcessor

# make sure whisper_loc is defined earlier (local folder or HF id)
processor = WhisperProcessor.from_pretrained(whisper_loc, language="English", task="transcribe")

def prepare_dataset(batch):
    # We stored paths only, so read from disk here:
    path = batch["audio"]["path"]
    y, _ = load_mp3_miniaudio(path, target_sr=16000)

    batch["input_features"] = processor.feature_extractor(
        y, sampling_rate=16000
    ).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

# IMPORTANT: keep a single process so workers don’t re-import audio stacks
vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,                          # ← this avoids backend/import issues
    desc="Preparing dataset",
)




from datasets import Dataset, Audio

raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}).cast_column(
    "audio", Audio(decode=False)  # store only file paths, don't decode
)
raw_datasets = raw.train_test_split(test_size=0.15, seed=91).with_format("python")

import numpy as np, audioread, resampy

def load_mp3(path: str, target_sr: int = 16000):
    # Decode with audioread (FFmpeg backend)
    with audioread.audio_open(path) as f:
        sr_native = f.samplerate
        ch = f.channels
        pcm = b"".join(chunk for chunk in f)

    # 16-bit PCM -> float32 [-1, 1]
    y = np.frombuffer(pcm, dtype=np.int16).astype(np.float32) / 32768.0

    # stereo -> mono
    if ch and ch > 1:
        y = y.reshape(-1, ch).mean(axis=1)

    # resample if needed
    if sr_native != target_sr:
        y = resampy.resample(y, sr_native, target_sr)

    return y, target_sr

from transformers import WhisperProcessor

# make sure whisper_loc (path or HF id) is defined earlier
processor = WhisperProcessor.from_pretrained(whisper_loc, language="English", task="transcribe")

def prepare_dataset(batch):
    path = batch["audio"]["path"]          # Audio(decode=False) gives you a path
    y, _ = load_mp3(path, target_sr=16000)

    batch["input_features"] = processor.feature_extractor(
        y, sampling_rate=16000
    ).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

# IMPORTANT: keep single process so workers don’t re-import audio backends
vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,
    desc="Preparing dataset",
)
bad = []

def prepare_dataset_safe(batch):
    path = batch["audio"]["path"]
    try:
        y, _ = load_mp3(path, target_sr=16000)
    except Exception as e:
        bad.append((path, str(e)))
        return {"_bad": True}
    return {
        "input_features": processor.feature_extractor(y, sampling_rate=16000).input_features[0],
        "labels": processor.tokenizer(batch["sentence"]).input_ids
    }

vectorized_datasets = raw_datasets.map(
    prepare_dataset_safe,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,
    desc="Preparing dataset",
).filter(lambda ex: not ex.get("_bad", False))

print("Skipped files:", len(bad))
for p, err in bad[:5]:
    print(" -", p, "->", err)




import jiwer

def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # replace -100 with pad_token_id
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id

    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    # compute WER with jiwer
    wer = jiwer.wer(label_str, pred_str) * 100

    return {"wer": wer}



print(vectorized_datasets)
print(vectorized_datasets["train"][0].keys())   # 


should show: dict_keys(['input_features','labels'])




from datasets.features.audio import set_audio_backend
set_audio_backend("librosa")   # put this BEFORE you call .cast_column(...)


pip install librosa audioread
# (ffmpeg is recommended so audioread can decode mp3)

from datasets import Dataset, Audio
from datasets.features.audio import set_audio_backend
set_audio_backend("librosa")

raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}) \
             .cast_column("audio", Audio(sampling_rate=16_000))



# ---- put this at the very top of the notebook ----
import sys, types
sf_stub = types.ModuleType("soundfile")
sf_stub.__spec__ = None
sf_stub.libsndfile_version = "0.0.0"  # forces features that require libsndfile to be disabled
sys.modules["soundfile"] = sf_stub

# deps purely in Python
%pip install -q librosa audioread

# now it's safe to import datasets without crashing
from datasets import Dataset, Audio
import librosa

raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}) \
             .cast_column("audio", Audio(decode=False))
raw_datasets = raw.train_test_split(test_size=0.15, seed=91)

def prepare_dataset(batch):
    path = batch["audio"]["path"]
    array, sr = librosa.load(path, sr=16000, mono=True)
    batch["input_features"] = processor.feature_extractor(array, sampling_rate=16000).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=8
)



# ---- put this at the very top of the notebook, before any datasets import ----
import sys, types, importlib, importlib.machinery, importlib.util

# create a stub module with a valid spec and the attribute datasets probes
sf_stub = types.ModuleType("soundfile")
sf_stub.__dict__.update({
    "__spec__": importlib.machinery.ModuleSpec("soundfile", loader=None),
    "libsndfile_version": "0.0.0",   # make the probe evaluate safely
})
sys.modules["soundfile"] = sf_stub

# ensure find_spec("soundfile") returns a spec instead of raising
_orig_find_spec = importlib.util.find_spec
def _fake_find_spec(name, package=None):
    if name == "soundfile":
        return importlib.machinery.ModuleSpec("soundfile", loader=None)
    return _orig_find_spec(name, package)
importlib.util.find_spec = _fake_find_spec

# optional: use librosa for decoding later
%pip install -q librosa audioread

from datasets import Dataset, Audio
import librosa

# build dataset WITHOUT decoding; decode with librosa in map()
raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}).cast_column(
    "audio", Audio(decode=False)
)

def prepare_dataset(batch):
    path = batch["audio"]["path"]
    array, sr = librosa.load(path, sr=16000, mono=True)
    batch["input_features"] = processor.feature_extractor(array, sampling_rate=16000).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch



# --- put this in the VERY FIRST cell, before any `datasets` import ---
import sys, types, importlib, importlib.machinery, importlib.util

# create a stub that satisfies datasets' probes
sf_stub = types.ModuleType("soundfile")
sf_stub.__spec__ = importlib.machinery.ModuleSpec("soundfile", loader=None)
sf_stub.__libsndfile_version__ = "0.0.0"   # <-- exact name expected by datasets
sf_stub.libsndfile_version = "0.0.0"       # (some versions check this too)
sys.modules["soundfile"] = sf_stub

# ensure find_spec("soundfile") returns a spec
_orig_find_spec = importlib.util.find_spec
def _fake_find_spec(name, package=None):
    if name == "soundfile":
        return importlib.machinery.ModuleSpec("soundfile", loader=None)
    return _orig_find_spec(name, package)
importlib.util.find_spec = _fake_find_spec

# optional pure-Python decoders
%pip install -q librosa audioread

from datasets import Dataset, Audio
import librosa

# build WITHOUT decoding; decode in map()
raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}) \
             .cast_column("audio", Audio(decode=False))

def prepare_dataset(batch):
    path = batch["audio"]["path"]
    array, _ = librosa.load(path, sr=16000, mono=True)
    batch["input_features"] = processor.feature_extractor(array, sampling_rate=16000).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch


# conda (recommended)
conda install -c conda-forge libsndfile ffmpeg soundfile librosa audioread
# --- STUB SOUNDFILE (no libsndfile needed) ---
import sys, types, importlib, importlib.machinery, importlib.util

# if a previous bad stub exists, remove it
sys.modules.pop("soundfile", None)

# make a minimal stub with the attributes datasets probes
sf_stub = types.ModuleType("soundfile")
sf_stub.__spec__ = importlib.machinery.ModuleSpec("soundfile", loader=None)
sf_stub.__file__ = "<soundfile-stub>"
sf_stub.__package__ = ""
sf_stub.__libsndfile_version__ = "0.0.0"   # <— EXACT NAME REQUIRED
sf_stub.libsndfile_version = "0.0.0"       # some versions also check this
sys.modules["soundfile"] = sf_stub

# make importlib.util.find_spec('soundfile') return a spec (not None)
_orig_find_spec = importlib.util.find_spec
def _safe_find_spec(name, package=None):
    if name == "soundfile":
        return sf_stub.__spec__
    return _orig_find_spec(name, package)
importlib.util.find_spec = _safe_find_spec

# --- SOUNDFILE STUB: must run before importing `datasets` ---
import sys, types, importlib, importlib.machinery, importlib.util

# remove any previously loaded soundfile
sys.modules.pop("soundfile", None)

# create stub with the exact attributes `datasets` expects
_sf = types.ModuleType("soundfile")
_sf.__spec__ = importlib.machinery.ModuleSpec("soundfile", loader=None)
_sf.__file__ = "<soundfile-stub>"
_sf.__package__ = ""
_sf.__libsndfile_version__ = "0.0.0"   # EXACT attribute name required
_sf.libsndfile_version = "0.0.0"       # some versions also check this
sys.modules["soundfile"] = _sf

# keep original find_spec, then patch safely (to avoid recursion)
_orig_find_spec = importlib.util.find_spec
def _safe_find_spec(name, package=None):
    if name == "soundfile":
        return _sf.__spec__
    return _orig_find_spec(name, package)
importlib.util.find_spec = _safe_find_spec

# quick self-check
import importlib as _il
mod = _il.import_module("soundfile")
assert hasattr(mod, "__libsndfile_version__"), "stub missing __libsndfile_version__"





raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}) \
             .cast_column("audio", Audio(decode=False))

raw_datasets = raw.train_test_split(test_size=0.15, seed=91)

def prepare_dataset(batch):
    path = batch["audio"]["path"]
    array, _ = librosa.load(path, sr=16000, mono=True)
    batch["input_features"] = processor.feature_extractor(array, sampling_rate=16000).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch



from datasets import Dataset, Audio
import librosa

# 1) Build WITHOUT decoding (very important)
raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}).cast_column(
    "audio", Audio(decode=False)
)

# 2) Split and force plain Python format so printing doesn’t decode
raw_datasets = raw.train_test_split(test_size=0.15, seed=91)
raw_datasets = raw_datasets.with_format("python")


# Don’t do: [r["sentence"] for r in raw_datasets["test"]]  # triggers decoding
print(raw_datasets["test"]["sentence"][:10])                # SAFE (column access)

# or drop the audio column before looking at rows:
print(raw_datasets["test"].remove_columns("audio")[:3])     # SAFE

def prepare_dataset(batch):
    path = batch["audio"]["path"]                     # path only; not decoded
    array, _ = librosa.load(path, sr=16000, mono=True)
    batch["input_features"] = processor.feature_extractor(
        array, sampling_rate=16000
    ).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=8
)

print(vectorized_datasets["train"][0].keys())  # should be dict_keys(['input_features','labels'])





import numpy as np, audioread, resampy

def load_audio_ar(path: str, sr: int = 16000) -> tuple[np.ndarray, int]:
    """Decode audio via audioread (FFmpeg/MediaFoundation/CoreAudio),
    convert to mono float32 in [-1,1], and resample to `sr`."""
    with audioread.audio_open(path) as f:
        sr_native = f.samplerate
        ch = f.channels

        # concatenate raw PCM chunks
        pcm = b"".join(b for b in f)

    # audioread gives 16-bit PCM for MP3/WAV → int16 → float32
    y = np.frombuffer(pcm, dtype=np.int16).astype(np.float32) / 32768.0

    # deinterleave → mono
    if ch > 1:
        y = y.reshape(-1, ch).mean(axis=1)

    # resample if needed
    if sr_native != sr:
        y = resampy.resample(y, sr_native, sr)

    return y, sr

def prepare_dataset(batch):
    path = batch["audio"]["path"]                 # path only; not decoded
    array, _ = load_audio_ar(path, sr=16000)      # <-- use audioread loader
    batch["input_features"] = processor.feature_extractor(
        array, sampling_rate=16000
    ).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

from datasets import Dataset, Audio
raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}).cast_column(
    "audio", Audio(decode=False)
)
raw_datasets = raw.train_test_split(test_size=0.15, seed=91).with_format("python")

# Also avoid multiprocessing (workers re-import audio stacks):
vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,                      # <- important
    desc="Preparing dataset"
)



import subprocess; subprocess.run(["ffmpeg","-version"])



# --- pick ONE loader ---

# If you have FFmpeg installed (recommended):
import numpy as np, audioread, resampy

def load_audio(path, sr=16000):
    with audioread.audio_open(path) as f:
        sr_native = f.samplerate
        ch = f.channels
        pcm = b"".join(b for b in f)
    y = np.frombuffer(pcm, dtype=np.int16).astype(np.float32) / 32768.0
    if ch > 1:
        y = y.reshape(-1, ch).mean(axis=1)
    if sr_native != sr:
        y = resampy.resample(y, sr_native, sr)
    return y, sr

# If you don't have FFmpeg but files are WAV only, you can use librosa:
# import librosa
# def load_audio(path, sr=16000):
#     y, _ = librosa.load(path, sr=sr, mono=True)
#     return y, sr

def prepare_dataset(batch):
    # dataset was built with Audio(decode=False), so we only have a file path
    path = batch["audio"]["path"]
    array, _ = load_audio(path, sr=16000)

    batch["input_features"] = processor.feature_extractor(
        array, sampling_rate=16000
    ).input_features[0]

    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

# IMPORTANT: run mapping in a single process so no audio stack gets re-imported
vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,                       # <-- not 8
    desc="Preparing dataset",
)

def prepare_dataset(batch):
    a = batch["audio"]
    if isinstance(a, dict) and "array" in a and a["array"] is not None:
        array = a["array"]
        sr = a.get("sampling_rate", 16000)
    else:
        path = a["path"] if isinstance(a, dict) else a
        array, sr = load_audio(path, sr=16000)

    batch["input_features"] = processor.feature_extractor(array, sampling_rate=16000).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch