# --- Live table of Step | Train Loss | Val Loss | WER during trainer.train() ---

from transformers.trainer_callback import TrainerCallback

class LiveMetricsTable(TrainerCallback):
    def __init__(self):
        # step -> {'step':int, 'train':float|None, 'val':float|None, 'wer':float|None}
        self.rows = {}
        self._printed_header = False

    def _row(self, step):
        step = int(step)
        if step not in self.rows:
            self.rows[step] = {"step": step, "train": None, "val": None, "wer": None}
        return self.rows[step]

    def _print_header_once(self):
        if not self._printed_header:
            hdr = f"{'Step':>8} | {'Train Loss':>12} | {'Val Loss':>10} | {'WER %':>7}"
            print(hdr)
            print("-" * len(hdr))
            self._printed_header = True

    # logs from training (contains 'loss')
    def on_log(self, args, state, control, logs=None, **kwargs):
        if not logs or "loss" not in logs:
            return
        row = self._row(state.global_step)
        row["train"] = float(logs["loss"])

    # metrics from evaluation (contains eval_loss and eval_wer/wer)
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if not metrics:
            return
        row = self._row(state.global_step)

        # HF sometimes prefixes with "eval_"
        if "eval_loss" in metrics:
            row["val"] = float(metrics["eval_loss"])
        elif "loss" in metrics:
            row["val"] = float(metrics["loss"])

        if "eval_wer" in metrics:
            row["wer"] = float(metrics["eval_wer"])
        elif "wer" in metrics:
            row["wer"] = float(metrics["wer"])

        # print the row when we have eval results
        self._print_header_once()
        tl = f"{row['train']:.4f}" if row["train"] is not None else "-"
        vl = f"{row['val']:.4f}"   if row["val"]   is not None else "-"
        wr = f"{row['wer']:.2f}"   if row["wer"]   is not None else "-"
        print(f"{row['step']:8d} | {tl:>12} | {vl:>10} | {wr:>7}")

# attach the callback
live = LiveMetricsTable()
trainer.add_callback(live)

# IMPORTANT: make sure eval actually runs during training
# (you likely already set these in your TrainingArguments)
# evaluation_strategy="steps", eval_steps=50,
# logging_strategy="steps", logging_steps=25,
# predict_with_generate=True, remove_unused_columns=False

# train and still get a final table after training
trainer.train()

# optional: print a final consolidated table sorted by step
if live.rows:
    print("\nFinal summary:")
    hdr = f"{'Step':>8} | {'Train Loss':>12} | {'Val Loss':>10} | {'WER %':>7}"
    print(hdr); print("-"*len(hdr))
    for s in sorted(live.rows):
        r = live.rows[s]
        tl = f"{r['train']:.4f}" if r["train"] is not None else "-"
        vl = f"{r['val']:.4f}"   if r["val"]   is not None else "-"
        wr = f"{r['wer']:.2f}"   if r["wer"]   is not None else "-"
        print(f"{s:8d} | {tl:>12} | {vl:>10} | {wr:>7}")









from transformers import TrainerCallback
from tabulate import tabulate

class LossWERCallback(TrainerCallback):
    def __init__(self):
        self.history = []

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is None:
            return
        # Capture training loss if available
        if "loss" in logs:
            self.history.append({
                "Step": state.global_step,
                "Training Loss": round(logs["loss"], 4),
                "Validation Loss": None,
                "WER": None
            })

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics is None:
            return
        # Update last record with eval metrics
        if self.history:
            self.history[-1]["Validation Loss"] = round(metrics.get("eval_loss", 0), 4)
            self.history[-1]["WER"] = round(metrics.get("eval_wer", 0), 4)
        
        # Print table
        print(tabulate(self.history, headers="keys"))

# Attach the callback to your trainer
trainer.add_callback(LossWERCallback())

# Train
trainer.train()






# ==== SETTINGS (edit these) ===============================================
SRC_DIR     = "/appdata/cortex/dev1/OrigAudio"   # where your MP3 files live
WHISPER_DIR = "/appdata/cortex/dev1/whisper_copy/distil-large-v2"  # or any HF id
SEED        = 91
# ==========================================================================

import os, glob, re, subprocess, numpy as np, random
from pathlib import Path
from datasets import Dataset, DatasetDict
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer
from dataclasses import dataclass
from typing import Any, Dict, List, Union
import torch

# ---------- 1) collect files + lightweight transcripts ----------
files = sorted(set(glob.glob(f"{SRC_DIR}/**/*.mp3", recursive=True) +
                   glob.glob(f"{SRC_DIR}/**/*.MP3", recursive=True)))
if not files:
    raise RuntimeError(f"No MP3 files found under {SRC_DIR}")

def fn_to_text(p: str) -> str:
    b = os.path.basename(p)
    if b.lower().endswith(".mp3"): b = b[:-4]
    # strip a leading "NNN-" if present
    i = b.find("-")
    s = b[i+1:] if i != -1 else b
    s = (s.replace("_"," ").replace("."," ").strip()
         .replace("dont","don't").replace("shell","she'll"))
    s = s[0:1].upper() + s[1:] if s else s
    s = s.replace(" i ", " I ")
    return s

transcripts = [fn_to_text(p) for p in files]
raw = Dataset.from_dict({"audio_path": files, "sentence": transcripts}).with_format("python")

# ---------- 2) split 80/20 ----------
raw_datasets = raw.train_test_split(test_size=0.20, seed=SEED)
print(raw_datasets)

# ---------- 3) tiny audio loader (NO librosa/soundfile) ----------
def load_audio_ffmpeg(path: str, target_sr: int = 16000):
    """
    Decodes with the ffmpeg CLI (no Python libs). Make sure 'ffmpeg' is on PATH.
    """
    cmd = ["ffmpeg","-v","error","-i",path,"-f","f32le","-ac","1","-ar",str(target_sr),"pipe:1"]
    out = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
    y = np.frombuffer(out.stdout, dtype=np.float32)
    return y, target_sr

# ---------- 4) processor + model ----------
processor = WhisperProcessor.from_pretrained(WHISPER_DIR, language="English", task="transcribe")
model = WhisperForConditionalGeneration.from_pretrained(WHISPER_DIR)
model.config.forced_decoder_ids = None
model.config.suppress_tokens = []
model.config.use_cache = False  # better with gradient checkpointing
model.config.language = "english"
model.config.task = "transcribe"

# ---------- 5) map: turn path -> input_features + labels ----------
def prepare_example(batch):
    y, _ = load_audio_ffmpeg(batch["audio_path"], target_sr=16000)
    batch["input_features"] = processor.feature_extractor(y, sampling_rate=16000).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

# keep single process to avoid re-importing audio stack
vectorized = raw_datasets.map(
    prepare_example,
    remove_columns=raw_datasets["train"].column_names,  # ['audio_path','sentence']
    num_proc=1,
    desc="Preparing dataset",
)

# ---------- 6) simple data collator ----------
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    decoder_start_token_id: int

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_features": f["input_features"]} for f in features]
        label_features = [{"input_ids": f["labels"]} for f in features]

        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
        # drop BOS if your tokenizer adds it; Whisper usually adds BOS later in generate()
        if labels.shape[1] > 0 and (labels[:, 0] == self.decoder_start_token_id).all().item():
            labels = labels[:, 1:]
        batch["labels"] = labels
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(
    processor=processor,
    decoder_start_token_id=model.config.decoder_start_token_id
)

# ---------- 7) pure-Python WER (no 'evaluate') ----------
def _levenshtein(a, b):
    m, n = len(a), len(b)
    dp = list(range(n + 1))
    for i in range(1, m + 1):
        prev, dp[0] = dp[0], i
        for j in range(1, n + 1):
            prev, dp[j] = dp[j], min(dp[j] + 1, dp[j-1] + 1, prev + (a[i-1] != b[j-1]))
    return dp[n]

def compute_metrics(pred):
    preds = pred.predictions[0] if isinstance(pred.predictions, tuple) else pred.predictions
    labels = pred.label_ids.copy()
    pad_id = processor.tokenizer.pad_token_id
    labels[labels == -100] = pad_id

    hyp = processor.tokenizer.batch_decode(preds,  skip_special_tokens=True)
    ref = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)

    total_err, total_words = 0, 0
    for r, h in zip(ref, hyp):
        r_w, h_w = r.strip().split(), h.strip().split()
        total_err  += _levenshtein(r_w, h_w)
        total_words += max(1, len(r_w))
    wer = 100.0 * total_err / total_words if total_words else 0.0
    return {"wer": wer}

# ---------- 8) training args (ensure eval runs & prints) ----------
args = Seq2SeqTrainingArguments(
    output_dir="./asr_out",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=1e-5,
    max_steps=1000,                    # or set num_train_epochs=3
    fp16=torch.cuda.is_available(),
    evaluation_strategy="steps",       # <-- will evaluate during training
    eval_steps=50,
    logging_strategy="steps",
    logging_steps=25,
    predict_with_generate=True,        # needed for decoding
    generation_max_length=225,
    remove_unused_columns=False,
    metric_for_best_model="wer",
    greater_is_better=False,
    load_best_model_at_end=True,
    report_to=[],                      # no wandb/tensorboard needed
    seed=SEED,
)

# ---------- 9) trainer + print-eval callback ----------
from transformers.trainer_callback import TrainerCallback
class PrintEvalCallback(TrainerCallback):
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics:
            nice = {k: round(float(v), 4) for k, v in metrics.items() if isinstance(v, (int, float))}
            print(f"\n[Eval @ step {state.global_step}] {nice}")

trainer = Seq2SeqTrainer(
    model=model,
    args=args,
    train_dataset=vectorized["train"],
    eval_dataset=vectorized["test"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor,
)
trainer.add_callback(PrintEvalCallback())

# ---------- 10) train + final evaluate ----------
print("train size:", len(vectorized["train"]), " test size:", len(vectorized["test"]))
trainer.train()
final_metrics = trainer.evaluate()
print("\nFinal evaluation:", {k: round(float(v), 4) for k, v in final_metrics.items()})




from transformers.trainer_callback import TrainerCallback

class PrintEvalCallback(TrainerCallback):
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics:
            nice = {k: round(float(v), 4) for k, v in metrics.items() if isinstance(v, (int, float))}
            print(f"\n[Eval @ step {state.global_step}] {nice}")

trainer.add_callback(PrintEvalCallback())

# --- pure-Python word error rate (no dependencies) ---
def _levenshtein(a, b):
    # a, b: lists of tokens
    m, n = len(a), len(b)
    dp = list(range(n + 1))
    for i in range(1, m + 1):
        prev, dp[0] = dp[0], i
        for j in range(1, n + 1):
            prev, dp[j] = dp[j], min(
                dp[j] + 1,            # deletion
                dp[j-1] + 1,          # insertion
                prev + (a[i-1] != b[j-1])  # substitution
            )
    return dp[n]

def compute_metrics(pred):
    # handle HF returning tuple for logits
    preds = pred.predictions[0] if isinstance(pred.predictions, tuple) else pred.predictions
    labels = pred.label_ids

    # replace -100 with pad so we can decode labels
    pad_id = processor.tokenizer.pad_token_id
    labels = labels.copy()
    labels[labels == -100] = pad_id

    pred_str  = processor.tokenizer.batch_decode(preds,  skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)

    # compute corpus WER
    total_err, total_words = 0, 0
    for ref, hyp in zip(label_str, pred_str):
        ref_words = ref.strip().split()
        hyp_words = hyp.strip().split()
        total_err  += _levenshtein(ref_words, hyp_words)
        total_words += max(1, len(ref_words))
    wer = 100.0 * total_err / total_words
    return {"wer": wer}





# 1) Make sure the 'audio' column never decodes
from datasets import Audio
raw_datasets = raw_datasets.cast_column("audio", Audio(decode=False))

print(raw_datasets["train"].features)  # should show Audio(..., decode=False)
# Access will now return a dict with a 'path' key; no decoding happens.



def to_path(ex):
    return {"audio_path": ex["audio"]["path"]}

raw_datasets = raw_datasets.map(to_path, remove_columns=["audio"])
print(raw_datasets["train"].features)  # 'audio_path': Value('string')



from pathlib import Path
import shutil

# ===== settings =====
OUT = Path("/appdata/cortex/dev1/export_split")  # where to save train/ and test/
PRESERVE_TREE = True                              # keep source subfolders
COMMON_ROOT = Path("/appdata/cortex/dev1/OrigAudio")  # used only if PRESERVE_TREE
# ====================

def _src_path(row):
    # works whether your dataset has 'audio_path' OR 'audio' (Audio(decode=False))
    return row.get("audio_path") or row["audio"]["path"]

def _save_split(split_name: str):
    out_dir = OUT / split_name
    out_dir.mkdir(parents=True, exist_ok=True)

    for row in raw_datasets[split_name]:
        src = Path(_src_path(row))
        if PRESERVE_TREE and COMMON_ROOT in src.parents:
            dst = out_dir / src.relative_to(COMMON_ROOT)   # keep subfolder structure
            dst.parent.mkdir(parents=True, exist_ok=True)
        else:
            dst = out_dir / src.name                       # flat folder; keep filename
        if not dst.exists():
            shutil.copy2(src, dst)

# run
_save_split("train")
_save_split("test")
print(f"Saved: {OUT / 'train'} and {OUT / 'test'}")


FOLDER = "/appdata/cortex/dev1/OrigAudio/one_particular_folder"  # or leave as "" to list all

def get_path(row):
    return row["audio_path"] if "audio_path" in row else row["audio"]["path"]

train_paths = [p for p in (get_path(r) for r in raw_datasets["train"]) if p.startswith(FOLDER)]
test_paths  = [p for p in (get_path(r) for r in raw_datasets["test"])  if p.startswith(FOLDER)]

print("=== TRAIN ==="); print("\n".join(train_paths) or "(none)")
print("\n=== TEST ===");  print("\n".join(test_paths)  or "(none)")



import os

FOLDER = "/appdata/cortex/dev1/OrigAudio/one_particular_folder"  # <- change me

def get_path(row):
    # works for either schema: {'audio_path': ...} OR {'audio': {'path': ...}}
    return row["audio_path"] if "audio_path" in row else row["audio"]["path"]

train_list = [get_path(r) for r in raw_datasets["train"] if get_path(r).startswith(FOLDER)]
test_list  = [get_path(r) for r in raw_datasets["test"]  if get_path(r).startswith(FOLDER)]

print("=== TRAIN ===")
print("\n".join(train_list) or "(none)")
print("\n=== TEST ===")
print("\n".join(test_list) or "(none)")





# 0) Build the base dataset called `raw`
from pathlib import Path
from datasets import Dataset
import glob, os, re

SRC = "/appdata/cortex/dev1/OrigAudio"   # <- your mp3 root

# list all mp3 paths
audio_files = sorted(set(
    glob.glob(f"{SRC}/**/*.mp3", recursive=True) +
    glob.glob(f"{SRC}/**/*.MP3", recursive=True)
))

# if you already have `transcripts`, keep it; else derive from filename:
def retrieve_transcript(p: str) -> str:
    b = os.path.basename(p)
    if b.lower().endswith(".mp3"): b = b[:-4]
    d = b.find("-")
    s = b[d+1:] if d != -1 else b
    s = s.replace("_"," ").replace("."," ").replace("dont","don't").replace("shell","she'll")
    return s.capitalize().replace(" i ", " I ")

transcripts = [retrieve_transcript(p) for p in audio_files]

# raw = single Dataset with paths + text (NOT datasets.Audio)
raw = Dataset.from_dict({"audio_path": audio_files, "sentence": transcripts}).with_format("python")
print(raw)  # sanity check




import os, re
from datasets import DatasetDict

# parse leading number like "001-" / "01-" / "139-"
def _lead_num(path: str):
    m = re.match(r"^(\d+)-", os.path.basename(path))
    return int(m.group(1)) if m else None

def _is_test(ex):
    n = _lead_num(ex["audio_path"])
    return (n is not None) and (1 <= n <= 139)

# split with filters (like your photo)
test_ds  = raw.filter(_is_test)
train_ds = raw.filter(lambda ex: not _is_test(ex))

raw_datasets = DatasetDict({"train": train_ds, "test": test_ds})
print(raw_datasets)  # sanity check
# DatasetDict({
#   train: Dataset({ num_rows: ... })
#   test:  Dataset({ num_rows: ... })
# })

# (re)vectorize exactly like you do later
vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets["train"].column_names,  # ['audio_path','sentence']
    num_proc=1,                                         # keep single process
    desc="Preparing dataset",
)


from pathlib import Path
import re, shutil

# Edit these:
SRC  = Path("/appdata/cortex/dev1/OrigAudio")
DEST = Path("/appdata/cortex/dev1/split_mp3")
MOVE = False   # True = move, False = copy

# Collect all MP3s (case-insensitive)
files = [p for p in SRC.rglob("*") if p.suffix.lower() == ".mp3"]

# Helper: parse leading number like "01-" / "139-"
def lead_num(name: str):
    m = re.match(r"^(\d+)-", name)   # start-of-name, digits, dash
    return int(m.group(1)) if m else None

# Select test = names with leading number in [1..139]
test = [p for p in files if (lambda n: n is not None and 1 <= n <= 139)(lead_num(p.name))]
test_set = set(test)
train = [p for p in files if p not in test_set]

# Make dirs
(TRAIN := DEST / "train").mkdir(parents=True, exist_ok=True)
(TEST  := DEST / "test").mkdir(parents=True, exist_ok=True)

# Copy/move, preserving subfolders
op = shutil.move if MOVE else shutil.copy2
for name, subset in (("test", test), ("train", train)):
    out_root = DEST / name
    for src in subset:
        dst = out_root / src.relative_to(SRC)
        dst.parent.mkdir(parents=True, exist_ok=True)
        op(src, dst)

print(f"Done. train={len(train)}  test={len(test)}  -> {DEST}")




import os

TO_EXCLUDE = "03-shell-carp.mp3"

def keep_row(ex):
    return os.path.basename(ex["audio_path"]) != TO_EXCLUDE

# remove it from both splits
raw_datasets = raw_datasets.filter(keep_row)

# (re)vectorize
vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,
    desc="Preparing dataset",
)

# then:
trainer.train()

from datasets import Dataset

raw = Dataset.from_dict({
    "audio_path": audio_files,      # <-- string paths, not Audio(...)
    "sentence":   transcripts
})
raw_datasets = raw.train_test_split(test_size=0.15, seed=91).with_format("python")

# pip install miniaudio numpy
import numpy as np, miniaudio

def load_mp3(path: str, target_sr: int = 16000):
    sb = miniaudio.decode_file(path)                      # floats in [-1,1]
    sr_native = getattr(sb, "sample_rate", 16000)
    ch        = getattr(sb, "channels",    1)
    y = np.asarray(sb.samples, dtype=np.float32)
    if ch > 1:
        y = y.reshape(-1, ch).mean(axis=1)               # stereo -> mono
    if sr_native != target_sr:
        n_new = int(len(y) * target_sr / sr_native)
        y = np.interp(
            np.linspace(0, len(y), n_new, endpoint=False),
            np.arange(len(y), dtype=np.float32),
            y.astype(np.float32)
        ).astype(np.float32)
    return y, target_sr



from transformers import WhisperProcessor

# make sure whisper_loc is set to your model folder or HF id
processor = WhisperProcessor.from_pretrained(whisper_loc, language="English", task="transcribe")

def prepare_dataset(batch):
    path = batch["audio_path"]                      # <-- use string path
    y, _ = load_mp3(path, target_sr=16000)          # miniaudio decoder
    batch["input_features"] = processor.feature_extractor(
        y, sampling_rate=16000
    ).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,                                     # keep single process
    desc="Preparing dataset",
)

import numpy as np, miniaudio

def load_mp3_miniaudio(path: str, target_sr: int = 16000):
    sb = miniaudio.decode_file(path)   # decodes to floats in [-1, 1]
    # be robust to field names
    sr_native = getattr(sb, "sample_rate", getattr(sb, "samplerate", 16000))
    ch        = getattr(sb, "channels",    getattr(sb, "nchannels", 1))
    y = np.asarray(sb.samples, dtype=np.float32)

    # stereo -> mono
    if ch and ch > 1:
        y = y.reshape(-1, ch).mean(axis=1)

    # simple resample to 16k if needed
    if sr_native != target_sr:
        n_new = int(len(y) * target_sr / sr_native)
        y = np.interp(
            np.linspace(0, len(y), n_new, endpoint=False),
            np.arange(len(y), dtype=np.float32),
            y.astype(np.float32)
        ).astype(np.float32)

    return y, target_sr

from transformers import WhisperProcessor

# make sure whisper_loc is defined earlier (local folder or HF id)
processor = WhisperProcessor.from_pretrained(whisper_loc, language="English", task="transcribe")

def prepare_dataset(batch):
    # We stored paths only, so read from disk here:
    path = batch["audio"]["path"]
    y, _ = load_mp3_miniaudio(path, target_sr=16000)

    batch["input_features"] = processor.feature_extractor(
        y, sampling_rate=16000
    ).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

# IMPORTANT: keep a single process so workers don’t re-import audio stacks
vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,                          # ← this avoids backend/import issues
    desc="Preparing dataset",
)




from datasets import Dataset, Audio

raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}).cast_column(
    "audio", Audio(decode=False)  # store only file paths, don't decode
)
raw_datasets = raw.train_test_split(test_size=0.15, seed=91).with_format("python")

import numpy as np, audioread, resampy

def load_mp3(path: str, target_sr: int = 16000):
    # Decode with audioread (FFmpeg backend)
    with audioread.audio_open(path) as f:
        sr_native = f.samplerate
        ch = f.channels
        pcm = b"".join(chunk for chunk in f)

    # 16-bit PCM -> float32 [-1, 1]
    y = np.frombuffer(pcm, dtype=np.int16).astype(np.float32) / 32768.0

    # stereo -> mono
    if ch and ch > 1:
        y = y.reshape(-1, ch).mean(axis=1)

    # resample if needed
    if sr_native != target_sr:
        y = resampy.resample(y, sr_native, target_sr)

    return y, target_sr

from transformers import WhisperProcessor

# make sure whisper_loc (path or HF id) is defined earlier
processor = WhisperProcessor.from_pretrained(whisper_loc, language="English", task="transcribe")

def prepare_dataset(batch):
    path = batch["audio"]["path"]          # Audio(decode=False) gives you a path
    y, _ = load_mp3(path, target_sr=16000)

    batch["input_features"] = processor.feature_extractor(
        y, sampling_rate=16000
    ).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

# IMPORTANT: keep single process so workers don’t re-import audio backends
vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,
    desc="Preparing dataset",
)
bad = []

def prepare_dataset_safe(batch):
    path = batch["audio"]["path"]
    try:
        y, _ = load_mp3(path, target_sr=16000)
    except Exception as e:
        bad.append((path, str(e)))
        return {"_bad": True}
    return {
        "input_features": processor.feature_extractor(y, sampling_rate=16000).input_features[0],
        "labels": processor.tokenizer(batch["sentence"]).input_ids
    }

vectorized_datasets = raw_datasets.map(
    prepare_dataset_safe,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,
    desc="Preparing dataset",
).filter(lambda ex: not ex.get("_bad", False))

print("Skipped files:", len(bad))
for p, err in bad[:5]:
    print(" -", p, "->", err)




import jiwer

def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # replace -100 with pad_token_id
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id

    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    # compute WER with jiwer
    wer = jiwer.wer(label_str, pred_str) * 100

    return {"wer": wer}



print(vectorized_datasets)
print(vectorized_datasets["train"][0].keys())   # 


should show: dict_keys(['input_features','labels'])




from datasets.features.audio import set_audio_backend
set_audio_backend("librosa")   # put this BEFORE you call .cast_column(...)


pip install librosa audioread
# (ffmpeg is recommended so audioread can decode mp3)

from datasets import Dataset, Audio
from datasets.features.audio import set_audio_backend
set_audio_backend("librosa")

raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}) \
             .cast_column("audio", Audio(sampling_rate=16_000))



# ---- put this at the very top of the notebook ----
import sys, types
sf_stub = types.ModuleType("soundfile")
sf_stub.__spec__ = None
sf_stub.libsndfile_version = "0.0.0"  # forces features that require libsndfile to be disabled
sys.modules["soundfile"] = sf_stub

# deps purely in Python
%pip install -q librosa audioread

# now it's safe to import datasets without crashing
from datasets import Dataset, Audio
import librosa

raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}) \
             .cast_column("audio", Audio(decode=False))
raw_datasets = raw.train_test_split(test_size=0.15, seed=91)

def prepare_dataset(batch):
    path = batch["audio"]["path"]
    array, sr = librosa.load(path, sr=16000, mono=True)
    batch["input_features"] = processor.feature_extractor(array, sampling_rate=16000).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=8
)



# ---- put this at the very top of the notebook, before any datasets import ----
import sys, types, importlib, importlib.machinery, importlib.util

# create a stub module with a valid spec and the attribute datasets probes
sf_stub = types.ModuleType("soundfile")
sf_stub.__dict__.update({
    "__spec__": importlib.machinery.ModuleSpec("soundfile", loader=None),
    "libsndfile_version": "0.0.0",   # make the probe evaluate safely
})
sys.modules["soundfile"] = sf_stub

# ensure find_spec("soundfile") returns a spec instead of raising
_orig_find_spec = importlib.util.find_spec
def _fake_find_spec(name, package=None):
    if name == "soundfile":
        return importlib.machinery.ModuleSpec("soundfile", loader=None)
    return _orig_find_spec(name, package)
importlib.util.find_spec = _fake_find_spec

# optional: use librosa for decoding later
%pip install -q librosa audioread

from datasets import Dataset, Audio
import librosa

# build dataset WITHOUT decoding; decode with librosa in map()
raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}).cast_column(
    "audio", Audio(decode=False)
)

def prepare_dataset(batch):
    path = batch["audio"]["path"]
    array, sr = librosa.load(path, sr=16000, mono=True)
    batch["input_features"] = processor.feature_extractor(array, sampling_rate=16000).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch



# --- put this in the VERY FIRST cell, before any `datasets` import ---
import sys, types, importlib, importlib.machinery, importlib.util

# create a stub that satisfies datasets' probes
sf_stub = types.ModuleType("soundfile")
sf_stub.__spec__ = importlib.machinery.ModuleSpec("soundfile", loader=None)
sf_stub.__libsndfile_version__ = "0.0.0"   # <-- exact name expected by datasets
sf_stub.libsndfile_version = "0.0.0"       # (some versions check this too)
sys.modules["soundfile"] = sf_stub

# ensure find_spec("soundfile") returns a spec
_orig_find_spec = importlib.util.find_spec
def _fake_find_spec(name, package=None):
    if name == "soundfile":
        return importlib.machinery.ModuleSpec("soundfile", loader=None)
    return _orig_find_spec(name, package)
importlib.util.find_spec = _fake_find_spec

# optional pure-Python decoders
%pip install -q librosa audioread

from datasets import Dataset, Audio
import librosa

# build WITHOUT decoding; decode in map()
raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}) \
             .cast_column("audio", Audio(decode=False))

def prepare_dataset(batch):
    path = batch["audio"]["path"]
    array, _ = librosa.load(path, sr=16000, mono=True)
    batch["input_features"] = processor.feature_extractor(array, sampling_rate=16000).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch


# conda (recommended)
conda install -c conda-forge libsndfile ffmpeg soundfile librosa audioread
# --- STUB SOUNDFILE (no libsndfile needed) ---
import sys, types, importlib, importlib.machinery, importlib.util

# if a previous bad stub exists, remove it
sys.modules.pop("soundfile", None)

# make a minimal stub with the attributes datasets probes
sf_stub = types.ModuleType("soundfile")
sf_stub.__spec__ = importlib.machinery.ModuleSpec("soundfile", loader=None)
sf_stub.__file__ = "<soundfile-stub>"
sf_stub.__package__ = ""
sf_stub.__libsndfile_version__ = "0.0.0"   # <— EXACT NAME REQUIRED
sf_stub.libsndfile_version = "0.0.0"       # some versions also check this
sys.modules["soundfile"] = sf_stub

# make importlib.util.find_spec('soundfile') return a spec (not None)
_orig_find_spec = importlib.util.find_spec
def _safe_find_spec(name, package=None):
    if name == "soundfile":
        return sf_stub.__spec__
    return _orig_find_spec(name, package)
importlib.util.find_spec = _safe_find_spec

# --- SOUNDFILE STUB: must run before importing `datasets` ---
import sys, types, importlib, importlib.machinery, importlib.util

# remove any previously loaded soundfile
sys.modules.pop("soundfile", None)

# create stub with the exact attributes `datasets` expects
_sf = types.ModuleType("soundfile")
_sf.__spec__ = importlib.machinery.ModuleSpec("soundfile", loader=None)
_sf.__file__ = "<soundfile-stub>"
_sf.__package__ = ""
_sf.__libsndfile_version__ = "0.0.0"   # EXACT attribute name required
_sf.libsndfile_version = "0.0.0"       # some versions also check this
sys.modules["soundfile"] = _sf

# keep original find_spec, then patch safely (to avoid recursion)
_orig_find_spec = importlib.util.find_spec
def _safe_find_spec(name, package=None):
    if name == "soundfile":
        return _sf.__spec__
    return _orig_find_spec(name, package)
importlib.util.find_spec = _safe_find_spec

# quick self-check
import importlib as _il
mod = _il.import_module("soundfile")
assert hasattr(mod, "__libsndfile_version__"), "stub missing __libsndfile_version__"





raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}) \
             .cast_column("audio", Audio(decode=False))

raw_datasets = raw.train_test_split(test_size=0.15, seed=91)

def prepare_dataset(batch):
    path = batch["audio"]["path"]
    array, _ = librosa.load(path, sr=16000, mono=True)
    batch["input_features"] = processor.feature_extractor(array, sampling_rate=16000).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch



from datasets import Dataset, Audio
import librosa

# 1) Build WITHOUT decoding (very important)
raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}).cast_column(
    "audio", Audio(decode=False)
)

# 2) Split and force plain Python format so printing doesn’t decode
raw_datasets = raw.train_test_split(test_size=0.15, seed=91)
raw_datasets = raw_datasets.with_format("python")


# Don’t do: [r["sentence"] for r in raw_datasets["test"]]  # triggers decoding
print(raw_datasets["test"]["sentence"][:10])                # SAFE (column access)

# or drop the audio column before looking at rows:
print(raw_datasets["test"].remove_columns("audio")[:3])     # SAFE

def prepare_dataset(batch):
    path = batch["audio"]["path"]                     # path only; not decoded
    array, _ = librosa.load(path, sr=16000, mono=True)
    batch["input_features"] = processor.feature_extractor(
        array, sampling_rate=16000
    ).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=8
)

print(vectorized_datasets["train"][0].keys())  # should be dict_keys(['input_features','labels'])





import numpy as np, audioread, resampy

def load_audio_ar(path: str, sr: int = 16000) -> tuple[np.ndarray, int]:
    """Decode audio via audioread (FFmpeg/MediaFoundation/CoreAudio),
    convert to mono float32 in [-1,1], and resample to `sr`."""
    with audioread.audio_open(path) as f:
        sr_native = f.samplerate
        ch = f.channels

        # concatenate raw PCM chunks
        pcm = b"".join(b for b in f)

    # audioread gives 16-bit PCM for MP3/WAV → int16 → float32
    y = np.frombuffer(pcm, dtype=np.int16).astype(np.float32) / 32768.0

    # deinterleave → mono
    if ch > 1:
        y = y.reshape(-1, ch).mean(axis=1)

    # resample if needed
    if sr_native != sr:
        y = resampy.resample(y, sr_native, sr)

    return y, sr

def prepare_dataset(batch):
    path = batch["audio"]["path"]                 # path only; not decoded
    array, _ = load_audio_ar(path, sr=16000)      # <-- use audioread loader
    batch["input_features"] = processor.feature_extractor(
        array, sampling_rate=16000
    ).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

from datasets import Dataset, Audio
raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}).cast_column(
    "audio", Audio(decode=False)
)
raw_datasets = raw.train_test_split(test_size=0.15, seed=91).with_format("python")

# Also avoid multiprocessing (workers re-import audio stacks):
vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,                      # <- important
    desc="Preparing dataset"
)



import subprocess; subprocess.run(["ffmpeg","-version"])



# --- pick ONE loader ---

# If you have FFmpeg installed (recommended):
import numpy as np, audioread, resampy

def load_audio(path, sr=16000):
    with audioread.audio_open(path) as f:
        sr_native = f.samplerate
        ch = f.channels
        pcm = b"".join(b for b in f)
    y = np.frombuffer(pcm, dtype=np.int16).astype(np.float32) / 32768.0
    if ch > 1:
        y = y.reshape(-1, ch).mean(axis=1)
    if sr_native != sr:
        y = resampy.resample(y, sr_native, sr)
    return y, sr

# If you don't have FFmpeg but files are WAV only, you can use librosa:
# import librosa
# def load_audio(path, sr=16000):
#     y, _ = librosa.load(path, sr=sr, mono=True)
#     return y, sr

def prepare_dataset(batch):
    # dataset was built with Audio(decode=False), so we only have a file path
    path = batch["audio"]["path"]
    array, _ = load_audio(path, sr=16000)

    batch["input_features"] = processor.feature_extractor(
        array, sampling_rate=16000
    ).input_features[0]

    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

# IMPORTANT: run mapping in a single process so no audio stack gets re-imported
vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,                       # <-- not 8
    desc="Preparing dataset",
)

def prepare_dataset(batch):
    a = batch["audio"]
    if isinstance(a, dict) and "array" in a and a["array"] is not None:
        array = a["array"]
        sr = a.get("sampling_rate", 16000)
    else:
        path = a["path"] if isinstance(a, dict) else a
        array, sr = load_audio(path, sr=16000)

    batch["input_features"] = processor.feature_extractor(array, sampling_rate=16000).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch