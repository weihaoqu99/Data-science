import sys, subprocess, importlib

# Install compatible versions
subprocess.check_call([sys.executable, "-m", "pip", "install", "--no-cache-dir",
                       "transformers==4.44.2", "accelerate==0.30.1", "sentencepiece", "datasets"])

# Verify versions and symbol
importlib.invalidate_caches()
import accelerate, transformers
from accelerate.utils import memory as _mem

print("Transformers:", transformers.__version__)
print("Accelerate  :", accelerate.__version__)
print("Has clear_device_cache? ", hasattr(_mem, "clear_device_cache"))



pip install transformers==4.44.2
pip install accelerate==0.30.0 sentencepiece datasets

import transformers
print(transformers.__version__)
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
from transformers import WhisperForConditionalGeneration, WhisperProcessor

print("âœ… All imports successful")


import sys, subprocess, site, os, shutil, importlib, glob

# 1) Uninstall transformers (ignore errors if not installed)
subprocess.run([sys.executable, "-m", "pip", "uninstall", "-y", "transformers"], check=False)

# 2) Hard-delete any leftover copies that can shadow a fresh install
paths = set(site.getsitepackages() + [site.getusersitepackages()])
for p in paths:
    d = os.path.join(p, "transformers")
    if os.path.isdir(d):
        print("Removing:", d)
        shutil.rmtree(d)
    # also remove any zipped/egg dists
    for z in glob.glob(os.path.join(p, "transformers*.dist-info")) + glob.glob(os.path.join(p, "transformers*.egg-info")):
        print("Removing:", z)
        shutil.rmtree(z)

# 3) Install a self-consistent version (recent & compatible)
#    4.45.x exports HybridCache/EncoderDecoderCache and matches trainer_seq2seq
subprocess.check_call([sys.executable, "-m", "pip", "install", "--no-cache-dir",
                       "transformers==4.45.2", "accelerate>=0.30.0", "sentencepiece", "huggingface_hub"])

# 4) Verify the *same* package is imported, and required symbols exist
importlib.invalidate_caches()
import transformers
from transformers import cache_utils
print("Transformers:", transformers.__version__, "from", transformers.__file__)
print("Has HybridCache? ", hasattr(cache_utils, "HybridCache"))
print("Has EncoderDecoderCache? ", hasattr(cache_utils, "EncoderDecoderCache"))

# 5) Final import test
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, WhisperForConditionalGeneration, WhisperProcessor
print("âœ… Imports OK")










import sys, subprocess, site, os, shutil, importlib

# 1) Uninstall and remove any leftovers that can shadow the new install
subprocess.run([sys.executable, "-m", "pip", "uninstall", "-y", "transformers"], check=False)
for p in set(site.getsitepackages() + [site.getusersitepackages()]):
    d = os.path.join(p, "transformers")
    if os.path.isdir(d):
        print("Removing:", d)
        shutil.rmtree(d)

# 2) Install a self-consistent, recent set (has EncoderDecoderCache)
subprocess.check_call([sys.executable, "-m", "pip", "install", "--no-cache-dir",
                       "transformers==4.43.3", "accelerate>=0.29.3", "sentencepiece", "huggingface_hub"])

# 3) Verify
importlib.invalidate_caches()
import transformers
print("Transformers:", transformers.__version__, "from", transformers.__file__)
from transformers import cache_utils
print("Has EncoderDecoderCache?", hasattr(cache_utils, "EncoderDecoderCache"))


from transformers import WhisperForConditionalGeneration, WhisperProcessor
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments








import transformers
from transformers import cache_utils as _cu
# expose the class where trainer_seq2seq expects it
transformers.EncoderDecoderCache = _cu.EncoderDecoderCache

from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments



import sys, subprocess

# Update pyopenssl and cryptography to latest
subprocess.check_call([sys.executable, "-m", "pip", "install", "--upgrade", "pyopenssl", "cryptography"])




import sys, subprocess, os, glob

# 1) Get fresh wheels
subprocess.check_call([sys.executable, "-m", "pip", "install", "--upgrade", "pip", "setuptools", "wheel"])

# 2) Install a wheel that ships a libsndfile (package name: sndfile) + SoundFile itself
subprocess.check_call([sys.executable, "-m", "pip", "install", "--no-cache-dir", "sndfile==0.2.1", "soundfile==0.12.1"])

# 3) Point SoundFile at the bundled libsndfile
import sndfile
libdir = os.path.dirname(sndfile.__file__)
cands = sorted(glob.glob(os.path.join(libdir, "libsndfile*.so*")))
if not cands:
    raise RuntimeError("Could not find bundled libsndfile in 'sndfile' package.")
os.environ["SOUNDFILE_LIBRARY"] = cands[0]  # SoundFile reads this env var

# 4) Verify
import soundfile as sf
print("libsndfile:", sf.__libsndfile_version__)



import sys, platform, soundfile, ctypes.util, os

print("Python:", sys.executable)
print("soundfile version:", soundfile.__version__)
print("soundfile module:", soundfile.__file__)
print("find_library('sndfile'):", ctypes.util.find_library('sndfile'))

# Optional: show common lib dirs
print("LD_LIBRARY_PATH:", os.environ.get("LD_LIBRARY_PATH"))

import subprocess, sys
subprocess.check_call([sys.executable, "-m", "pip", "install", "--upgrade", "pip", "setuptools", "wheel"])
subprocess.check_call([sys.executable, "-m", "pip", "install", "--force-reinstall", "--no-cache-dir", "soundfile==0.12.1"])

import os
os.environ["LD_LIBRARY_PATH"] = (os.environ.get("LD_LIBRARY_PATH","") + ":/usr/lib:/usr/lib/x86_64-linux-gnu").lstrip(":")
# now import soundfile
import soundfile as sf
print(sf.__libsndfile_version__)

import subprocess, sys
subprocess.check_call(["conda", "install", "-y", "-c", "conda-forge", "libsndfile", "soundfile"])






import os
os.environ["LD_LIBRARY_PATH"] = (os.environ.get("LD_LIBRARY_PATH","") + ":/usr/lib:/usr/lib/x86_64-linux-gnu").lstrip(":")
# now import soundfile
import soundfile as sf
print(sf.__libsndfile_version__)

import os
import re

# Output folder for test references
TEST_REF_DIR = "./test_references_txt"
os.makedirs(TEST_REF_DIR, exist_ok=True)

# Work only with the test split from raw_datasets
test_split = raw_datasets["test"]

for row in test_split:
    audio_path = row["audio_path"]
    ref = row["sentence"]

    base = os.path.splitext(os.path.basename(audio_path))[0]  
    # e.g. "08-I'm not able to use my card right now"

    # Extract the number from the start of the filename
    match = re.match(r"(\d+)-", base)
    if match:
        num = match.group(1)   # "08"
    else:
        num = "NA"             # fallback if no number

    # New filename: ground_truth_<num>.txt
    new_name = f"ground_truth_{num}.txt"
    out_file = os.path.join(TEST_REF_DIR, new_name)

    with open(out_file, "w", encoding="utf-8") as f:
        f.write(ref + "\n")

print(f"âœ… Wrote {len(test_split)} ground truth files to {TEST_REF_DIR}")




import os
import re

# Output folder for test references
TEST_REF_DIR = "./test_references_txt"
os.makedirs(TEST_REF_DIR, exist_ok=True)

# Work only with the test split from raw_datasets
test_split = raw_datasets["test"]

for row in test_split:
    audio_path = row["audio_path"]
    ref = row["sentence"]

    base = os.path.splitext(os.path.basename(audio_path))[0]  
    # e.g. "08-I'm not able to use my card right now"

    # Split number + text part
    match = re.match(r"(\d+)-(.*)", base)
    if match:
        num = match.group(1)          # "08"
        text_part = match.group(2).strip()  # "I'm not able to use my card right now"
    else:
        num = "NA"
        text_part = base

    # Clean text: replace spaces with underscores
    safe_text = text_part.replace(" ", "_")

    # New filename: <sentence_with_underscores>_<num>.txt
    new_name = f"{safe_text}_{num}.txt"
    out_file = os.path.join(TEST_REF_DIR, new_name)

    with open(out_file, "w", encoding="utf-8") as f:
        f.write(ref + "\n")

print(f"âœ… Wrote {len(test_split)} test reference files to {TEST_REF_DIR}")






import os
import re

# Your test split
test_split = raw_datasets["test"]

# Folder to save reference text files
REF_OUT_DIR = "./test_references_txt"
os.makedirs(REF_OUT_DIR, exist_ok=True)

for row in test_split:
    audio_path = row["audio_path"]
    ref = row["sentence"]

    # Extract number and text from filename
    base = os.path.splitext(os.path.basename(audio_path))[0]
    match = re.match(r"(\d+)-(.*)", base)

    if match:
        num = match.group(1)   # e.g., "08"
        text_part = match.group(2).strip()  # e.g., "I'm not able to use my card right now"
    else:
        num = "NA"
        text_part = base

    # Clean text: replace spaces with underscores
    safe_text = text_part.replace(" ", "_")

    # New name: <sentence>_<num>.txt
    out_file = os.path.join(REF_OUT_DIR, f"{safe_text}_{num}.txt")

    with open(out_file, "w", encoding="utf-8") as f:
        f.write(ref + "\n")

print(f"[OK] Wrote {len(test_split)} files in '<sentence>_<num>.txt' format")





import os

# Your test split
test_split = raw_datasets["test"]

# Folder to save reference text files
REF_OUT_DIR = "./test_references_txt"
os.makedirs(REF_OUT_DIR, exist_ok=True)

for row in test_split:
    audio_path = row["audio_path"]
    ref = row["sentence"]

    # take the base name of the audio file (without extension)
    base = os.path.splitext(os.path.basename(audio_path))[0]  
    # e.g. "ground_truth_08"

    # use the same name for the .txt file
    out_file = os.path.join(REF_OUT_DIR, base + ".txt")

    with open(out_file, "w", encoding="utf-8") as f:
        f.write(ref + "\n")

print(f"[OK] Wrote {len(test_split)} reference text files to {REF_OUT_DIR}")





import os, re

# Your test split
test_split = raw_datasets["test"]

# Folder to save reference text files
REF_OUT_DIR = "./test_references_txt"
os.makedirs(REF_OUT_DIR, exist_ok=True)

for row in test_split:
    audio_path = row["audio_path"]
    ref = row["sentence"]

    base = os.path.splitext(os.path.basename(audio_path))[0]  # e.g. abc_ground_truth_001

    # extract trailing number if present
    match = re.search(r"(\d+)$", base)
    if match:
        suffix = match.group(1)  # e.g. "001"
        out_file = os.path.join(REF_OUT_DIR, f"reference_{suffix}.txt")
    else:
        out_file = os.path.join(REF_OUT_DIR, base + ".txt")

    with open(out_file, "w", encoding="utf-8") as f:
        f.write(ref + "\n")

print(f"[OK] Wrote {len(test_split)} reference text files to {REF_OUT_DIR}")




import os

# Your test split
test_split = raw_datasets["test"]

# Folder to save reference text files
REF_OUT_DIR = "./test_references_txt"
os.makedirs(REF_OUT_DIR, exist_ok=True)

for i, row in enumerate(test_split):
    audio_path = row["audio_path"]
    ref = row["sentence"]

    # use base filename (without extension) as txt name
    base = os.path.splitext(os.path.basename(audio_path))[0]
    out_file = os.path.join(REF_OUT_DIR, base + ".txt")

    with open(out_file, "w", encoding="utf-8") as f:
        f.write(ref + "\n")

print(f"[OK] Wrote {len(test_split)} reference text files to {REF_OUT_DIR}")






trainer.save_model(ADAPTER_DIR)
print(f"[OK] LoRA adapter saved to: {ADAPTER_DIR}")
# ----------------------------------------------------

# ======================== TESTING ========================
# Your BASE + ADAPTER snippet (unchanged in spirit)
from peft import PeftModel

ADAPTER = ADAPTER_DIR
device  = "cuda" if torch.cuda.is_available() else "cpu"

base_model = WhisperForConditionalGeneration.from_pretrained(BASE).to(device).eval()
model = PeftModel.from_pretrained(base_model, ADAPTER).to(device).eval()
processor = WhisperProcessor.from_pretrained(BASE, language="English", task="transcribe")

# Avoid forced decoder IDs when generating
if getattr(model, "generation_config", None):
    model.generation_config.forced_decoder_ids = None
    model.generation_config.suppress_tokens = []

# Test set from the same split
test_paths = [row["audio_path"] for row in raw_datasets["test"]]
test_refs  = [row["sentence"] for row in raw_datasets["test"]]
prompt_ids = processor.get_decoder_prompt_ids(language="english", task="transcribe")

preds, wers = [], []
for s in range(0, len(test_paths), BATCH_TEST):
    e = min(s + BATCH_TEST, len(test_paths))
    feats_batch = []
    for p in test_paths[s:e]:
        wav, sr = load_mp3(p, SAMPLE_RATE)
        feats = processor.feature_extractor(wav, sampling_rate=sr).input_features
        feats_batch.append(torch.as_tensor(feats, dtype=torch.float32))
    feats = torch.cat(feats_batch, dim=0).to(device)
    dec_in = torch.as_tensor(prompt_ids, device=device).repeat(feats.size(0), 1)
    with torch.no_grad():
        gen = model.generate(inputs=feats, decoder_input_ids=dec_in, max_length=MAX_LEN)
        txt = processor.tokenizer.batch_decode(gen, skip_special_tokens=True)
    preds.extend(txt)
    wers.extend([wer_pct(r, h) for r, h in zip(test_refs[s:e], txt)])

overall = float(np.mean(wers)) if wers else math.nan
print(f"Overall Test WER: {overall:.2f}% | Samples: {len(test_paths)}")

df = pd.DataFrame({"audio_path": test_paths, "reference": test_refs, "prediction": preds, "WER%": wers})
df.to_csv(CSV_OUT, index=False)
print(f"[OK] Saved CSV: {CSV_OUT}")

# quick peek
for i in range(min(5, len(df))):
    print("\n", os.path.basename(df['audio_path'][i]))
    print("REF:", df['reference'][i])
    print("HYP:", df['prediction'][i])
    print("WER: %.2f%%" % df['WER%'][i])
# ====================== END TESTING ===






# pip install -q peft
from peft import LoraConfig, get_peft_model, PeftModel, TaskType

# ----- LoRA -----
# Whisper uses names like q_proj/k_proj/v_proj/out_proj in its attention layers
lora_config = LoraConfig(
    r=512,                      # rank
    lora_alpha=128,             # scaling
    lora_dropout=0.0,           # keep 0.0 unless you want regularization
    bias="none",
    task_type=TaskType.SEQ_2_SEQ_LM,
    target_modules=["q_proj", "k_proj", "v_proj", "out_proj"]  # attention projections
    # If you also want FFN adapters, add: "fc1", "fc2"
    ,
    use_rslora=True             # RS-LoRA like in your screenshot
)

# enable grads on inputs for peft+gradient checkpointing combos
model.enable_input_require_grads()

# wrap base model with LoRA adapters
model = get_peft_model(model, lora_config)

# (optional) sanity check
def print_trainable_parameters(m):
    trainable, total = 0, 0
    for _, p in m.named_parameters():
        total += p.numel()
        if p.requires_grad:
            trainable += p.numel()
    print(f"Trainable params: {trainable:,} | Total: {total:,} | %: {100*trainable/total:.2f}%")

print_trainable_parameters(model)
# ----------------


trainer.save_model("/appdata/cortex/dev1/shob/ASR_model/asr_finetuning/distil_whisper_v2_finetuned_2_lora")


from transformers import WhisperForConditionalGeneration, WhisperProcessor
from peft import PeftModel

BASE = distil_loc  # same path you used for the base model (e.g., distil-large-v2 copy)
ADAPTER = "/appdata/cortex/dev1/shob/ASR_model/asr_finetuning/distil_whisper_v2_finetuned_2_lora"

device = "cuda" if torch.cuda.is_available() else "cpu"

base_model = WhisperForConditionalGeneration.from_pretrained(BASE).to(device).eval()
model = PeftModel.from_pretrained(base_model, ADAPTER).to(device).eval()

processor = WhisperProcessor.from_pretrained(BASE, language="English", task="transcribe")

# Avoid forced decoder IDs when generating
if getattr(model, "generation_config", None):
    model.generation_config.forced_decoder_ids = None
    model.generation_config.suppress_tokens = []

















# ==== Whisper ASR: Test ALL MP3s + CSV (robust prompt shape) ====
import os, torch, numpy as np, pandas as pd, miniaudio
from transformers import WhisperForConditionalGeneration, WhisperProcessor

# --- config ---
SAVE_DIR   = "/appdata/cortex/dev1/shob/ASR model/asr finetuning/distil_loc_Basemodel"
FALLBACK   = "/appdata/cortex/dev1/whisper_copy/distil-large-v2"  # only if processor wasn't saved
BATCH_SIZE = 8
MAX_LEN    = 225
CSV_OUT    = "test_predictions_from_saved_model.csv"

device = "cuda" if torch.cuda.is_available() else "cpu"

# --- load model/processor ---
model = WhisperForConditionalGeneration.from_pretrained(SAVE_DIR).to(device).eval()
try:
    processor = WhisperProcessor.from_pretrained(SAVE_DIR, language="English", task="transcribe")
except Exception:
    processor = WhisperProcessor.from_pretrained(FALLBACK, language="English", task="transcribe")

# avoid generate() conflicts
if getattr(model, "generation_config", None):
    model.generation_config.forced_decoder_ids = None
    model.generation_config.suppress_tokens = []

# --- helpers ---
def load_mp3(path, target_sr=16000):
    sb = miniaudio.decode_file(path)
    sr, ch = getattr(sb, "sample_rate", 16000), getattr(sb, "channels", 1)
    y = np.asarray(sb.samples, dtype=np.float32)
    if ch > 1:
        y = y.reshape(-1, ch).mean(axis=1)
    if sr != target_sr:
        n = int(len(y) * target_sr / sr)
        y = np.interp(np.linspace(0, len(y), n, endpoint=False),
                      np.arange(len(y), dtype=np.float32), y.astype(np.float32)).astype(np.float32)
    return y, target_sr

def levenshtein(a, b):
    m, n = len(a), len(b); dp = list(range(n+1)); prev = [0]*(n+1)
    for i in range(1, m+1):
        prev, dp = dp, [i]+[0]*n
        for j in range(1, n+1):
            dp[j] = min(prev[j]+1, dp[j-1]+1, prev[j-1] + (a[i-1] != b[j-1]))
    return dp[n]

def wer_pct(ref, hyp):
    r, h = ref.split(), hyp.split()
    if len(r) == 0:  # avoid division by zero
        return 0.0 if len(h) == 0 else 100.0
    raw_wer = 100.0 * levenshtein(r, h) / len(r)
    if raw_wer > 100.0:
        print(f"[WARN] WER > 100% detected ({raw_wer:.2f}%). Clamping to 100%.")
        raw_wer = 100.0
    return raw_wer

def make_batch_prompt(prompt_ids, batch_size, device):
    """Return decoder_input_ids with shape (B, P) regardless of incoming shape."""
    t = torch.as_tensor(prompt_ids, device=device)
    if t.ndim == 1:        # [P] -> [1,P]
        t = t.unsqueeze(0)
    elif t.ndim >= 3:      # rare: flatten extras -> [1,P]
        t = t.view(1, -1)
    t = t[:1]
    return t.repeat(batch_size, 1)  # (B, P)

# --- test set from your prepared split ---
test_paths = [row["audio_path"] for row in raw_datasets["test"]]
test_refs  = [row["sentence"]   for row in raw_datasets["test"]]

prompt_ids = processor.get_decoder_prompt_ids(language="english", task="transcribe")

preds, wers = [], []
for s in range(0, len(test_paths), BATCH_SIZE):
    e = min(s + BATCH_SIZE, len(test_paths))
    feats_list = []
    for p in test_paths[s:e]:
        wav, sr = load_mp3(p, 16000)
        feats_np = processor.feature_extractor(wav, sampling_rate=sr).input_features  # (1,80,T)
        feats_list.append(torch.as_tensor(feats_np, dtype=torch.float32))
    feats = torch.cat(feats_list, dim=0).to(device)                 # (B,80,T)
    dec_in = make_batch_prompt(prompt_ids, feats.size(0), device)   # (B,P)

    with torch.no_grad():
        gen = model.generate(inputs=feats, decoder_input_ids=dec_in, max_length=MAX_LEN)
    batch_txt = processor.tokenizer.batch_decode(gen, skip_special_tokens=True)
    preds.extend(batch_txt)

# metrics + CSV
wers = [wer_pct(r, h) for r, h in zip(test_refs, preds)]
overall_wer = sum(wers) / max(1, len(wers))
print(f"Overall Test WER: {overall_wer:.2f}%  |  Samples: {len(preds)}")

df = pd.DataFrame({"audio_path": test_paths, "reference": test_refs, "prediction": preds, "WER_%": wers})
df.to_csv(CSV_OUT, index=False)
print(f"Saved: {CSV_OUT}")

# quick peek (with safeguard applied)
for i in range(min(5, len(df))):
    print(f"\n[{i}] {os.path.basename(df['audio_path'][i])}")
    print("REF:", df["reference"][i])
    print("HYP:", df["prediction"][i])
    print("WER: %.2f%%" % df["WER_%"][i])











import torch, numpy as np, pandas as pd, miniaudio
from transformers import WhisperProcessor, WhisperForConditionalGeneration

# --- base Distil-Whisper model ---
base_id = "distil-whisper/distil-large-v2"
device = "cuda" if torch.cuda.is_available() else "cpu"

processor = WhisperProcessor.from_pretrained(base_id, language="English", task="transcribe")
model = WhisperForConditionalGeneration.from_pretrained(base_id).to(device).eval()

# --- helper: load mp3 without librosa ---
def load_mp3(path, target_sr=16000):
    sb = miniaudio.decode_file(path)
    sr, ch = getattr(sb, "sample_rate", 16000), getattr(sb, "channels", 1)
    y = np.asarray(sb.samples, dtype=np.float32)
    if ch > 1: y = y.reshape(-1, ch).mean(axis=1)
    if sr != target_sr:
        n = int(len(y) * target_sr / sr)
        y = np.interp(np.linspace(0, len(y), n, endpoint=False),
                      np.arange(len(y), dtype=np.float32), y.astype(np.float32)).astype(np.float32)
    return y, target_sr

# --- WER functions ---
def levenshtein(a, b):
    m, n = len(a), len(b); dp = list(range(n+1)); prev = [0]*(n+1)
    for i in range(1, m+1):
        prev, dp = dp, [i]+[0]*n
        for j in range(1, n+1):
            dp[j] = min(prev[j]+1, dp[j-1]+1, prev[j-1] + (a[i-1] != b[j-1]))
    return dp[n]

def wer_pct(ref, hyp):
    r, h = ref.split(), hyp.split()
    return 100.0 * levenshtein(r, h) / max(1, len(r))

# --- get your existing test data from raw_datasets ---
test_paths = [row["audio_path"] for row in raw_datasets["test"]]
test_refs  = [row["sentence"]   for row in raw_datasets["test"]]

# --- prompt tokens ---
prompt_ids = processor.get_decoder_prompt_ids(language="english", task="transcribe")

# --- run inference ---
preds, wers = [], []
for pth, ref in zip(test_paths, test_refs):
    wav, sr = load_mp3(pth, 16000)
    feats = processor.feature_extractor(wav, sampling_rate=sr).input_features
    feats = torch.as_tensor(feats, dtype=torch.float32, device=device)
    dec_in = torch.as_tensor([prompt_ids], device=device)  # (1, P)

    with torch.no_grad():
        gen_ids = model.generate(inputs=feats, decoder_input_ids=dec_in, max_length=225)

    hyp = processor.tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]
    preds.append(hyp)
    wers.append(wer_pct(ref, hyp))

# --- save CSV ---
df = pd.DataFrame({"audio_path": test_paths, "reference": test_refs, "prediction": preds, "WER_%": wers})
df.to_csv("base_model_test_results.csv", index=False)

# --- summary ---
overall_wer = sum(wers) / max(1, len(wers))
print(f"Base Model Overall Test WER: {overall_wer:.2f}%  |  Samples: {len(wers)}")

# preview first 5
for i in range(min(5, len(df))):
    print(f"\n[{i}] {df['audio_path'][i]}")
    print("REF:", df["reference"][i])
    print("HYP:", df["prediction"][i])
    print("WER: %.2f%%" % df["WER_%"][i])






1) Base model & libs
	â€¢	Chose Distil-Whisper base: distil-whisper/distil-large-v2.
	â€¢	Used WhisperForConditionalGeneration + WhisperProcessor from ðŸ¤— Transformers.
	â€¢	Avoided librosa/ffmpeg; used miniaudio to load MP3s.

2) Build dataset from MP3 filenames
	â€¢	Collected MP3s (e.g., /appdata/cortex/dev1/origAudio/*.mp3).
	â€¢	Derived transcripts from filenames with your retrieve_transcript() cleanup (underscores â†’ spaces, fixes like dontâ†’don't, capitalization, etc.).
	â€¢	Created a HuggingFace Dataset with:
	â€¢	audio_path (full file path)
	â€¢	sentence (reference text)
	â€¢	Split into train/test (e.g., 85/15).

3) Feature/label preparation (no librosa)
	â€¢	Wrote load_mp3() using miniaudio:
	â€¢	Decode â†’ mono â†’ resample to 16kHz.
	â€¢	For each example:
	â€¢	processor.feature_extractor(...) â†’ input_features (80Ã—T log-Mels).
	â€¢	processor.tokenizer(...) â†’ labels (input IDs).

4) Data collator
	â€¢	Batched input_features and padded labels.
	â€¢	Replaced label padding with -100 (so loss ignores it).
	â€¢	Stripped leading decoder_start_token_id when present.

5) Model setup
	â€¢	Loaded base (or local) Distil-Whisper with:
	â€¢	WhisperForConditionalGeneration.from_pretrained(...)
	â€¢	WhisperProcessor.from_pretrained(..., language="English", task="transcribe")
	â€¢	Set:
	â€¢	model.config.language = "english"
	â€¢	model.config.task = "transcribe"

6) Training
	â€¢	Used Seq2SeqTrainer with typical ASR settings:
	â€¢	per_device_train_batch_size, gradient_accumulation_steps, learning_rate=1e-5,
	â€¢	max_steps (e.g., 1000), fp16 when GPU available,
	â€¢	predict_with_generate=True, generation_max_length=225,
	â€¢	evaluation_strategy="steps" to compute WER during training.
	â€¢	Metrics:
	â€¢	Implemented WER via Levenshtein distance on word tokens.
	â€¢	Trained successfully.
	â€¢	Saved both model and processor to your save dir:
	â€¢	/appdata/cortex/dev1/shob/ASR model/asr finetuning/distil_loc_Basemodel

7) Inference/testing on ALL test MP3s
	â€¢	Reloaded the saved model and processor.
	â€¢	Fixed generation config conflicts by clearing:
	â€¢	model.generation_config.forced_decoder_ids = None
	â€¢	model.generation_config.suppress_tokens = [] (optional)
	â€¢	Built Whisper prompt once:
	â€¢	prompt_ids = processor.get_decoder_prompt_ids(language="english", task="transcribe")
	â€¢	Important fix (error you hit):
	â€¢	Created a robust make_batch_prompt(...) to ensure decoder_input_ids has shape (B, P) for any batch size, avoiding:
	â€¢	â€œsize mismatchâ€ and
	â€¢	â€œrepeat dims smaller than tensor dimsâ€.

8) Batched generation
	â€¢	Processed test files in batches (e.g., BATCH_SIZE=8):
	â€¢	MP3 â†’ features â†’ stack to (B, 80, T)
	â€¢	decoder_input_ids = (B, P) from prompt_ids
	â€¢	model.generate(inputs=feats, decoder_input_ids=dec_in, max_length=225)
	â€¢	Decoded tokens â†’ predicted text (HYP).

9) Scoring + CSV
	â€¢	Computed per-sample WER with your Levenshtein.
	â€¢	Computed Overall Test WER (mean of per-sample WERs).
	â€¢	Wrote CSV with columns:
	â€¢	audio_path, reference, prediction, WER_%
	â€¢	Previewed first 5 items.
	â€¢	Final small fix: used df["WER_%"][i] (bracket indexing), not df.WER_%[i].

10) What you see as results
	â€¢	Console output:
	â€¢	Overall Test WER: XX.XX% | Samples: N
	â€¢	First 5 lines with REF:, HYP:, and WER:
	â€¢	File output:
	â€¢	test_predictions_from_saved_model.csv (or test_predictions.csv) saved in your working dir (or SAVE_DIR










# quick peek (fixed)
for i in range(min(5, len(df))):
    print(f"\n[{i}] {os.path.basename(df['audio_path'][i])}")
    print("REF:", df["reference"][i])
    print("HYP:", df["prediction"][i])
    print("WER: %.2f%%" % df["WER_%"][i])




# ==== Whisper ASR: Test ALL MP3s + CSV (robust prompt shape) ====
import os, torch, numpy as np, pandas as pd, miniaudio
from transformers import WhisperForConditionalGeneration, WhisperProcessor

# --- config ---
SAVE_DIR   = "/appdata/cortex/dev1/shob/ASR model/asr finetuning/distil_loc_Basemodel"
FALLBACK   = "/appdata/cortex/dev1/whisper_copy/distil-large-v2"  # only if processor wasn't saved
BATCH_SIZE = 8
MAX_LEN    = 225
CSV_OUT    = "test_predictions_from_saved_model.csv"

device = "cuda" if torch.cuda.is_available() else "cpu"

# --- load model/processor ---
model = WhisperForConditionalGeneration.from_pretrained(SAVE_DIR).to(device).eval()
try:
    processor = WhisperProcessor.from_pretrained(SAVE_DIR, language="English", task="transcribe")
except Exception:
    processor = WhisperProcessor.from_pretrained(FALLBACK, language="English", task="transcribe")

# avoid generate() conflicts
if getattr(model, "generation_config", None):
    model.generation_config.forced_decoder_ids = None
    model.generation_config.suppress_tokens = []

# --- helpers ---
def load_mp3(path, target_sr=16000):
    sb = miniaudio.decode_file(path)
    sr, ch = getattr(sb, "sample_rate", 16000), getattr(sb, "channels", 1)
    y = np.asarray(sb.samples, dtype=np.float32)
    if ch > 1: y = y.reshape(-1, ch).mean(axis=1)
    if sr != target_sr:
        n = int(len(y) * target_sr / sr)
        y = np.interp(np.linspace(0, len(y), n, endpoint=False),
                      np.arange(len(y), dtype=np.float32), y.astype(np.float32)).astype(np.float32)
    return y, target_sr

def levenshtein(a, b):
    m, n = len(a), len(b); dp = list(range(n+1)); prev = [0]*(n+1)
    for i in range(1, m+1):
        prev, dp = dp, [i]+[0]*n
        for j in range(1, n+1):
            dp[j] = min(prev[j]+1, dp[j-1]+1, prev[j-1] + (a[i-1] != b[j-1]))
    return dp[n]

def wer_pct(ref, hyp):
    r, h = ref.split(), hyp.split()
    return 100.0 * levenshtein(r, h) / max(1, len(r))

def make_batch_prompt(prompt_ids, batch_size, device):
    """Return decoder_input_ids with shape (B, P) regardless of incoming shape."""
    t = torch.as_tensor(prompt_ids, device=device)
    if t.ndim == 1:        # [P] -> [1,P]
        t = t.unsqueeze(0)
    elif t.ndim >= 3:      # rare: flatten extras -> [1,P]
        t = t.view(1, -1)
    # now t is [1,P] or [K,P]; take first row to be safe then repeat to B
    t = t[:1]
    return t.repeat(batch_size, 1)  # (B, P)

# --- test set from your prepared split (already in memory from training) ---
test_paths = [row["audio_path"] for row in raw_datasets["test"]]
test_refs  = [row["sentence"]   for row in raw_datasets["test"]]

prompt_ids = processor.get_decoder_prompt_ids(language="english", task="transcribe")

preds, wers = [], []
for s in range(0, len(test_paths), BATCH_SIZE):
    e = min(s + BATCH_SIZE, len(test_paths))
    # features
    feats_list = []
    for p in test_paths[s:e]:
        wav, sr = load_mp3(p, 16000)
        feats_np = processor.feature_extractor(wav, sampling_rate=sr).input_features  # (1,80,T)
        feats_list.append(torch.as_tensor(feats_np, dtype=torch.float32))
    feats = torch.cat(feats_list, dim=0).to(device)                 # (B,80,T)
    dec_in = make_batch_prompt(prompt_ids, feats.size(0), device)   # (B,P)

    with torch.no_grad():
        gen = model.generate(inputs=feats, decoder_input_ids=dec_in, max_length=MAX_LEN)
    batch_txt = processor.tokenizer.batch_decode(gen, skip_special_tokens=True)
    preds.extend(batch_txt)

# metrics + CSV
wers = [wer_pct(r, h) for r, h in zip(test_refs, preds)]
overall_wer = sum(wers) / max(1, len(wers))
print(f"Overall Test WER: {overall_wer:.2f}%  |  Samples: {len(preds)}")

df = pd.DataFrame({"audio_path": test_paths, "reference": test_refs, "prediction": preds, "WER_%": wers})
df.to_csv(CSV_OUT, index=False)
print(f"Saved: {CSV_OUT}")

# quick peek
for i in range(min(5, len(df))):
    print(f"\n[{i}] {os.path.basename(df.audio_path[i])}\nREF: {df.reference[i]}\nHYP: {df.prediction[i]}\nWER: {df.WER_%[i]:.2f}%")







# ===== Whisper ASR: Test ALL MP3s and save CSV =====
import os, torch, numpy as np, pandas as pd, miniaudio
from transformers import WhisperForConditionalGeneration, WhisperProcessor

# --- CONFIG: update if needed ---
SAVE_DIR   = "/appdata/cortex/dev1/shob/ASR model/asr finetuning/distil_loc_Basemodel"
FALLBACK   = "/appdata/cortex/dev1/whisper_copy/distil-large-v2"  # used only if processor wasn't saved
BATCH_SIZE = 8
MAX_LEN    = 225
CSV_OUT    = "test_predictions_from_saved_model.csv"

device = "cuda" if torch.cuda.is_available() else "cpu"

# --- load model & processor ---
model = WhisperForConditionalGeneration.from_pretrained(SAVE_DIR).to(device).eval()
try:
    processor = WhisperProcessor.from_pretrained(SAVE_DIR, language="English", task="transcribe")
except Exception:
    processor = WhisperProcessor.from_pretrained(FALLBACK, language="English", task="transcribe")

# avoid generate() conflicts
if getattr(model, "generation_config", None):
    model.generation_config.forced_decoder_ids = None
    model.generation_config.suppress_tokens = []

# --- helpers ---
def load_mp3(path, target_sr=16000):
    sb = miniaudio.decode_file(path)
    sr, ch = getattr(sb, "sample_rate", 16000), getattr(sb, "channels", 1)
    y = np.asarray(sb.samples, dtype=np.float32)
    if ch > 1: y = y.reshape(-1, ch).mean(axis=1)
    if sr != target_sr:
        n = int(len(y) * target_sr / sr)
        y = np.interp(np.linspace(0, len(y), n, endpoint=False),
                      np.arange(len(y), dtype=np.float32), y.astype(np.float32)).astype(np.float32)
    return y, target_sr

def levenshtein(a, b):
    m, n = len(a), len(b); dp = list(range(n+1)); prev = [0]*(n+1)
    for i in range(1, m+1):
        prev, dp = dp, [i]+[0]*n
        for j in range(1, n+1):
            dp[j] = min(prev[j]+1, dp[j-1]+1, prev[j-1] + (a[i-1] != b[j-1]))
    return dp[n]

def wer_pct(ref, hyp):
    r, h = ref.split(), hyp.split()
    return 100.0 * levenshtein(r, h) / max(1, len(r))

# --- get test set from your prepared split ---
# raw_datasets["test"] must exist from training and include 'audio_path' + 'sentence'
test_paths = [row["audio_path"] for row in raw_datasets["test"]]
test_refs  = [row["sentence"]   for row in raw_datasets["test"]]

# --- decoder prompt (language+task) ---
prompt_ids = processor.get_decoder_prompt_ids(language="english", task="transcribe")
prompt_t   = torch.as_tensor(prompt_ids, device=device)  # (P,)

# --- batched inference ---
preds = []
for s in range(0, len(test_paths), BATCH_SIZE):
    e = min(s + BATCH_SIZE, len(test_paths))
    feats_list = []
    for p in test_paths[s:e]:
        wav, sr = load_mp3(p, 16000)
        feats_np = processor.feature_extractor(wav, sampling_rate=sr).input_features  # (1,80,T)
        feats_list.append(torch.as_tensor(feats_np, dtype=torch.float32))
    feats = torch.cat(feats_list, dim=0).to(device)                # (B,80,T)
    dec_in = prompt_t.unsqueeze(0).repeat(feats.size(0), 1)        # (B,P)

    with torch.no_grad():
        gen = model.generate(inputs=feats, decoder_input_ids=dec_in, max_length=MAX_LEN)
    preds.extend(processor.tokenizer.batch_decode(gen, skip_special_tokens=True))

# --- metrics + CSV ---
wers = [wer_pct(r, h) for r, h in zip(test_refs, preds)]
overall_wer = sum(wers) / max(1, len(wers))
print(f"Overall Test WER: {overall_wer:.2f}%  |  Samples: {len(preds)}")

df = pd.DataFrame({"audio_path": test_paths, "reference": test_refs, "prediction": preds, "WER_%": wers})
df.to_csv(CSV_OUT, index=False)
print(f"Saved: {CSV_OUT}")

# quick peek
for i in range(min(5, len(df))):
    print(f"\n[{i}] {os.path.basename(df.audio_path[i])}\nREF: {df.reference[i]}\nHYP: {df.prediction[i]}\nWER: {df.WER_%[i]:.2f}%")









# =========================
# ASR TESTING / INFERENCE
# =========================
# Requirements: torch, transformers, numpy, pandas, miniaudio
# Assumes you still have `raw_datasets` from training with columns:
#   raw_datasets["test"][i] -> {"audio_path": <path>, "sentence": <reference>}

import os
import torch
import numpy as np
import miniaudio
import pandas as pd
from typing import List
from transformers import WhisperForConditionalGeneration, WhisperProcessor

# -----------------
# Paths & devices
# -----------------
save_dir = "/appdata/cortex/dev1/shob/ASR model/asr finetuning/distil_loc_Basemodel"  # <- your saved model
fallback_proc = "/appdata/cortex/dev1/whisper_copy/distil-large-v2"                   # if processor not saved
device = "cuda" if torch.cuda.is_available() else "cpu"

# -----------------
# Load model & processor
# -----------------
model = WhisperForConditionalGeneration.from_pretrained(save_dir).to(device)
try:
    processor = WhisperProcessor.from_pretrained(save_dir, language="English", task="transcribe")
except Exception:
    processor = WhisperProcessor.from_pretrained(fallback_proc, language="English", task="transcribe")

# Keep config consistent with training
model.config.language = "english"
model.config.task = "transcribe"
# Avoid generation conflict:
if hasattr(model, "generation_config") and model.generation_config is not None:
    model.generation_config.forced_decoder_ids = None
    model.generation_config.suppress_tokens = []

model.eval()

# -----------------
# Audio loader (no librosa/ffmpeg needed)
# -----------------
def load_mp3(path: str, target_sr: int = 16000):
    sb = miniaudio.decode_file(path)
    sr_native = getattr(sb, "sample_rate", 16000)
    ch = getattr(sb, "channels", 1)
    y = np.asarray(sb.samples, dtype=np.float32)
    if ch > 1:
        y = y.reshape(-1, ch).mean(axis=1)  # stereo -> mono
    if sr_native != target_sr:
        n_new = int(len(y) * target_sr / sr_native)
        y = np.interp(
            np.linspace(0, len(y), n_new, endpoint=False),
            np.arange(len(y), dtype=np.float32),
            y.astype(np.float32)
        ).astype(np.float32)
    return y, target_sr

# -----------------
# WER helpers (your Levenshtein)
# -----------------
def levenshtein(a: List[str], b: List[str]) -> int:
    m, n = len(a), len(b)
    dp = list(range(n + 1))
    prev = [0] * (n + 1)
    for i in range(1, m + 1):
        prev, dp = dp, [i] + [0] * n
        for j in range(1, n + 1):
            dp[j] = min(prev[j] + 1, dp[j - 1] + 1, prev[j - 1] + (a[i - 1] != b[j - 1]))
    return dp[n]

def sample_wer(ref: str, hyp: str) -> float:
    ref_tok = ref.strip().split()
    hyp_tok = hyp.strip().split()
    denom = max(1, len(ref_tok))
    return 100.0 * levenshtein(ref_tok, hyp_tok) / denom

# -----------------
# Build decoder prompt for Whisper and run inference
# -----------------
prompt_ids = processor.get_decoder_prompt_ids(language="english", task="transcribe")
decoder_input_ids = torch.tensor([prompt_ids], device=device)

# Pull test split from your prepared dataset
test_paths = [row["audio_path"] for row in raw_datasets["test"]]
test_refs  = [row["sentence"]   for row in raw_datasets["test"]]

preds = []
per_sample_wers = []

for pth, ref in zip(test_paths, test_refs):
    wav, sr = load_mp3(pth, target_sr=16000)
    feats = processor.feature_extractor(wav, sampling_rate=sr).input_features
    feats = torch.tensor(feats, dtype=torch.float32, device=device)

    with torch.no_grad():
        gen_ids = model.generate(
            inputs=feats,
            decoder_input_ids=decoder_input_ids,  # <-- fixes forced_decoder_ids error
            max_length=225
        )

    hyp = processor.tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]
    preds.append(hyp)
    per_sample_wers.append(sample_wer(ref, hyp))

# -----------------
# Results: print first 5, overall WER, save CSV
# -----------------
print("\n=== First 5 test predictions ===")
for i in range(min(5, len(preds))):
    print(f"\n[{i}] file: {os.path.basename(test_paths[i])}")
    print("REF:", test_refs[i])
    print("HYP:", preds[i])
    print("WER:%0.2f" % per_sample_wers[i])

overall_wer = sum(per_sample_wers) / max(1, len(per_sample_wers))
print(f"\n=== Overall Test WER: {overall_wer:.2f}% ===")

df = pd.DataFrame({
    "audio_path": test_paths,
    "reference":  test_refs,
    "prediction": preds,
    "WER_%":      per_sample_wers
})
out_csv = "test_predictions_from_saved_model.csv"
df.to_csv(out_csv, index=False)
print(f"Saved full test results to: {out_csv}")









# ===== Inference on test data with the SAVED model =====

import os
import torch
import numpy as np
import miniaudio
import pandas as pd
from transformers import WhisperForConditionalGeneration, WhisperProcessor

# --- paths ---
save_dir = "/appdata/cortex/dev1/shob/ASR model/asr finetuning/distil_loc_Basemodel"  # your saved model
# if processor wasn't saved with the model, fall back to the original training processor path:
fallback_proc = "/appdata/cortex/dev1/whisper_copy/distil-large-v2"

# --- load model & processor ---
device = "cuda" if torch.cuda.is_available() else "cpu"
model = WhisperForConditionalGeneration.from_pretrained(save_dir).to(device)
try:
    processor = WhisperProcessor.from_pretrained(save_dir, language="English", task="transcribe")
except Exception:
    processor = WhisperProcessor.from_pretrained(fallback_proc, language="English", task="transcribe")

model.config.forced_decoder_ids = None
model.config.suppress_tokens = []
model.config.language = "english"
model.config.task = "transcribe"
model.eval()

# --- utility: mp3 loader (same idea you used for training) ---
def load_mp3(path: str, target_sr: int = 16000):
    sb = miniaudio.decode_file(path)
    sr_native = getattr(sb, "sample_rate", 16000)
    ch = getattr(sb, "channels", 1)
    y = np.asarray(sb.samples, dtype=np.float32)
    if ch > 1:
        y = y.reshape(-1, ch).mean(axis=1)   # stereo -> mono
    if sr_native != target_sr:
        n_new = int(len(y) * target_sr / sr_native)
        y = np.interp(
            np.linspace(0, len(y), n_new, endpoint=False),
            np.arange(len(y), dtype=np.float32),
            y.astype(np.float32)
        ).astype(np.float32)
    return y, target_sr

# --- WER helpers (your Levenshtein-based computation) ---
def levenshtein(a, b):
    m, n = len(a), len(b)
    dp = list(range(n + 1))
    prev = [0] * (n + 1)
    for i in range(1, m + 1):
        prev, dp = dp, [i] + [0] * n
        for j in range(1, n + 1):
            dp[j] = min(prev[j] + 1, dp[j - 1] + 1, prev[j - 1] + (a[i - 1] != b[j - 1]))
    return dp[n]

def sample_wer(ref: str, hyp: str) -> float:
    ref_tokens = ref.strip().split()
    hyp_tokens = hyp.strip().split()
    denom = max(1, len(ref_tokens))
    return 100.0 * levenshtein(ref_tokens, hyp_tokens) / denom

# --- build test table from your raw/processed datasets already in memory ---
# raw_datasets["test"] has 'audio_path' and 'sentence' (you created these during prep)
test_paths = [row["audio_path"] for row in raw_datasets["test"]]
test_refs  = [row["sentence"]   for row in raw_datasets["test"]]

# --- run inference on the FIRST 5 examples ("expected results 5") ---
N_SHOW = min(5, len(test_paths))
print(f"\n=== Predictions for first {N_SHOW} test items ===")
shown = []
for i in range(N_SHOW):
    wav, sr = load_mp3(test_paths[i], target_sr=16000)
    feats = processor.feature_extractor(wav, sampling_rate=sr).input_features
    feats = torch.tensor(feats, dtype=torch.float32, device=device)

    with torch.no_grad():
        gen_ids = model.generate(
            inputs=feats,
            max_length=225
        )

    hyp = processor.tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]
    ref = test_refs[i]
    w = sample_wer(ref, hyp)
    print(f"\n[{i}] file: {os.path.basename(test_paths[i])}")
    print("REF:", ref)
    print("HYP:", hyp)
    print("WER:%0.2f" % w)
    shown.append((test_paths[i], ref, hyp, w))

# --- (optional) run FULL test set and save CSV ---
DO_SAVE_CSV = True
if DO_SAVE_CSV:
    preds = []
    for pth, ref in zip(test_paths, test_refs):
        wav, sr = load_mp3(pth, target_sr=16000)
        feats = processor.feature_extractor(wav, sampling_rate=sr).input_features
        feats = torch.tensor(feats, dtype=torch.float32, device=device)
        with torch.no_grad():
            gen_ids = model.generate(inputs=feats, max_length=225)
        hyp = processor.tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]
        preds.append(hyp)

    df = pd.DataFrame({
        "audio_path": test_paths,
        "reference":  test_refs,
        "prediction": preds,
        "WER_%":      [sample_wer(r, h) for r, h in zip(test_refs, preds)]
    })
    out_csv = "test_predictions_from_saved_model.csv"
    df.to_csv(out_csv, index=False)
    print(f"\nSaved full test predictions to: {out_csv}")










# ==== ONE-CELL, PATH-LESS, NO-SOUNDFILE/LIBROSA/TORCHAUDIO ====
import sys, subprocess, importlib, wave, os, shutil
import numpy as np
import torch
from transformers import pipeline

# ---------------- EDIT ME ----------------
mp3_path = "/appdata/cortex/devl/zknze7ne/whisper_training/sample_for_synthetic.mp3"
# model = ...        # your preloaded model
# processor = ...    # your preloaded processor
# ----------------------------------------

# 1) Ensure imageio-ffmpeg is available and get the bundled ffmpeg binary path
def ensure_imageio_ffmpeg():
    try:
        import imageio_ffmpeg
    except Exception:
        # install into the current kernel env
        subprocess.run([sys.executable, "-m", "pip", "install", "-q", "imageio-ffmpeg"], check=True)
        import imageio_ffmpeg
    return imageio_ffmpeg.get_ffmpeg_exe()

ffmpeg_bin = ensure_imageio_ffmpeg()
print("Using ffmpeg at:", ffmpeg_bin)

# 2) Determine target sampling rate from your processor (fallback 16k)
target_sr = getattr(getattr(processor, "feature_extractor", None), "sampling_rate", 16000)

# 3) Convert MP3 -> temp WAV (mono, target_sr) using that exact ffmpeg binary
tmp_wav = mp3_path + ".tmp.16k.wav"
subprocess.run(
    [ffmpeg_bin, "-y", "-i", mp3_path, "-ac", "1", "-ar", str(target_sr), "-f", "wav", tmp_wav],
    check=True
)

# 4) Read WAV via stdlib (no external audio libs)
with wave.open(tmp_wav, "rb") as wf:
    sr = wf.getframerate()
    nframes = wf.getnframes()
    sampwidth = wf.getsampwidth()
    nch = wf.getnchannels()
    audio_bytes = wf.readframes(nframes)

# We asked ffmpeg to write 16-bit PCM
if sampwidth != 2:
    raise RuntimeError(f"Expected 16-bit WAV, got sampwidth={sampwidth}")

audio = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) / 32768.0
if nch > 1:
    audio = audio.reshape(-1, nch).mean(axis=1).astype(np.float32)

# 5) HF ASR pipeline with YOUR model
pipe = pipeline(
    "automatic-speech-recognition",
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device=0 if torch.cuda.is_available() else -1,
    chunk_length_s=30,
    stride_length_s=(4, 2),
)

# 6) Transcribe & print text
out = pipe({"array": audio, "sampling_rate": sr})
print(out["text"])

# 7) Cleanup temp WAV (optional)
try:
    os.remove(tmp_wav)
except OSError:
    pass






import shutil; print(shutil.which("ffmpeg"))

# ===================== EDIT ME =====================
mp3_path = "/appdata/cortex/devl/zknze7ne/whisper_training/sample_for_synthetic.mp3"  # your MP3
# Your OWN already-loaded model & processor (keep your existing objects)
# model = ...
# processor = ...
# ==================================================

import os, sys, shutil, subprocess
import numpy as np
import torch
from transformers import pipeline

# model/processor sampling rate (fallback 16k)
target_sr = getattr(getattr(globals().get("processor", None), "feature_extractor", None), "sampling_rate", 16000)

def _resample_np_mono(x: np.ndarray, sr_in: int, sr_out: int) -> np.ndarray:
    """Pure-NumPy linear resample (no scipy/torchaudio). x is mono float32 [-1,1]."""
    if sr_in == sr_out: 
        return x.astype(np.float32, copy=False)
    n_out = int(round(len(x) * sr_out / sr_in))
    # Guard tiny/empty inputs
    if n_out <= 1 or len(x) <= 1:
        return x.astype(np.float32, copy=False)
    t_in  = np.linspace(0.0, len(x) / sr_in, num=len(x), endpoint=False, dtype=np.float64)
    t_out = np.linspace(0.0, len(x) / sr_out, num=n_out,  endpoint=False, dtype=np.float64)
    y = np.interp(t_out, t_in, x.astype(np.float64))
    return y.astype(np.float32)

def _try_torchaudio(path: str):
    try:
        import torchaudio
    except Exception as e:
        return None, None, f"torchaudio not available: {e}"
    try:
        wav, sr = torchaudio.load(path)  # [C, T]
        if wav.ndim != 2 or wav.size(0) == 0:
            return None, None, "torchaudio: invalid shape"
        # to mono
        if wav.size(0) > 1:
            wav = wav.mean(dim=0, keepdim=False)
        else:
            wav = wav.squeeze(0)
        wav = wav.detach().cpu().float().numpy()  # float32 in [-1,1]
        wav = _resample_np_mono(wav, sr, target_sr)
        return wav, target_sr, None
    except Exception as e:
        return None, None, f"torchaudio failed: {e}"

def _try_librosa(path: str):
    try:
        import librosa
    except Exception as e:
        return None, None, f"librosa not available: {e}"
    try:
        # Force pure audioread backend (avoids soundfile)
        audio, sr = librosa.load(path, sr=None, mono=True, backend="audioread")
        # librosa may give float64; ensure float32
        audio = audio.astype(np.float32, copy=False)
        audio = _resample_np_mono(audio, sr, target_sr)
        return audio, target_sr, None
    except Exception as e:
        return None, None, f"librosa(audioread) failed: {e}"

def _find_ffmpeg():
    # Allow explicit override
    if "FFMPEG_BIN" in os.environ and os.path.isfile(os.environ["FFMPEG_BIN"]):
        return os.environ["FFMPEG_BIN"]
    cand = shutil.which("ffmpeg")
    if cand:
        return cand
    candidates = [
        "/usr/bin/ffmpeg", "/usr/local/bin/ffmpeg", os.path.expanduser("~/.local/bin/ffmpeg"),
        "/opt/homebrew/bin/ffmpeg",  # mac (arm)
        os.path.join(sys.prefix, "bin", "ffmpeg"),            # conda/env
        os.path.join(sys.prefix, "Library", "bin", "ffmpeg"), # conda-win
        r"C:\ffmpeg\bin\ffmpeg.exe", r"C:\Program Files\ffmpeg\bin\ffmpeg.exe",
    ]
    for c in candidates:
        if os.path.isfile(c):
            return c
    return None

def _try_ffmpeg(path: str):
    ffmpeg = _find_ffmpeg()
    if not ffmpeg:
        return None, None, "ffmpeg not found from Python (set FFMPEG_BIN env or add to PATH)"
    try:
        cmd = [
            ffmpeg, "-i", path,
            "-f", "s16le", "-acodec", "pcm_s16le",
            "-ac", "1", "-ar", str(target_sr),
            "-hide_banner", "-loglevel", "error",
            "pipe:1"
        ]
        proc = subprocess.run(cmd, check=True, stdout=subprocess.PIPE)
        audio_i16 = np.frombuffer(proc.stdout, dtype=np.int16)
        if audio_i16.size == 0:
            return None, None, "ffmpeg returned empty PCM"
        audio = (audio_i16.astype(np.float32) / 32768.0)
        return audio, target_sr, None
    except subprocess.CalledProcessError as e:
        return None, None, f"ffmpeg decode failed (returncode {e.returncode})"
    except Exception as e:
        return None, None, f"ffmpeg error: {e}"

# ---------- load MP3 with robust fallback chain ----------
errors = []
audio, sr, err = _try_torchaudio(mp3_path)
if err: errors.append(err)
if audio is None:
    audio, sr, err = _try_librosa(mp3_path)
    if err: errors.append(err)
if audio is None:
    audio, sr, err = _try_ffmpeg(mp3_path)
    if err: errors.append(err)

if audio is None:
    raise RuntimeError(
        "Could not decode MP3 with any backend.\n"
        + "\n".join(f"- {e}" for e in errors)
        + "\nTips:\n"
        + "â€¢ If torchaudio failed, it may lack MP3 support; try installing a build with mp3 (lame).\n"
        + "â€¢ If librosa failed due to audioread/ffprobe, ensure ffmpeg is installed and reachable from this Python.\n"
        + "â€¢ For ffmpeg, set an explicit path inside Python:\n"
        + "    os.environ['FFMPEG_BIN'] = '/usr/local/bin/ffmpeg'   # or r'C:\\ffmpeg\\bin\\ffmpeg.exe'\n"
    )

# ---------- run your ASR pipeline ----------
pipe = pipeline(
    "automatic-speech-recognition",
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device=0 if torch.cuda.is_available() else -1,
    chunk_length_s=30,
    stride_length_s=(4, 2),
)

result = pipe({"array": audio, "sampling_rate": sr})
print(result["text"])










import os
os.environ["PATH"] = "/usr/local/bin:" + os.environ["PATH"]


import os, sys, shutil, subprocess
import numpy as np
import torch
from transformers import pipeline

# ========= 1) Point this to your MP3 =========
mp3_path = "/appdata/cortex/devl/zknze7ne/whisper_training/sample_for_synthetic.mp3"

# ========= 2) Your loaded model & processor =========
# model = ...
# processor = ...
target_sr = getattr(getattr(processor, "feature_extractor", None), "sampling_rate", 16000)

# ========= 3) Robust ffmpeg discovery =========
def find_ffmpeg():
    # A) Respect explicit env override if you want to hard-set it:
    #    os.environ["FFMPEG_BIN"] = r"C:\ffmpeg\bin\ffmpeg.exe"
    if "FFMPEG_BIN" in os.environ and os.path.isfile(os.environ["FFMPEG_BIN"]):
        return os.environ["FFMPEG_BIN"]

    # B) Try PATH first
    p = shutil.which("ffmpeg")
    if p: 
        return p

    # C) Probe common locations for Linux/Mac/Conda/Windows
    candidates = [
        "/usr/bin/ffmpeg",
        "/usr/local/bin/ffmpeg",
        "/opt/homebrew/bin/ffmpeg",              # Apple Silicon (brew)
        os.path.expanduser("~/.local/bin/ffmpeg"),
        os.path.join(sys.prefix, "bin", "ffmpeg"),            # conda/env/bin/ffmpeg
        os.path.join(sys.prefix, "Library", "bin", "ffmpeg"), # conda on Windows
        r"C:\ffmpeg\bin\ffmpeg.exe",
        r"C:\Program Files\ffmpeg\bin\ffmpeg.exe",
    ]
    for c in candidates:
        if os.path.isfile(c):
            return c

    # D) Last attempt: extend PATH if ffmpeg installed but not visible
    extra_paths = [
        "/usr/local/bin", "/usr/bin", "/opt/homebrew/bin",
        os.path.join(sys.prefix, "bin"),
        os.path.join(sys.prefix, "Library", "bin"),
        r"C:\ffmpeg\bin", r"C:\Program Files\ffmpeg\bin",
    ]
    os.environ["PATH"] = os.pathsep.join(extra_paths + [os.environ.get("PATH", "")])
    p = shutil.which("ffmpeg")
    if p:
        return p

    raise RuntimeError(
        "ffmpeg not found from Python. If it is installed, either:\n"
        "  1) Set os.environ['FFMPEG_BIN'] to the full path of ffmpeg, or\n"
        "  2) Add its folder to PATH inside this notebook and re-run."
    )

ffmpeg_cmd = find_ffmpeg()

# ========= 4) Decode MP3 â†’ float32 PCM with ffmpeg (no soundfile/librosa/torchaudio) =========
cmd = [
    ffmpeg_cmd, "-i", mp3_path,
    "-f", "s16le", "-acodec", "pcm_s16le",
    "-ac", "1", "-ar", str(target_sr),
    "-hide_banner", "-loglevel", "error",
    "pipe:1"
]
proc = subprocess.run(cmd, check=True, stdout=subprocess.PIPE)

audio_int16 = np.frombuffer(proc.stdout, dtype=np.int16)
audio = (audio_int16.astype(np.float32) / 32768.0)

# ========= 5) ASR pipeline with YOUR model =========
pipe = pipeline(
    "automatic-speech-recognition",
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device=0 if torch.cuda.is_available() else -1,
    chunk_length_s=30,
    stride_length_s=(4, 2),
)

result = pipe({"array": audio, "sampling_rate": target_sr})
print(result["text"])






import subprocess, shutil, sys
import numpy as np
import torch
from transformers import pipeline

# ---- your preloaded model & processor here ----
# model = ...
# processor = ...
# -----------------------------------------------

# sampling rate expected by your model/feature_extractor (fallback 16k)
target_sr = getattr(getattr(processor, "feature_extractor", None), "sampling_rate", 16000)

# 1) Ensure ffmpeg is available
if shutil.which("ffmpeg") is None:
    raise RuntimeError(
        "ffmpeg is not available on PATH. Please install ffmpeg or add it to PATH. "
        "No soundfile/librosa/torchaudio backends are used."
    )

# 2) Path to your MP3
mp3_path = "/appdata/cortex/devl/zknze7ne/whisper_training/sample_for_synthetic.mp3"

# 3) Decode MP3 -> raw 16-bit PCM mono at target_sr via ffmpeg (no temp files)
#    Output is written to stdout (pipe:1)
cmd = [
    "ffmpeg",
    "-i", mp3_path,
    "-f", "s16le",           # raw 16-bit little-endian PCM
    "-acodec", "pcm_s16le",
    "-ac", "1",              # mono
    "-ar", str(target_sr),   # resample
    "-hide_banner",
    "-loglevel", "error",
    "pipe:1"
]

proc = subprocess.run(cmd, check=True, stdout=subprocess.PIPE)

# 4) Convert bytes -> float32 waveform in [-1, 1]
audio_int16 = np.frombuffer(proc.stdout, dtype=np.int16)
audio = audio_int16.astype(np.float32) / 32768.0

# 5) Build ASR pipeline using YOUR model/processor
pipe = pipeline(
    "automatic-speech-recognition",
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device=0 if torch.cuda.is_available() else -1,
    chunk_length_s=30,
    stride_length_s=(4, 2),
)

# 6) Transcribe & print
result = pipe({"array": audio, "sampling_rate": target_sr})
print(result["text"])






# --- deps (no librosa / no soundfile)
import torch
import torchaudio
from transformers import pipeline

# --- your preloaded model & processor
# model = ...
# processor = ...

# target sampling rate from your processor (fallback 16k)
target_sr = getattr(getattr(processor, "feature_extractor", None), "sampling_rate", 16000)

# --- load MP3 with torchaudio
mp3_path = "/appdata/cortex/devl/zknze7ne/whisper_training/sample_for_synthetic.mp3"
wave, sr = torchaudio.load(mp3_path)           # wave: [channels, time], dtype=float32/-1..1

# mono
if wave.size(0) > 1:
    wave = wave.mean(dim=0, keepdim=True)

# resample if needed
if sr != target_sr:
    resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)
    wave = resampler(wave)

# to 1D numpy for the pipeline
audio = wave.squeeze(0).cpu().numpy()

# --- build ASR pipeline with your model
pipe = pipeline(
    "automatic-speech-recognition",
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device=0 if torch.cuda.is_available() else -1,
    chunk_length_s=30,
    stride_length_s=(4, 2),
)

# --- transcribe & print
result = pipe({"array": audio, "sampling_rate": target_sr})
print(result["text"])






# --- deps (no librosa / no soundfile)
import torch
import torchaudio
from transformers import pipeline

# --- your preloaded model & processor
# model = ...
# processor = ...

# target sampling rate from your processor (fallback 16k)
target_sr = getattr(getattr(processor, "feature_extractor", None), "sampling_rate", 16000)

# --- load MP3 with torchaudio
mp3_path = "/appdata/cortex/devl/zknze7ne/whisper_training/sample_for_synthetic.mp3"
wave, sr = torchaudio.load(mp3_path)           # wave: [channels, time], dtype=float32/-1..1

# mono
if wave.size(0) > 1:
    wave = wave.mean(dim=0, keepdim=True)

# resample if needed
if sr != target_sr:
    resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)
    wave = resampler(wave)

# to 1D numpy for the pipeline
audio = wave.squeeze(0).cpu().numpy()

# --- build ASR pipeline with your model
pipe = pipeline(
    "automatic-speech-recognition",
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device=0 if torch.cuda.is_available() else -1,
    chunk_length_s=30,
    stride_length_s=(4, 2),
)

# --- transcribe & print
result = pipe({"array": audio, "sampling_rate": target_sr})
print(result["text"])






import torch
import librosa
from transformers import pipeline

# Your own model and processor (already loaded in your environment)
# model = ...
# processor = ...

target_sr = getattr(getattr(processor, "feature_extractor", None), "sampling_rate", 16000)

# Load MP3 using audioread backend (no soundfile needed)
mp3_path = "/appdata/cortex/devl/zknze7ne/whisper_training/sample_for_synthetic.mp3"
audio, sr = librosa.load(mp3_path, sr=target_sr, mono=True, backend='audioread')

pipe = pipeline(
    "automatic-speech-recognition",
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    torch_dtype=torch.float16,
    device=0 if torch.cuda.is_available() else -1
)

# Run inference and print text
result = pipe({"array": audio, "sampling_rate": target_sr})
print(result["text"])





# --- deps
import torch
import librosa
from transformers import pipeline

# --- your existing model/processor (keep what you already have)
# model = <your loaded model>
# processor = <your loaded processor>  # with .tokenizer and .feature_extractor

# pick the model's expected sampling rate if available, else default to 16k
target_sr = getattr(getattr(processor, "feature_extractor", None), "sampling_rate", 16000)

# --- load mp3 WITHOUT soundfile
mp3_path = "/appdata/cortex/devl/zknze7ne/whisper_training/sample_for_synthetic.mp3"
audio, sr = librosa.load(mp3_path, sr=target_sr, mono=True)  # resamples to target_sr

# --- build ASR pipeline with your model
pipe = pipeline(
    task="automatic-speech-recognition",
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    torch_dtype=torch.float16,
    device=0 if torch.cuda.is_available() else -1,
    chunk_length_s=30,        # handles long audio
    stride_length_s=(4, 2),   # cross-chunk context
)

# --- run & print transcription
out = pipe({"array": audio, "sampling_rate": target_sr})
print(out["text"])









#

import librosa
from transformers import pipeline
import torch

path = "/appdata/cortex/dev1/03-shell_carp_on_it_if_you_let_her.mp3"

try:
    # force librosa to avoid soundfile (we removed it) and use audioread
    audio, sr = librosa.load(path, sr=16000, mono=True)
except Exception as e:
    raise RuntimeError(
        "MP3 decoding failed because this environment has no MP3 backend "
        "(no ffmpeg/sox/gstreamer). Convert the file to WAV outside this "
        "environment and try again."
    ) from e

asr = pipeline(
    "automatic-speech-recognition",
    model="openai/whisper-small",
    device=0 if torch.cuda.is_available() else -1,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
)

print(asr({"array": audio, "sampling_rate": sr})["text"])







Install system + Python dependencies from inside Jupyter
!apt-get update -y && apt-get install -y libsndfile1 ffmpeg
!pip install -U librosa audioread soundfile

import librosa
from transformers import pipeline
import torch

# Load MP3 (mono, 16 kHz)
mp3_path = "/appdata/cortex/dev1/kzhne7me/whisper_training/sample_for_synthetic.mp3"
audio, sr = librosa.load(mp3_path, sr=16000, mono=True)

# Whisper ASR pipeline
asr = pipeline(
    "automatic-speech-recognition",
    model="openai/whisper-small",  # or your fine-tuned model
    device=0 if torch.cuda.is_available() else -1,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
)

result = asr({"array": audio, "sampling_rate": sr})
print(result["text"])




import librosa, audioread, subprocess, sys
print("librosa:", librosa.__version__, "at", librosa.__file__)
print("ffmpeg:", subprocess.check_output(["ffmpeg","-version"]).decode().splitlines()[0])

import librosa
from transformers import pipeline
import torch

# --- Load one MP3 (mono, 16 kHz). Change path to your file.
path = "/appdata/cortex/dev1/kzhne7me/whisper_training/sample_for_synthetic.mp3"
aud, sr = librosa.load(path, sr=16000, mono=True)

# --- If you already created 'pipe' earlier, use it directly:
# result = pipe({"array": aud, "sampling_rate": 16000})

# Otherwise create a quick pipeline here (GPU if available):
pipe = pipeline(
    "automatic-speech-recognition",
    model="openai/whisper-small",              # or your fine-tuned model path
    device=0 if torch.cuda.is_available() else -1,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
)

result = pipe({"array": aud, "sampling_rate": 16000})
print(result["text"])



import librosa
from transformers import pipeline

# 1) Load MP3 with librosa (mono, 16k, optional segment)
path = "/appdata/cortex/dev1/03-shell_carp_on_it_if_you_let_her.mp3"
y, sr = librosa.load(path, sr=16000, mono=True, offset=0.0, duration=None)

# 2) Run ASR
asr = pipeline(
    "automatic-speech-recognition",
    model="openai/whisper-small",
    device=0,                    # set to -1 if no GPU
    torch_dtype="auto"
)

result = asr({"array": y, "sampling_rate": sr})
print(result["text"])


import torch
import torchaudio
from torchaudio.transforms import Resample
from transformers import pipeline

# Load MP3 (mono + resample to 16k for ASR)
path = "/appdata/cortex/dev1/origAudio/03-shell_carp_on_it_if_you_let_her.mp3"
waveform, sr = torchaudio.load(path)
if waveform.size(0) > 1:
    waveform = waveform.mean(dim=0, keepdim=True)
if sr != 16000:
    waveform = Resample(sr, 16000)(waveform)
waveform = waveform.squeeze(0).to(torch.float32)

# ASR pipeline
asr = pipeline("automatic-speech-recognition",
               model="openai/whisper-small",
               device=0 if torch.cuda.is_available() else -1,
               torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)

# Transcribe
result = asr({"array": waveform.numpy(), "sampling_rate": 16000})
print(result["text"])





import audioread
import numpy as np

def read_mp3(filename):
    with audioread.audio_open(filename) as f:
        sr = f.samplerate
        channels = f.channels
        audio_data = b''.join([buf for buf in f])
        audio_np = np.frombuffer(audio_data, dtype=np.int16)
        audio_np = audio_np.reshape(-1, channels)
        return audio_np, sr

waveform, sr = read_mp3("your_file.mp3")
print(waveform.shape, sr)



from pydub import AudioSegment
import io
import torchaudio

# Load MP3 into memory and convert to WAV
mp3_path = "/appdata/cortex/dev1/03-shell_carp_on_it_if_you_let_her.mp3"
audio = AudioSegment.from_mp3(mp3_path)

# Export to an in-memory buffer as WAV
buf = io.BytesIO()
audio.export(buf, format="wav")
buf.seek(0)

# Now load the WAV buffer with torchaudio
waveform, sr = torchaudio.load(buf)
print(waveform.shape, sr)


import torchaudio

# Set backend to sox_io (usually has MP3 support if torchaudio is installed correctly)
torchaudio.set_audio_backend("sox_io")

# Load MP3
aud, sr = torchaudio.load("/appdata/cortex/dev1/03-shell_carp_on_it_if_you_let_her.mp3")
print(aud.shape, sr)




import torchaudio

aud, sr = torchaudio.load("/appdata/cortex/dev1/03_shell_carp_on_it_if_you_let_her.mp3")

from transformers import pipeline
import torchaudio

# Load audio without librosa
aud, sr = torchaudio.load("/appdata/cortex/dev1/03_shell_carp_on_it_if_you_let_her.mp3")

pipe = pipeline(
    "automatic-speech-recognition",
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    batch_size=1,
    torch_dtype=torch.float16,
    device="cuda",
)

# Run inference
print(pipe(aud.numpy()))




print(tabulate(self.history, headers="keys"))


from transformers import TrainerCallback
from tabulate import tabulate

class LossWERCallback(TrainerCallback):
    def __init__(self):
        self.history = []

    def on_log(self, args, state, control, logs=None, **kwargs):
        if "loss" in logs:
            self.history.append({
                "Step": state.global_step,
                "Training Loss": round(logs["loss"], 4),
                "Validation Loss": None,
                "WER": None
            })

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if self.history and metrics:
            self.history[-1]["Validation Loss"] = round(metrics.get("eval_loss", 0), 4)
            self.history[-1]["WER"] = round(metrics.get("eval_wer", 0), 4)

        # Print table each evaluation
        print(tabulate(self.history, headers="keys"))

# Attach callback
trainer.add_callback(LossWERCallback())



from transformers import TrainerCallback
from tabulate import tabulate

class LossWERCallback(TrainerCallback):
    def __init__(self):
        self.history = []

    def on_log(self, args, state, control, logs=None, **kwargs):
        if "loss" in logs:
            self.history.append({
                "Step": state.global_step,
                "Training Loss": round(logs["loss"], 4),
                "Validation Loss": None,
                "WER": None
            })

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if self.history and metrics:
            self.history[-1]["Validation Loss"] = round(metrics.get("eval_loss", 0), 4)
            self.history[-1]["WER"] = round(metrics.get("eval_wer", 0), 4)

        # Print table each evaluation
        print(tabulate(self.history, headers="keys"))

# Attach callback
trainer.add_callback(LossWERCallback())






from transformers.trainer_callback import TrainerCallback

class SolidMetricsTable(TrainerCallback):
    def __init__(self):
        self._printed_header = False

    def _print_header_once(self):
        if not self._printed_header:
            hdr = f"{'Step':>8} | {'Train Loss':>12} | {'Val Loss':>10} | {'WER %':>7}"
            print(hdr); print("-"*len(hdr))
            self._printed_header = True

    def _latest_train_loss(self, state, step):
        last = None
        for e in state.log_history:
            if "loss" in e and int(e.get("step", step)) <= step:
                last = float(e["loss"])
        return last

    def on_evaluate(self, args, state, control, metrics=None, **kw):
        if not metrics: return
        step = int(state.global_step)
        train_loss = self._latest_train_loss(state, step)
        val_loss   = float(metrics.get("eval_loss", metrics.get("loss", float("nan"))))
        wer        = float(metrics.get("eval_wer", metrics.get("wer", float("nan"))))
        self._print_header_once()
        tl = "-" if train_loss is None else f"{train_loss:.4f}"
        vl = "-" if val_loss != val_loss else f"{val_loss:.4f}"   # NaN check
        wr = "-" if wer      != wer      else f"{wer:.2f}"
        print(f"{step:8d} | {tl:>12} | {vl:>10} | {wr:>7}")

trainer.add_callback(SolidMetricsTable())
trainer.train()







# --- Live table of Step | Train Loss | Val Loss | WER during trainer.train() ---

from transformers.trainer_callback import TrainerCallback

class LiveMetricsTable(TrainerCallback):
    def __init__(self):
        # step -> {'step':int, 'train':float|None, 'val':float|None, 'wer':float|None}
        self.rows = {}
        self._printed_header = False

    def _row(self, step):
        step = int(step)
        if step not in self.rows:
            self.rows[step] = {"step": step, "train": None, "val": None, "wer": None}
        return self.rows[step]

    def _print_header_once(self):
        if not self._printed_header:
            hdr = f"{'Step':>8} | {'Train Loss':>12} | {'Val Loss':>10} | {'WER %':>7}"
            print(hdr)
            print("-" * len(hdr))
            self._printed_header = True

    # logs from training (contains 'loss')
    def on_log(self, args, state, control, logs=None, **kwargs):
        if not logs or "loss" not in logs:
            return
        row = self._row(state.global_step)
        row["train"] = float(logs["loss"])

    # metrics from evaluation (contains eval_loss and eval_wer/wer)
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if not metrics:
            return
        row = self._row(state.global_step)

        # HF sometimes prefixes with "eval_"
        if "eval_loss" in metrics:
            row["val"] = float(metrics["eval_loss"])
        elif "loss" in metrics:
            row["val"] = float(metrics["loss"])

        if "eval_wer" in metrics:
            row["wer"] = float(metrics["eval_wer"])
        elif "wer" in metrics:
            row["wer"] = float(metrics["wer"])

        # print the row when we have eval results
        self._print_header_once()
        tl = f"{row['train']:.4f}" if row["train"] is not None else "-"
        vl = f"{row['val']:.4f}"   if row["val"]   is not None else "-"
        wr = f"{row['wer']:.2f}"   if row["wer"]   is not None else "-"
        print(f"{row['step']:8d} | {tl:>12} | {vl:>10} | {wr:>7}")

# attach the callback
live = LiveMetricsTable()
trainer.add_callback(live)

# IMPORTANT: make sure eval actually runs during training
# (you likely already set these in your TrainingArguments)
# evaluation_strategy="steps", eval_steps=50,
# logging_strategy="steps", logging_steps=25,
# predict_with_generate=True, remove_unused_columns=False

# train and still get a final table after training
trainer.train()

# optional: print a final consolidated table sorted by step
if live.rows:
    print("\nFinal summary:")
    hdr = f"{'Step':>8} | {'Train Loss':>12} | {'Val Loss':>10} | {'WER %':>7}"
    print(hdr); print("-"*len(hdr))
    for s in sorted(live.rows):
        r = live.rows[s]
        tl = f"{r['train']:.4f}" if r["train"] is not None else "-"
        vl = f"{r['val']:.4f}"   if r["val"]   is not None else "-"
        wr = f"{r['wer']:.2f}"   if r["wer"]   is not None else "-"
        print(f"{s:8d} | {tl:>12} | {vl:>10} | {wr:>7}")









from transformers import TrainerCallback
from tabulate import tabulate

class LossWERCallback(TrainerCallback):
    def __init__(self):
        self.history = []

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is None:
            return
        # Capture training loss if available
        if "loss" in logs:
            self.history.append({
                "Step": state.global_step,
                "Training Loss": round(logs["loss"], 4),
                "Validation Loss": None,
                "WER": None
            })

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics is None:
            return
        # Update last record with eval metrics
        if self.history:
            self.history[-1]["Validation Loss"] = round(metrics.get("eval_loss", 0), 4)
            self.history[-1]["WER"] = round(metrics.get("eval_wer", 0), 4)
        
        # Print table
        print(tabulate(self.history, headers="keys"))

# Attach the callback to your trainer
trainer.add_callback(LossWERCallback())

# Train
trainer.train()






# ==== SETTINGS (edit these) ===============================================
SRC_DIR     = "/appdata/cortex/dev1/OrigAudio"   # where your MP3 files live
WHISPER_DIR = "/appdata/cortex/dev1/whisper_copy/distil-large-v2"  # or any HF id
SEED        = 91
# ==========================================================================

import os, glob, re, subprocess, numpy as np, random
from pathlib import Path
from datasets import Dataset, DatasetDict
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer
from dataclasses import dataclass
from typing import Any, Dict, List, Union
import torch

# ---------- 1) collect files + lightweight transcripts ----------
files = sorted(set(glob.glob(f"{SRC_DIR}/**/*.mp3", recursive=True) +
                   glob.glob(f"{SRC_DIR}/**/*.MP3", recursive=True)))
if not files:
    raise RuntimeError(f"No MP3 files found under {SRC_DIR}")

def fn_to_text(p: str) -> str:
    b = os.path.basename(p)
    if b.lower().endswith(".mp3"): b = b[:-4]
    # strip a leading "NNN-" if present
    i = b.find("-")
    s = b[i+1:] if i != -1 else b
    s = (s.replace("_"," ").replace("."," ").strip()
         .replace("dont","don't").replace("shell","she'll"))
    s = s[0:1].upper() + s[1:] if s else s
    s = s.replace(" i ", " I ")
    return s

transcripts = [fn_to_text(p) for p in files]
raw = Dataset.from_dict({"audio_path": files, "sentence": transcripts}).with_format("python")

# ---------- 2) split 80/20 ----------
raw_datasets = raw.train_test_split(test_size=0.20, seed=SEED)
print(raw_datasets)

# ---------- 3) tiny audio loader (NO librosa/soundfile) ----------
def load_audio_ffmpeg(path: str, target_sr: int = 16000):
    """
    Decodes with the ffmpeg CLI (no Python libs). Make sure 'ffmpeg' is on PATH.
    """
    cmd = ["ffmpeg","-v","error","-i",path,"-f","f32le","-ac","1","-ar",str(target_sr),"pipe:1"]
    out = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
    y = np.frombuffer(out.stdout, dtype=np.float32)
    return y, target_sr

# ---------- 4) processor + model ----------
processor = WhisperProcessor.from_pretrained(WHISPER_DIR, language="English", task="transcribe")
model = WhisperForConditionalGeneration.from_pretrained(WHISPER_DIR)
model.config.forced_decoder_ids = None
model.config.suppress_tokens = []
model.config.use_cache = False  # better with gradient checkpointing
model.config.language = "english"
model.config.task = "transcribe"

# ---------- 5) map: turn path -> input_features + labels ----------
def prepare_example(batch):
    y, _ = load_audio_ffmpeg(batch["audio_path"], target_sr=16000)
    batch["input_features"] = processor.feature_extractor(y, sampling_rate=16000).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

# keep single process to avoid re-importing audio stack
vectorized = raw_datasets.map(
    prepare_example,
    remove_columns=raw_datasets["train"].column_names,  # ['audio_path','sentence']
    num_proc=1,
    desc="Preparing dataset",
)

# ---------- 6) simple data collator ----------
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    decoder_start_token_id: int

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_features": f["input_features"]} for f in features]
        label_features = [{"input_ids": f["labels"]} for f in features]

        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
        # drop BOS if your tokenizer adds it; Whisper usually adds BOS later in generate()
        if labels.shape[1] > 0 and (labels[:, 0] == self.decoder_start_token_id).all().item():
            labels = labels[:, 1:]
        batch["labels"] = labels
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(
    processor=processor,
    decoder_start_token_id=model.config.decoder_start_token_id
)

# ---------- 7) pure-Python WER (no 'evaluate') ----------
def _levenshtein(a, b):
    m, n = len(a), len(b)
    dp = list(range(n + 1))
    for i in range(1, m + 1):
        prev, dp[0] = dp[0], i
        for j in range(1, n + 1):
            prev, dp[j] = dp[j], min(dp[j] + 1, dp[j-1] + 1, prev + (a[i-1] != b[j-1]))
    return dp[n]

def compute_metrics(pred):
    preds = pred.predictions[0] if isinstance(pred.predictions, tuple) else pred.predictions
    labels = pred.label_ids.copy()
    pad_id = processor.tokenizer.pad_token_id
    labels[labels == -100] = pad_id

    hyp = processor.tokenizer.batch_decode(preds,  skip_special_tokens=True)
    ref = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)

    total_err, total_words = 0, 0
    for r, h in zip(ref, hyp):
        r_w, h_w = r.strip().split(), h.strip().split()
        total_err  += _levenshtein(r_w, h_w)
        total_words += max(1, len(r_w))
    wer = 100.0 * total_err / total_words if total_words else 0.0
    return {"wer": wer}

# ---------- 8) training args (ensure eval runs & prints) ----------
args = Seq2SeqTrainingArguments(
    output_dir="./asr_out",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=1e-5,
    max_steps=1000,                    # or set num_train_epochs=3
    fp16=torch.cuda.is_available(),
    evaluation_strategy="steps",       # <-- will evaluate during training
    eval_steps=50,
    logging_strategy="steps",
    logging_steps=25,
    predict_with_generate=True,        # needed for decoding
    generation_max_length=225,
    remove_unused_columns=False,
    metric_for_best_model="wer",
    greater_is_better=False,
    load_best_model_at_end=True,
    report_to=[],                      # no wandb/tensorboard needed
    seed=SEED,
)

# ---------- 9) trainer + print-eval callback ----------
from transformers.trainer_callback import TrainerCallback
class PrintEvalCallback(TrainerCallback):
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics:
            nice = {k: round(float(v), 4) for k, v in metrics.items() if isinstance(v, (int, float))}
            print(f"\n[Eval @ step {state.global_step}] {nice}")

trainer = Seq2SeqTrainer(
    model=model,
    args=args,
    train_dataset=vectorized["train"],
    eval_dataset=vectorized["test"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor,
)
trainer.add_callback(PrintEvalCallback())

# ---------- 10) train + final evaluate ----------
print("train size:", len(vectorized["train"]), " test size:", len(vectorized["test"]))
trainer.train()
final_metrics = trainer.evaluate()
print("\nFinal evaluation:", {k: round(float(v), 4) for k, v in final_metrics.items()})




from transformers.trainer_callback import TrainerCallback

class PrintEvalCallback(TrainerCallback):
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics:
            nice = {k: round(float(v), 4) for k, v in metrics.items() if isinstance(v, (int, float))}
            print(f"\n[Eval @ step {state.global_step}] {nice}")

trainer.add_callback(PrintEvalCallback())

# --- pure-Python word error rate (no dependencies) ---
def _levenshtein(a, b):
    # a, b: lists of tokens
    m, n = len(a), len(b)
    dp = list(range(n + 1))
    for i in range(1, m + 1):
        prev, dp[0] = dp[0], i
        for j in range(1, n + 1):
            prev, dp[j] = dp[j], min(
                dp[j] + 1,            # deletion
                dp[j-1] + 1,          # insertion
                prev + (a[i-1] != b[j-1])  # substitution
            )
    return dp[n]

def compute_metrics(pred):
    # handle HF returning tuple for logits
    preds = pred.predictions[0] if isinstance(pred.predictions, tuple) else pred.predictions
    labels = pred.label_ids

    # replace -100 with pad so we can decode labels
    pad_id = processor.tokenizer.pad_token_id
    labels = labels.copy()
    labels[labels == -100] = pad_id

    pred_str  = processor.tokenizer.batch_decode(preds,  skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)

    # compute corpus WER
    total_err, total_words = 0, 0
    for ref, hyp in zip(label_str, pred_str):
        ref_words = ref.strip().split()
        hyp_words = hyp.strip().split()
        total_err  += _levenshtein(ref_words, hyp_words)
        total_words += max(1, len(ref_words))
    wer = 100.0 * total_err / total_words
    return {"wer": wer}





# 1) Make sure the 'audio' column never decodes
from datasets import Audio
raw_datasets = raw_datasets.cast_column("audio", Audio(decode=False))

print(raw_datasets["train"].features)  # should show Audio(..., decode=False)
# Access will now return a dict with a 'path' key; no decoding happens.



def to_path(ex):
    return {"audio_path": ex["audio"]["path"]}

raw_datasets = raw_datasets.map(to_path, remove_columns=["audio"])
print(raw_datasets["train"].features)  # 'audio_path': Value('string')



from pathlib import Path
import shutil

# ===== settings =====
OUT = Path("/appdata/cortex/dev1/export_split")  # where to save train/ and test/
PRESERVE_TREE = True                              # keep source subfolders
COMMON_ROOT = Path("/appdata/cortex/dev1/OrigAudio")  # used only if PRESERVE_TREE
# ====================

def _src_path(row):
    # works whether your dataset has 'audio_path' OR 'audio' (Audio(decode=False))
    return row.get("audio_path") or row["audio"]["path"]

def _save_split(split_name: str):
    out_dir = OUT / split_name
    out_dir.mkdir(parents=True, exist_ok=True)

    for row in raw_datasets[split_name]:
        src = Path(_src_path(row))
        if PRESERVE_TREE and COMMON_ROOT in src.parents:
            dst = out_dir / src.relative_to(COMMON_ROOT)   # keep subfolder structure
            dst.parent.mkdir(parents=True, exist_ok=True)
        else:
            dst = out_dir / src.name                       # flat folder; keep filename
        if not dst.exists():
            shutil.copy2(src, dst)

# run
_save_split("train")
_save_split("test")
print(f"Saved: {OUT / 'train'} and {OUT / 'test'}")


FOLDER = "/appdata/cortex/dev1/OrigAudio/one_particular_folder"  # or leave as "" to list all

def get_path(row):
    return row["audio_path"] if "audio_path" in row else row["audio"]["path"]

train_paths = [p for p in (get_path(r) for r in raw_datasets["train"]) if p.startswith(FOLDER)]
test_paths  = [p for p in (get_path(r) for r in raw_datasets["test"])  if p.startswith(FOLDER)]

print("=== TRAIN ==="); print("\n".join(train_paths) or "(none)")
print("\n=== TEST ===");  print("\n".join(test_paths)  or "(none)")



import os

FOLDER = "/appdata/cortex/dev1/OrigAudio/one_particular_folder"  # <- change me

def get_path(row):
    # works for either schema: {'audio_path': ...} OR {'audio': {'path': ...}}
    return row["audio_path"] if "audio_path" in row else row["audio"]["path"]

train_list = [get_path(r) for r in raw_datasets["train"] if get_path(r).startswith(FOLDER)]
test_list  = [get_path(r) for r in raw_datasets["test"]  if get_path(r).startswith(FOLDER)]

print("=== TRAIN ===")
print("\n".join(train_list) or "(none)")
print("\n=== TEST ===")
print("\n".join(test_list) or "(none)")





# 0) Build the base dataset called `raw`
from pathlib import Path
from datasets import Dataset
import glob, os, re

SRC = "/appdata/cortex/dev1/OrigAudio"   # <- your mp3 root

# list all mp3 paths
audio_files = sorted(set(
    glob.glob(f"{SRC}/**/*.mp3", recursive=True) +
    glob.glob(f"{SRC}/**/*.MP3", recursive=True)
))

# if you already have `transcripts`, keep it; else derive from filename:
def retrieve_transcript(p: str) -> str:
    b = os.path.basename(p)
    if b.lower().endswith(".mp3"): b = b[:-4]
    d = b.find("-")
    s = b[d+1:] if d != -1 else b
    s = s.replace("_"," ").replace("."," ").replace("dont","don't").replace("shell","she'll")
    return s.capitalize().replace(" i ", " I ")

transcripts = [retrieve_transcript(p) for p in audio_files]

# raw = single Dataset with paths + text (NOT datasets.Audio)
raw = Dataset.from_dict({"audio_path": audio_files, "sentence": transcripts}).with_format("python")
print(raw)  # sanity check




import os, re
from datasets import DatasetDict

# parse leading number like "001-" / "01-" / "139-"
def _lead_num(path: str):
    m = re.match(r"^(\d+)-", os.path.basename(path))
    return int(m.group(1)) if m else None

def _is_test(ex):
    n = _lead_num(ex["audio_path"])
    return (n is not None) and (1 <= n <= 139)

# split with filters (like your photo)
test_ds  = raw.filter(_is_test)
train_ds = raw.filter(lambda ex: not _is_test(ex))

raw_datasets = DatasetDict({"train": train_ds, "test": test_ds})
print(raw_datasets)  # sanity check
# DatasetDict({
#   train: Dataset({ num_rows: ... })
#   test:  Dataset({ num_rows: ... })
# })

# (re)vectorize exactly like you do later
vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets["train"].column_names,  # ['audio_path','sentence']
    num_proc=1,                                         # keep single process
    desc="Preparing dataset",
)


from pathlib import Path
import re, shutil

# Edit these:
SRC  = Path("/appdata/cortex/dev1/OrigAudio")
DEST = Path("/appdata/cortex/dev1/split_mp3")
MOVE = False   # True = move, False = copy

# Collect all MP3s (case-insensitive)
files = [p for p in SRC.rglob("*") if p.suffix.lower() == ".mp3"]

# Helper: parse leading number like "01-" / "139-"
def lead_num(name: str):
    m = re.match(r"^(\d+)-", name)   # start-of-name, digits, dash
    return int(m.group(1)) if m else None

# Select test = names with leading number in [1..139]
test = [p for p in files if (lambda n: n is not None and 1 <= n <= 139)(lead_num(p.name))]
test_set = set(test)
train = [p for p in files if p not in test_set]

# Make dirs
(TRAIN := DEST / "train").mkdir(parents=True, exist_ok=True)
(TEST  := DEST / "test").mkdir(parents=True, exist_ok=True)

# Copy/move, preserving subfolders
op = shutil.move if MOVE else shutil.copy2
for name, subset in (("test", test), ("train", train)):
    out_root = DEST / name
    for src in subset:
        dst = out_root / src.relative_to(SRC)
        dst.parent.mkdir(parents=True, exist_ok=True)
        op(src, dst)

print(f"Done. train={len(train)}  test={len(test)}  -> {DEST}")




import os

TO_EXCLUDE = "03-shell-carp.mp3"

def keep_row(ex):
    return os.path.basename(ex["audio_path"]) != TO_EXCLUDE

# remove it from both splits
raw_datasets = raw_datasets.filter(keep_row)

# (re)vectorize
vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,
    desc="Preparing dataset",
)

# then:
trainer.train()

from datasets import Dataset

raw = Dataset.from_dict({
    "audio_path": audio_files,      # <-- string paths, not Audio(...)
    "sentence":   transcripts
})
raw_datasets = raw.train_test_split(test_size=0.15, seed=91).with_format("python")

# pip install miniaudio numpy
import numpy as np, miniaudio

def load_mp3(path: str, target_sr: int = 16000):
    sb = miniaudio.decode_file(path)                      # floats in [-1,1]
    sr_native = getattr(sb, "sample_rate", 16000)
    ch        = getattr(sb, "channels",    1)
    y = np.asarray(sb.samples, dtype=np.float32)
    if ch > 1:
        y = y.reshape(-1, ch).mean(axis=1)               # stereo -> mono
    if sr_native != target_sr:
        n_new = int(len(y) * target_sr / sr_native)
        y = np.interp(
            np.linspace(0, len(y), n_new, endpoint=False),
            np.arange(len(y), dtype=np.float32),
            y.astype(np.float32)
        ).astype(np.float32)
    return y, target_sr



from transformers import WhisperProcessor

# make sure whisper_loc is set to your model folder or HF id
processor = WhisperProcessor.from_pretrained(whisper_loc, language="English", task="transcribe")

def prepare_dataset(batch):
    path = batch["audio_path"]                      # <-- use string path
    y, _ = load_mp3(path, target_sr=16000)          # miniaudio decoder
    batch["input_features"] = processor.feature_extractor(
        y, sampling_rate=16000
    ).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,                                     # keep single process
    desc="Preparing dataset",
)

import numpy as np, miniaudio

def load_mp3_miniaudio(path: str, target_sr: int = 16000):
    sb = miniaudio.decode_file(path)   # decodes to floats in [-1, 1]
    # be robust to field names
    sr_native = getattr(sb, "sample_rate", getattr(sb, "samplerate", 16000))
    ch        = getattr(sb, "channels",    getattr(sb, "nchannels", 1))
    y = np.asarray(sb.samples, dtype=np.float32)

    # stereo -> mono
    if ch and ch > 1:
        y = y.reshape(-1, ch).mean(axis=1)

    # simple resample to 16k if needed
    if sr_native != target_sr:
        n_new = int(len(y) * target_sr / sr_native)
        y = np.interp(
            np.linspace(0, len(y), n_new, endpoint=False),
            np.arange(len(y), dtype=np.float32),
            y.astype(np.float32)
        ).astype(np.float32)

    return y, target_sr

from transformers import WhisperProcessor

# make sure whisper_loc is defined earlier (local folder or HF id)
processor = WhisperProcessor.from_pretrained(whisper_loc, language="English", task="transcribe")

def prepare_dataset(batch):
    # We stored paths only, so read from disk here:
    path = batch["audio"]["path"]
    y, _ = load_mp3_miniaudio(path, target_sr=16000)

    batch["input_features"] = processor.feature_extractor(
        y, sampling_rate=16000
    ).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

# IMPORTANT: keep a single process so workers donâ€™t re-import audio stacks
vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,                          # â† this avoids backend/import issues
    desc="Preparing dataset",
)




from datasets import Dataset, Audio

raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}).cast_column(
    "audio", Audio(decode=False)  # store only file paths, don't decode
)
raw_datasets = raw.train_test_split(test_size=0.15, seed=91).with_format("python")

import numpy as np, audioread, resampy

def load_mp3(path: str, target_sr: int = 16000):
    # Decode with audioread (FFmpeg backend)
    with audioread.audio_open(path) as f:
        sr_native = f.samplerate
        ch = f.channels
        pcm = b"".join(chunk for chunk in f)

    # 16-bit PCM -> float32 [-1, 1]
    y = np.frombuffer(pcm, dtype=np.int16).astype(np.float32) / 32768.0

    # stereo -> mono
    if ch and ch > 1:
        y = y.reshape(-1, ch).mean(axis=1)

    # resample if needed
    if sr_native != target_sr:
        y = resampy.resample(y, sr_native, target_sr)

    return y, target_sr

from transformers import WhisperProcessor

# make sure whisper_loc (path or HF id) is defined earlier
processor = WhisperProcessor.from_pretrained(whisper_loc, language="English", task="transcribe")

def prepare_dataset(batch):
    path = batch["audio"]["path"]          # Audio(decode=False) gives you a path
    y, _ = load_mp3(path, target_sr=16000)

    batch["input_features"] = processor.feature_extractor(
        y, sampling_rate=16000
    ).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

# IMPORTANT: keep single process so workers donâ€™t re-import audio backends
vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,
    desc="Preparing dataset",
)
bad = []

def prepare_dataset_safe(batch):
    path = batch["audio"]["path"]
    try:
        y, _ = load_mp3(path, target_sr=16000)
    except Exception as e:
        bad.append((path, str(e)))
        return {"_bad": True}
    return {
        "input_features": processor.feature_extractor(y, sampling_rate=16000).input_features[0],
        "labels": processor.tokenizer(batch["sentence"]).input_ids
    }

vectorized_datasets = raw_datasets.map(
    prepare_dataset_safe,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,
    desc="Preparing dataset",
).filter(lambda ex: not ex.get("_bad", False))

print("Skipped files:", len(bad))
for p, err in bad[:5]:
    print(" -", p, "->", err)




import jiwer

def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # replace -100 with pad_token_id
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id

    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    # compute WER with jiwer
    wer = jiwer.wer(label_str, pred_str) * 100

    return {"wer": wer}



print(vectorized_datasets)
print(vectorized_datasets["train"][0].keys())   # 


should show: dict_keys(['input_features','labels'])




from datasets.features.audio import set_audio_backend
set_audio_backend("librosa")   # put this BEFORE you call .cast_column(...)


pip install librosa audioread
# (ffmpeg is recommended so audioread can decode mp3)

from datasets import Dataset, Audio
from datasets.features.audio import set_audio_backend
set_audio_backend("librosa")

raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}) \
             .cast_column("audio", Audio(sampling_rate=16_000))



# ---- put this at the very top of the notebook ----
import sys, types
sf_stub = types.ModuleType("soundfile")
sf_stub.__spec__ = None
sf_stub.libsndfile_version = "0.0.0"  # forces features that require libsndfile to be disabled
sys.modules["soundfile"] = sf_stub

# deps purely in Python
%pip install -q librosa audioread

# now it's safe to import datasets without crashing
from datasets import Dataset, Audio
import librosa

raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}) \
             .cast_column("audio", Audio(decode=False))
raw_datasets = raw.train_test_split(test_size=0.15, seed=91)

def prepare_dataset(batch):
    path = batch["audio"]["path"]
    array, sr = librosa.load(path, sr=16000, mono=True)
    batch["input_features"] = processor.feature_extractor(array, sampling_rate=16000).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=8
)



# ---- put this at the very top of the notebook, before any datasets import ----
import sys, types, importlib, importlib.machinery, importlib.util

# create a stub module with a valid spec and the attribute datasets probes
sf_stub = types.ModuleType("soundfile")
sf_stub.__dict__.update({
    "__spec__": importlib.machinery.ModuleSpec("soundfile", loader=None),
    "libsndfile_version": "0.0.0",   # make the probe evaluate safely
})
sys.modules["soundfile"] = sf_stub

# ensure find_spec("soundfile") returns a spec instead of raising
_orig_find_spec = importlib.util.find_spec
def _fake_find_spec(name, package=None):
    if name == "soundfile":
        return importlib.machinery.ModuleSpec("soundfile", loader=None)
    return _orig_find_spec(name, package)
importlib.util.find_spec = _fake_find_spec

# optional: use librosa for decoding later
%pip install -q librosa audioread

from datasets import Dataset, Audio
import librosa

# build dataset WITHOUT decoding; decode with librosa in map()
raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}).cast_column(
    "audio", Audio(decode=False)
)

def prepare_dataset(batch):
    path = batch["audio"]["path"]
    array, sr = librosa.load(path, sr=16000, mono=True)
    batch["input_features"] = processor.feature_extractor(array, sampling_rate=16000).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch



# --- put this in the VERY FIRST cell, before any `datasets` import ---
import sys, types, importlib, importlib.machinery, importlib.util

# create a stub that satisfies datasets' probes
sf_stub = types.ModuleType("soundfile")
sf_stub.__spec__ = importlib.machinery.ModuleSpec("soundfile", loader=None)
sf_stub.__libsndfile_version__ = "0.0.0"   # <-- exact name expected by datasets
sf_stub.libsndfile_version = "0.0.0"       # (some versions check this too)
sys.modules["soundfile"] = sf_stub

# ensure find_spec("soundfile") returns a spec
_orig_find_spec = importlib.util.find_spec
def _fake_find_spec(name, package=None):
    if name == "soundfile":
        return importlib.machinery.ModuleSpec("soundfile", loader=None)
    return _orig_find_spec(name, package)
importlib.util.find_spec = _fake_find_spec

# optional pure-Python decoders
%pip install -q librosa audioread

from datasets import Dataset, Audio
import librosa

# build WITHOUT decoding; decode in map()
raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}) \
             .cast_column("audio", Audio(decode=False))

def prepare_dataset(batch):
    path = batch["audio"]["path"]
    array, _ = librosa.load(path, sr=16000, mono=True)
    batch["input_features"] = processor.feature_extractor(array, sampling_rate=16000).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch


# conda (recommended)
conda install -c conda-forge libsndfile ffmpeg soundfile librosa audioread
# --- STUB SOUNDFILE (no libsndfile needed) ---
import sys, types, importlib, importlib.machinery, importlib.util

# if a previous bad stub exists, remove it
sys.modules.pop("soundfile", None)

# make a minimal stub with the attributes datasets probes
sf_stub = types.ModuleType("soundfile")
sf_stub.__spec__ = importlib.machinery.ModuleSpec("soundfile", loader=None)
sf_stub.__file__ = "<soundfile-stub>"
sf_stub.__package__ = ""
sf_stub.__libsndfile_version__ = "0.0.0"   # <â€” EXACT NAME REQUIRED
sf_stub.libsndfile_version = "0.0.0"       # some versions also check this
sys.modules["soundfile"] = sf_stub

# make importlib.util.find_spec('soundfile') return a spec (not None)
_orig_find_spec = importlib.util.find_spec
def _safe_find_spec(name, package=None):
    if name == "soundfile":
        return sf_stub.__spec__
    return _orig_find_spec(name, package)
importlib.util.find_spec = _safe_find_spec

# --- SOUNDFILE STUB: must run before importing `datasets` ---
import sys, types, importlib, importlib.machinery, importlib.util

# remove any previously loaded soundfile
sys.modules.pop("soundfile", None)

# create stub with the exact attributes `datasets` expects
_sf = types.ModuleType("soundfile")
_sf.__spec__ = importlib.machinery.ModuleSpec("soundfile", loader=None)
_sf.__file__ = "<soundfile-stub>"
_sf.__package__ = ""
_sf.__libsndfile_version__ = "0.0.0"   # EXACT attribute name required
_sf.libsndfile_version = "0.0.0"       # some versions also check this
sys.modules["soundfile"] = _sf

# keep original find_spec, then patch safely (to avoid recursion)
_orig_find_spec = importlib.util.find_spec
def _safe_find_spec(name, package=None):
    if name == "soundfile":
        return _sf.__spec__
    return _orig_find_spec(name, package)
importlib.util.find_spec = _safe_find_spec

# quick self-check
import importlib as _il
mod = _il.import_module("soundfile")
assert hasattr(mod, "__libsndfile_version__"), "stub missing __libsndfile_version__"





raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}) \
             .cast_column("audio", Audio(decode=False))

raw_datasets = raw.train_test_split(test_size=0.15, seed=91)

def prepare_dataset(batch):
    path = batch["audio"]["path"]
    array, _ = librosa.load(path, sr=16000, mono=True)
    batch["input_features"] = processor.feature_extractor(array, sampling_rate=16000).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch



from datasets import Dataset, Audio
import librosa

# 1) Build WITHOUT decoding (very important)
raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}).cast_column(
    "audio", Audio(decode=False)
)

# 2) Split and force plain Python format so printing doesnâ€™t decode
raw_datasets = raw.train_test_split(test_size=0.15, seed=91)
raw_datasets = raw_datasets.with_format("python")


# Donâ€™t do: [r["sentence"] for r in raw_datasets["test"]]  # triggers decoding
print(raw_datasets["test"]["sentence"][:10])                # SAFE (column access)

# or drop the audio column before looking at rows:
print(raw_datasets["test"].remove_columns("audio")[:3])     # SAFE

def prepare_dataset(batch):
    path = batch["audio"]["path"]                     # path only; not decoded
    array, _ = librosa.load(path, sr=16000, mono=True)
    batch["input_features"] = processor.feature_extractor(
        array, sampling_rate=16000
    ).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=8
)

print(vectorized_datasets["train"][0].keys())  # should be dict_keys(['input_features','labels'])





import numpy as np, audioread, resampy

def load_audio_ar(path: str, sr: int = 16000) -> tuple[np.ndarray, int]:
    """Decode audio via audioread (FFmpeg/MediaFoundation/CoreAudio),
    convert to mono float32 in [-1,1], and resample to `sr`."""
    with audioread.audio_open(path) as f:
        sr_native = f.samplerate
        ch = f.channels

        # concatenate raw PCM chunks
        pcm = b"".join(b for b in f)

    # audioread gives 16-bit PCM for MP3/WAV â†’ int16 â†’ float32
    y = np.frombuffer(pcm, dtype=np.int16).astype(np.float32) / 32768.0

    # deinterleave â†’ mono
    if ch > 1:
        y = y.reshape(-1, ch).mean(axis=1)

    # resample if needed
    if sr_native != sr:
        y = resampy.resample(y, sr_native, sr)

    return y, sr

def prepare_dataset(batch):
    path = batch["audio"]["path"]                 # path only; not decoded
    array, _ = load_audio_ar(path, sr=16000)      # <-- use audioread loader
    batch["input_features"] = processor.feature_extractor(
        array, sampling_rate=16000
    ).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

from datasets import Dataset, Audio
raw = Dataset.from_dict({"audio": audio_files, "sentence": transcripts}).cast_column(
    "audio", Audio(decode=False)
)
raw_datasets = raw.train_test_split(test_size=0.15, seed=91).with_format("python")

# Also avoid multiprocessing (workers re-import audio stacks):
vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,                      # <- important
    desc="Preparing dataset"
)



import subprocess; subprocess.run(["ffmpeg","-version"])



# --- pick ONE loader ---

# If you have FFmpeg installed (recommended):
import numpy as np, audioread, resampy

def load_audio(path, sr=16000):
    with audioread.audio_open(path) as f:
        sr_native = f.samplerate
        ch = f.channels
        pcm = b"".join(b for b in f)
    y = np.frombuffer(pcm, dtype=np.int16).astype(np.float32) / 32768.0
    if ch > 1:
        y = y.reshape(-1, ch).mean(axis=1)
    if sr_native != sr:
        y = resampy.resample(y, sr_native, sr)
    return y, sr

# If you don't have FFmpeg but files are WAV only, you can use librosa:
# import librosa
# def load_audio(path, sr=16000):
#     y, _ = librosa.load(path, sr=sr, mono=True)
#     return y, sr

def prepare_dataset(batch):
    # dataset was built with Audio(decode=False), so we only have a file path
    path = batch["audio"]["path"]
    array, _ = load_audio(path, sr=16000)

    batch["input_features"] = processor.feature_extractor(
        array, sampling_rate=16000
    ).input_features[0]

    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

# IMPORTANT: run mapping in a single process so no audio stack gets re-imported
vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1,                       # <-- not 8
    desc="Preparing dataset",
)

def prepare_dataset(batch):
    a = batch["audio"]
    if isinstance(a, dict) and "array" in a and a["array"] is not None:
        array = a["array"]
        sr = a.get("sampling_rate", 16000)
    else:
        path = a["path"] if isinstance(a, dict) else a
        array, sr = load_audio(path, sr=16000)

    batch["input_features"] = processor.feature_extractor(array, sampling_rate=16000).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch