#!/usr/bin/env python
import os
import sys
import traceback
from pathlib import Path

import pandas as pd
import torch
import torch.nn.functional as F
from transformers import RobertaTokenizer, RobertaModel

import evaluate   # loads your local bertscore.py
from fastapi import FastAPI, HTTPException
from fastapi.responses import FileResponse
from pydantic import BaseModel

# ─── Configuration ─────────────────────────────────────────────────────────
# where your RoBERTa-large repo and baseline .tsv live:
MODEL_DIR     = Path(r"C:/Users/ZKC7HOU/Documents/4. BERT score/roberta-large")
BASELINE_PATH = MODEL_DIR / "roberta-large.tsv"
DEVICE        = "cuda" if torch.cuda.is_available() else "cpu"

# ─── 1) load your local bertscore metric ────────────────────────────────
METRIC_SCRIPT_DIR = Path(__file__).parent / "automation_script"
try:
    scorer = evaluate.load(
        path=str(METRIC_SCRIPT_DIR.resolve()),
        module_type="metric"
    )
    print("✅ Loaded local bertscore metric")
except Exception as e:
    print("❌ ERROR loading local bertscore metric:", e, file=sys.stderr)
    scorer = None

# ─── 2) load local RoBERTa tokenizer+model ───────────────────────────────
if not MODEL_DIR.is_dir():
    print(f"❌ ERROR: cannot find model dir at {MODEL_DIR}", file=sys.stderr)
    sys.exit(1)

try:
    tokenizer = RobertaTokenizer.from_pretrained(
        str(MODEL_DIR), local_files_only=True, use_fast=False
    )
    model = (
        RobertaModel.from_pretrained(str(MODEL_DIR), local_files_only=True)
        .to(DEVICE)
        .eval()
    )
    print("✅ Loaded local RoBERTa-large model/tokenizer")
except Exception as e:
    print("❌ ERROR loading RoBERTa:", e, file=sys.stderr)
    sys.exit(1)

# ─── FastAPI setup ─────────────────────────────────────────────────────────
app = FastAPI()

class ScoreRequest(BaseModel):
    reference: list[str]
    candidate: list[str]

class ScoreResponse(BaseModel):
    precision:  list[float]
    recall:     list[float]
    f1:         list[float]
    model_type: str = "roberta-large"
    version:    str = "0.3.12"


@app.post("/bertscore/", response_model=ScoreResponse)
async def calculate_score(request: ScoreRequest):
    if scorer is None:
        raise HTTPException(500, "Metric failed to load")
    if len(request.reference) != len(request.candidate):
        raise HTTPException(400, "reference and candidate must have the same length")
    if not request.reference:
        raise HTTPException(400, "no sentences provided")

    try:
        out = scorer.compute(
            predictions=[c.strip() for c in request.candidate],
            references=[r.strip() for r in request.reference],
            model_type=str(MODEL_DIR),
            num_layers=17,
            device=DEVICE,
            rescale_with_baseline=True,
            baseline_path=str(BASELINE_PATH),
            lang="en"
        )
        P, R, F1 = out["precision"], out["recall"], out["f1"]

        # clamp to [0,1] and round to 6dp
        pts = [round(min(max(x,0.0),1.0), 6) for x in P]
        rts = [round(min(max(x,0.0),1.0), 6) for x in R]
        fts = [round(min(max(x,0.0),1.0), 6) for x in F1]

        return ScoreResponse(precision=pts, recall=rts, f1=fts)

    except Exception as e:
        print(traceback.format_exc(), file=sys.stderr)
        raise HTTPException(500, str(e))


# ─── CLI mode (unchanged) ───────────────────────────────────────────────────
def cli_main(input_files: list[str], output_dir: Path):
    dfs = []
    for fn in input_files:
        p = Path(fn)
        suf = p.suffix.lower()
        if suf in (".xls", ".xlsx"):
            dfs.append(pd.read_excel(p))
        elif suf == ".csv":
            dfs.append(pd.read_csv(p))
        else:
            print(f"❌ ERROR: unsupported file type {suf}", file=sys.stderr)
            sys.exit(1)

    if not dfs:
        print("❌ ERROR: no data loaded", file=sys.stderr)
        sys.exit(1)

    df = pd.concat(dfs, ignore_index=True)
    for col in ("reference", "candidate"):
        if col not in df.columns:
            print(f"❌ ERROR: missing required column '{col}'", file=sys.stderr)
            sys.exit(1)

    # compute exactly the same via scorer.compute()
    out = scorer.compute(
        predictions=[c.strip() for c in df["candidate"].astype(str)],
        references=[r.strip() for r in df["reference"].astype(str)],
        model_type=str(MODEL_DIR),
        num_layers=17,
        device=DEVICE,
        rescale_with_baseline=True,
        baseline_path=str(BASELINE_PATH),
        lang="en"
    )
    df["precision"] = out["precision"]
    df["recall"]    = out["recall"]
    df["f1"]        = out["f1"]

    # write Excel: Detailed + one‐row Model metrics
    output_dir.mkdir(parents=True, exist_ok=True)
    out_fp = output_dir / "Bertscore_output.xlsx"
    with pd.ExcelWriter(out_fp, engine="xlsxwriter") as writer:
        df[["reference","candidate","precision","recall","f1"]] \
          .to_excel(writer, sheet_name="Detailed metrics", index=False)

        summary = pd.DataFrame([{
            "precision": df["precision"].mean(),
            "recall":    df["recall"].mean(),
            "f1":        df["f1"].mean()
        }]).round(6)
        summary.to_excel(writer, sheet_name="Model metrics", index=False)

    print(f"✔️ Report saved to: {out_fp.resolve()}")


# ─── entrypoint ────────────────────────────────────────────────────────────
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Offline BERTScore CLI + FastAPI server"
    )
    parser.add_argument(
        "--output_dir",
        type=Path,
        help="If set, writes Bertscore_output.xlsx here (CLI mode)"
    )
    parser.add_argument(
        "files",
        nargs="*",
        help="Input .csv/.xls/.xlsx (only in CLI mode)"
    )
    args = parser.parse_args()

    if args.output_dir and args.files:
        cli_main(args.files, args.output_dir)
    else:
        import uvicorn
        uvicorn.run("server:app", host="0.0.0.0", port=8000, reload=True)
        