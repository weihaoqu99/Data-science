import evaluate
import torch

# Load your local BERTScore metric
scorer = evaluate.load(r"C:/Users/ZKC7HOU/Documents/4. BERT score/bertscore")

# Define model path and baseline path
MODEL_PATH = r"C:/Users/ZKC7HOU/Documents/4. BERT score/roberta-large"
BASELINE_PATH = r"C:/Users/ZKC7HOU/Documents/4. BERT score/roberta-large/roberta-large.tsv"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Input lists
references = ["This is a reference sentence.", "Another test reference."]
candidates = ["This is a candidate sentence.", "Another candidate version."]

# Compute BERTScore
result = scorer.compute(
    predictions=candidates,
    references=references,
    model_type=MODEL_PATH,
    num_layers=17,
    device=DEVICE,
    rescale_with_baseline=True,
    baseline_path=BASELINE_PATH,
    lang="en"
)

# Print results
for i, (p, r, f) in enumerate(zip(result["precision"], result["recall"], result["f1"])):
    print(f"\nPair {i+1}:")
    print(f"  Precision: {round(p, 6)}")
    print(f"  Recall   : {round(r, 6)}")
    print(f"  F1 Score : {round(f, 6)}")