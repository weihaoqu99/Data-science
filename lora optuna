# ===================== ADD-ON: Optuna LoRA Search =====================
# Drop this cell into your notebook AFTER model/processor/datasets/collator/metrics
# and BEFORE you build your final Trainer. Your original cells stay unchanged.

import gc, os, json, math
import optuna
import torch

from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType,
    PeftModel,
)

from transformers import (
    WhisperForConditionalGeneration,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
)

# -------- fast tuning subsets (tiny to keep trials quick) --------
TUNE_TRAIN_SAMPLES = min(200, len(vectorized_datasets["train"]))
TUNE_EVAL_SAMPLES  = min(100, len(vectorized_datasets["test"]))
# we will (re)select inside each trial from the *vectorized* datasets

# -------- fresh base model for each trial (keeps your main `model` untouched) --------
def _fresh_base():
    m = WhisperForConditionalGeneration.from_pretrained(whisper_loc)
    # Safe even if you aren’t using 8-bit; keeps norms in fp32 etc.
    m = prepare_model_for_kbit_training(m)
    m.gradient_checkpointing_enable()
    m.enable_input_require_grads()
    # keep your config consistent
    m.config.suppress_tokens = []
    m.config.use_cache = False
    m.config.language = "english"
    m.config.task = "transcribe"
    return m

def objective(trial: optuna.Trial) -> float:
    # ---- LoRA search space (includes 128 for r/alpha) ----
    r           = trial.suggest_categorical("r", [16, 32, 64, 128])
    alpha       = trial.suggest_categorical("alpha", [16, 32, 64, 128])
    dropout     = trial.suggest_float("dropout", 0.01, 0.15, step=0.02)
    use_rslora  = trial.suggest_categorical("use_rslora", [True, False])
    target_mods = trial.suggest_categorical(
        "target_modules",
        [["q_proj","v_proj"], ["q_proj","k_proj","v_proj"]]
    )

    temp_model = _fresh_base()

    # Build LoRA config (RS-LoRA if supported by your PEFT)
    try:
        lcfg = LoraConfig(
            r=r,
            lora_alpha=alpha,
            lora_dropout=dropout,
            target_modules=target_mods,
            bias="none",
            task_type=TaskType.SEQ_2_SEQ_LM,
            use_rslora=use_rslora,
        )
    except TypeError:
        # Older PEFT fallback (no `use_rslora` kwarg)
        lcfg = LoraConfig(
            r=r,
            lora_alpha=alpha,
            lora_dropout=dropout,
            target_modules=target_mods,
            bias="none",
            task_type=TaskType.SEQ_2_SEQ_LM,
        )

    temp_model = get_peft_model(temp_model, lcfg)

    # Use *already vectorized* subsets so "input_features" are present
    trial_train = vectorized_datasets["train"].select(range(TUNE_TRAIN_SAMPLES))
    trial_eval  = vectorized_datasets["test"].select(range(TUNE_EVAL_SAMPLES))

    # Keep batch schema intact for speech (prevents dropping `input_features`)
    trial_args = Seq2SeqTrainingArguments(
        output_dir=f"./optuna_trial_{trial.number}",
        per_device_train_batch_size=2,
        per_device_eval_batch_size=2,
        num_train_epochs=1,
        logging_strategy="steps",
        logging_steps=10,
        evaluation_strategy="steps",
        eval_steps=50,
        save_strategy="no",
        predict_with_generate=True,
        generation_max_length=225,
        fp16=torch.cuda.is_available(),
        report_to=["none"],
        dataloader_num_workers=0,
        remove_unused_columns=False,
        label_names=["labels"],
    )

    trial_trainer = Seq2SeqTrainer(
        model=temp_model,
        args=trial_args,
        train_dataset=trial_train,
        eval_dataset=trial_eval,
        data_collator=data_collator,
        tokenizer=processor,
        compute_metrics=compute_metrics,
    )

    trial_trainer.train()
    metrics = trial_trainer.evaluate()
    wer = float(metrics.get("eval_wer", metrics.get("wer", math.inf)))  # lower is better

    # cleanup between trials
    del trial_trainer, temp_model
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    trial.report(wer, step=0)
    if trial.should_prune():
        raise optuna.TrialPruned()
    return wer

# -------- run the study --------
study = optuna.create_study(
    direction="minimize",
    sampler=optuna.samplers.TPESampler(n_startup_trials=4, multivariate=True),
    pruner=optuna.pruners.MedianPruner(n_startup_trials=4),
)
study.optimize(objective, n_trials=10, show_progress_bar=False)

print("[Optuna] Best trial:", study.best_trial.number)
print("[Optuna] Best params:", study.best_trial.params)
print("[Optuna] Best WER:", study.best_trial.value)

os.makedirs("./optuna_results", exist_ok=True)
with open("./optuna_results/best_lora_params.json", "w") as f:
    json.dump(
        {"trial": study.best_trial.number, "params": study.best_trial.params, "wer": study.best_trial.value},
        f, indent=2
    )

# -------- attach BEST LoRA to YOUR current `model` (non-invasive) --------
# This keeps your original code intact. If you already added a LoRA earlier,
# we add a new adapter and activate it; otherwise we wrap the base model.
best = study.best_trial.params
best_kwargs = dict(
    r=best["r"],
    lora_alpha=best["alpha"],
    lora_dropout=best["dropout"],
    target_modules=best["target_modules"],
    bias="none",
    task_type=TaskType.SEQ_2_SEQ_LM,
)

try:
    best_cfg = LoraConfig(**best_kwargs, use_rslora=best.get("use_rslora", True))
except TypeError:
    best_cfg = LoraConfig(**best_kwargs)

# Prepare/training-friendly (idempotent if already done)
model = prepare_model_for_kbit_training(model)
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

ADAPTER_NAME = "optuna_best"

if isinstance(model, PeftModel):
    # Add alongside any existing adapter and activate it
    model.add_adapter(ADAPTER_NAME, best_cfg)
    model.set_adapter(ADAPTER_NAME)
else:
    # First time adding LoRA
    model = get_peft_model(model, best_cfg)

model.print_trainable_parameters()
print(f"✅ Optuna LoRA attached. Active adapter: {ADAPTER_NAME if isinstance(model, PeftModel) else 'default'}")
# =================== END Optuna LoRA Search (add-on) ==================