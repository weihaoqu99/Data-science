# ============================================================
# Imports & Setup
# ============================================================
import os, glob, re, json, math
from dataclasses import dataclass
from typing import Any, Dict, List, Union
from datetime import datetime

import torch
import evaluate
import numpy as np
import librosa

from datasets import Dataset, Audio, concatenate_datasets
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    TrainerCallback,
    EarlyStoppingCallback,   # <-- ADDED
    pipeline,
)

# ---- PEFT / LoRA
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType,
)

# ---- Optuna
import optuna

# os.environ["CUDA_VISIBLE_DEVICES"] = "1"

# ============================================================
# Utility: convert g711 to pcm
# ============================================================
def convert_g711_to_pcm(pathname):
    collection = []
    with open(pathname, 'rb') as f:
        while (inter := f.read(1)):
            inter = int(ord(inter))
            if inter == 127:
                collection.append(-1)
                continue
            inter = ~inter
            sign = inter & 0x80
            exponent = (inter & 0x70) >> 4
            data = inter & 0x0f
            data |= 0x10
            data <<= 1
            data <<= exponent + 2
            data += 0x84
            data = data if sign == 0 else -data
            collection.append(data)
    collection = np.array(collection, dtype=np.float32)/32768
    collection = librosa.resample(collection, orig_sr=8000, target_sr=16000, scale=False)
    return collection

# ============================================================
# Data Preparation
# ============================================================
audio_files = glob.glob("origAudio/*.mp3")

def retrieve_transcript(input_str: str) -> str:
    tmp = input_str[input_str.find("-")+1:-4]  # strip out track number and suffix
    tmp = tmp.replace("_", " ").replace(".", " ")
    tmp = tmp.replace("shell", "she'll")
    tmp = tmp.replace("dont", "don't")
    tmp = tmp.capitalize()
    tmp = tmp.replace("i ", "I ")
    return tmp

transcripts = [retrieve_transcript(s) for s in audio_files]

num_samples = len(audio_files)
raw_datasets = Dataset.from_dict({
    "audio": audio_files[:num_samples],
    "sentence": transcripts[:num_samples]
}).cast_column("audio", Audio(sampling_rate=16_000))

raw_datasets = raw_datasets.train_test_split(test_size=0.15, seed=91)

# ============================================================
# Whisper Model & Processor
# ============================================================
whisper_loc = "/appdata/cortex/dev4/shobha/ASR/whisper_copy/distil-large-v2"
processor = WhisperProcessor.from_pretrained(whisper_loc, language="English", task="transcribe")

def prepare_dataset(batch):
    audio = batch["audio"]
    batch["input_features"] = processor.feature_extractor(
        audio["array"], sampling_rate=audio["sampling_rate"]
    ).input_features[0]
    transcription = batch["sentence"]
    batch["labels"] = processor.tokenizer(transcription).input_ids
    return batch

vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets["train"].column_names,
    num_proc=1
)

# ============================================================
# Data Collator
# ============================================================
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    decoder_start_token_id: int

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_features": f["input_features"]} for f in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        label_features = [{"input_ids": f["labels"]} for f in features]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels
        return batch

# ============================================================
# Metrics
# ============================================================
metric = evaluate.load("wer")

def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id
    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)
    wer = 100 * metric.compute(predictions=pred_str, references=label_str)
    return {"wer": wer}

# ============================================================
# Custom Callback
# ============================================================
class CustomCallback(TrainerCallback):
    def __init__(self):
        self.audio_files = audio_files.copy()
        self.audios = [convert_g711_to_pcm(f) for f in self.audio_files]

    def on_evaluate(self, args, state, control, model, **kwargs):
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        pipe = pipeline(
            "automatic-speech-recognition",
            model=model,
            tokenizer=processor.tokenizer,
            feature_extractor=processor.feature_extractor,
            chunk_length_s=5,
            batch_size=1,
            device=device
        )
        output = [pipe(aud)["text"] for aud in self.audios]
        with open(f"records/{state.global_step}_5s.txt", 'w') as f:
            for trans in output:
                f.write(trans + '\n')
        del pipe

# ============================================================
# LoRA Fix Example
# ============================================================
model = prepare_model_for_kbit_training(
    WhisperForConditionalGeneration.from_pretrained(whisper_loc)
)
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

lora_cfg = LoraConfig(
    r=64,
    lora_alpha=64,
    use_rslora=True,
    lora_dropout=0.13,
    target_modules=["q_proj", "v_proj"],
)
model = get_peft_model(model, lora_cfg)

# ============================================================
# Training Args
# ============================================================
training_args = Seq2SeqTrainingArguments(
    output_dir="./L01",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=1e-5,
    warmup_steps=500,
    max_steps=1000,
    gradient_checkpointing=True,
    fp16=True,
    eval_strategy="steps",
    per_device_eval_batch_size=4,
    predict_with_generate=True,
    generation_max_length=225,
    seed=91,
    data_seed=91,
    logging_steps=5,
    eval_steps=5,
    save_strategy="steps",
    save_total_limit=2,
    report_to=["none"],
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    push_to_hub=False,
    dataloader_num_workers=8,
    dataloader_prefetch_factor=2,
)

# ============================================================
# Trainer
# ============================================================
trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=vectorized_datasets["train"],
    eval_dataset=vectorized_datasets["test"],
    data_collator=DataCollatorSpeechSeq2SeqWithPadding(
        processor, model.config.decoder_start_token_id
    ),
    tokenizer=processor,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)],  # <-- early stopping
)

trainer.train()