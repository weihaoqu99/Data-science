# ============================================
# Optuna search to find best LoRA settings
# (keeps your original code unchanged)
# ============================================
import gc, os, json, math
import optuna

from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType,
)

# --- tiny, fast slices for the search (so it doesn't take forever)
TUNE_TRAIN_SAMPLES = min(200, len(vectorized_datasets["train"]))
TUNE_EVAL_SAMPLES  = min(100, len(vectorized_datasets["test"]))
tune_train = vectorized_datasets["train"].select(range(TUNE_TRAIN_SAMPLES))
tune_eval  = vectorized_datasets["test"].select(range(TUNE_EVAL_SAMPLES))

# If you want to reload a fresh base model for each trial (recommended),
# define this using the same 'whisper_loc' you already use:
def _fresh_base():
    m = WhisperForConditionalGeneration.from_pretrained(whisper_loc)
    # Safe even if you aren't using 8-bit; keeps norms in fp32, etc.
    m = prepare_model_for_kbit_training(m)
    m.gradient_checkpointing_enable()
    m.enable_input_require_grads()
    # keep your same config flags:
    m.config.suppress_tokens = []
    m.config.use_cache = False
    m.config.language = "english"
    m.config.task = "transcribe"
    return m

def objective(trial: optuna.Trial) -> float:
    # --- search space (adjust/expand as you like)
    r           = trial.suggest_categorical("r", [16, 32, 64])
    alpha       = trial.suggest_categorical("alpha", [16, 32, 64])
    dropout     = trial.suggest_float("dropout", 0.01, 0.15, step=0.02)
    use_rslora  = trial.suggest_categorical("use_rslora", [True, False])
    target_mods = trial.suggest_categorical(
        "target_modules",
        [["q_proj","v_proj"], ["q_proj","k_proj","v_proj"]]
    )

    temp_model = _fresh_base()

    # Build LoRA config (RS-LoRA if your PEFT supports it)
    try:
        lcfg = LoraConfig(
            r=r,
            lora_alpha=alpha,
            lora_dropout=dropout,
            target_modules=target_mods,
            bias="none",
            task_type=TaskType.SEQ_2_SEQ_LM,
            use_rslora=use_rslora,
        )
    except TypeError:
        # Older PEFT fallback (no RS-LoRA flag)
        lcfg = LoraConfig(
            r=r,
            lora_alpha=alpha,
            lora_dropout=dropout,
            target_modules=target_mods,
            bias="none",
            task_type=TaskType.SEQ_2_SEQ_LM,
        )

    temp_model = get_peft_model(temp_model, lcfg)

    # Short, comparable run so Optuna can rank trials
    trial_args = Seq2SeqTrainingArguments(
        output_dir=f"./optuna_trial_{trial.number}",
        per_device_train_batch_size=2,
        per_device_eval_batch_size=2,
        num_train_epochs=1,
        logging_strategy="steps",
        logging_steps=10,
        evaluation_strategy="steps",
        eval_steps=50,
        save_strategy="no",
        predict_with_generate=True,
        generation_max_length=225,
        fp16=torch.cuda.is_available(),
        report_to=["none"],
        dataloader_num_workers=0,  # avoid multiprocessing overhead for tiny runs
    )

    trial_trainer = Seq2SeqTrainer(
        model=temp_model,
        args=trial_args,
        train_dataset=tune_train,
        eval_dataset=tune_eval,
        data_collator=data_collator,
        tokenizer=processor,
        compute_metrics=compute_metrics,  # your WER
    )

    trial_trainer.train()
    metrics = trial_trainer.evaluate()
    # Your compute_metrics returns {"wer": …}; Trainer wraps it as "eval_wer"
    wer = float(metrics.get("eval_wer", math.inf))

    # Cleanup GPU/CPU between trials
    del trial_trainer, temp_model
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    trial.report(wer, step=0)
    if trial.should_prune():
        raise optuna.TrialPruned()
    return wer

# Create and run the study
study = optuna.create_study(
    direction="minimize",
    sampler=optuna.samplers.TPESampler(n_startup_trials=4, multivariate=True),
    pruner=optuna.pruners.MedianPruner(n_startup_trials=4),
)
study.optimize(objective, n_trials=10, show_progress_bar=False)

print("[Optuna] Best trial:", study.best_trial.number)
print("[Optuna] Best params:", study.best_trial.params)
print("[Optuna] Best WER:", study.best_trial.value)

os.makedirs("./optuna_results", exist_ok=True)
with open("./optuna_results/best_lora_params.json", "w") as f:
    json.dump(study.best_trial.params, f, indent=2)

# ------------------------------------------------------------
# Apply BEST LoRA to YOUR existing `model` for the real training
# (No other parts of your script are changed)
# ------------------------------------------------------------
# Ensure prep (idempotent/safe to call again)
model = prepare_model_for_kbit_training(model)
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

best = study.best_trial.params
try:
    best_cfg = LoraConfig(
        r=best["r"],
        lora_alpha=best["alpha"],
        lora_dropout=best["dropout"],
        target_modules=best["target_modules"],
        bias="none",
        task_type=TaskType.SEQ_2_SEQ_LM,
        use_rslora=best.get("use_rslora", True),
    )
except TypeError:
    best_cfg = LoraConfig(
        r=best["r"],
        lora_alpha=best["alpha"],
        lora_dropout=best["dropout"],
        target_modules=best["target_modules"],
        bias="none",
        task_type=TaskType.SEQ_2_SEQ_LM,
    )

model = get_peft_model(model, best_cfg)
model.print_trainable_parameters()  # sanity check: only LoRA params trainable
# ============================================
# End Optuna block — continue to your Trainer()
# ============================================