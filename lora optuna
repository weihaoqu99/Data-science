# ============================================================
# Imports & Setup
# ============================================================
import os, glob, re, json, math
import torch
import evaluate
import numpy as np
import librosa
from dataclasses import dataclass
from typing import Any, Dict, List, Union
from datetime import datetime

from datasets import Dataset, Audio, concatenate_datasets
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    TrainerCallback,
    pipeline,
    EarlyStoppingCallback,   # <-- ADDED
)

# ---- PEFT / LoRA
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType,
)

# ---- Optuna
import optuna

os.environ["CUDA_VISIBLE_DEVICES"] = "1"

# ============================================================
# Utility: convert g711 to pcm
# ============================================================
def convert_g711_to_pcm(pathname):
    collection = []
    with open(pathname, 'rb') as f:
        while (inter := f.read(1)):
            inter = int(ord(inter))
            if inter == 127:
                collection.append(-1)
                continue
            inter = ~inter
            sign = inter & 0x80
            exponent = (inter & 0x70) >> 4
            data = inter & 0x0f
            data |= 0x10
            data <<= 1
            data <<= exponent + 2
            data += 0x84
            data = data if sign == 0 else -data
            collection.append(data)
    collection = np.array(collection, dtype=np.float32)/32768
    collection = librosa.resample(collection, orig_sr=8000, target_sr=16000, scale=False)
    return collection

# ============================================================
# Data Preparation
# ============================================================
audio_files = glob.glob("origAudio/*.mp3")

def retrieve_transcript(input_str: str) -> str:
    tmp = input_str[input_str.find("-")+1:-4]
    tmp = tmp.replace("_", " ").replace(".", " ")
    tmp = tmp.replace("shell", "she'll")
    tmp = tmp.replace("dont", "don't")
    tmp = tmp.capitalize()
    tmp = tmp.replace("i ", "I ")
    return tmp

transcripts = [retrieve_transcript(s) for s in audio_files]

num_samples = len(audio_files)
raw_datasets = Dataset.from_dict({
    "audio": audio_files[:num_samples],
    "sentence": transcripts[:num_samples]
}).cast_column("audio", Audio(sampling_rate=16_000))

raw_datasets = raw_datasets.train_test_split(test_size=0.15, seed=91)

# ============================================================
# Whisper Model & Processor
# ============================================================
whisper_loc = "/appdata/cortex/dev4/shobha/ASR/whisper_copy/distil-large-v2"
processor = WhisperProcessor.from_pretrained(whisper_loc, language="English", task="transcribe")

def prepare_dataset(batch):
    audio = batch["audio"]
    batch["input_features"] = processor.feature_extractor(
        audio["array"], sampling_rate=audio["sampling_rate"]
    ).input_features[0]
    transcription = batch["sentence"]
    batch["labels"] = processor.tokenizer(transcription).input_ids
    return batch

vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets["train"].column_names,
    num_proc=1
)

# ============================================================
# Data Collator
# ============================================================
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    decoder_start_token_id: int

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_features": f["input_features"]} for f in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        label_features = [{"input_ids": f["labels"]} for f in features]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels
        return batch

# ============================================================
# Metrics
# ============================================================
metric = evaluate.load("wer")

def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id
    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)
    wer = 100 * metric.compute(predictions=pred_str, references=label_str)
    return {"wer": wer}

# ============================================================
# Custom Callback
# ============================================================
class CustomCallback(TrainerCallback):
    def __init__(self):
        self.audio_files = audio_files.copy()
        self.audios = [convert_g711_to_pcm(f) for f in self.audio_files]

    def on_evaluate(self, args, state, control, model, **kwargs):
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        pipe = pipeline(
            "automatic-speech-recognition",
            model=model,
            tokenizer=processor.tokenizer,
            feature_extractor=processor.feature_extractor,
            chunk_length_s=5,
            batch_size=1,
            device=device
        )
        output = [pipe(aud)["text"] for aud in self.audios]
        with open(f"records/{state.global_step}_5s.txt", 'w') as f:
            for trans in output:
                f.write(trans + '\n')
        del pipe

# ============================================================
# Optuna + LoRA Search
# ============================================================
TUNE_TRAIN_SAMPLES = min(200, len(vectorized_datasets["train"]))
TUNE_EVAL_SAMPLES = min(100, len(vectorized_datasets["test"]))
tune_train = vectorized_datasets["train"].select(range(TUNE_TRAIN_SAMPLES))
tune_eval = vectorized_datasets["test"].select(range(TUNE_EVAL_SAMPLES))

def _fresh_base():
    m = WhisperForConditionalGeneration.from_pretrained(whisper_loc)
    m = prepare_model_for_kbit_training(m)
    m.gradient_checkpointing_enable()
    m.enable_input_require_grads()
    m.config.suppress_tokens = []
    m.config.use_cache = False
    m.config.language = "english"
    m.config.task = "transcribe"
    return m

def objective(trial: optuna.Trial) -> float:
    r = trial.suggest_categorical("r", [16, 32, 64, 128])
    alpha = trial.suggest_categorical("alpha", [16, 32, 64, 128])
    dropout = trial.suggest_float("dropout", 0.01, 0.15, step=0.02)
    use_rslora = trial.suggest_categorical("use_rslora", [True, False])
    target_mods = trial.suggest_categorical(
        "target_modules", [["q_proj","v_proj"], ["q_proj","k_proj","v_proj"]]
    )

    temp_model = _fresh_base()
    lcfg = LoraConfig(
        r=r,
        lora_alpha=alpha,
        lora_dropout=dropout,
        target_modules=target_mods,
        bias="none",
        # task_type=TaskType.SEQ_2_SEQ_LM,    # optional
        use_rslora=use_rslora,
    )
    temp_model = get_peft_model(temp_model, lcfg)

    args = Seq2SeqTrainingArguments(
        output_dir=f".optuna_trial_{trial.number}",
        per_device_train_batch_size=2,
        per_device_eval_batch_size=2,
        num_train_epochs=1,
        logging_strategy="steps",
        logging_steps=10,
        eval_strategy="steps",          # deprecation-safe
        eval_steps=50,
        save_strategy="no",
        predict_with_generate=True,
        generation_max_length=225,
        fp16=torch.cuda.is_available(),
        report_to=["none"],
        dataloader_num_workers=0,
        load_best_model_at_end=True,           # <-- ADDED for early stop to pick best
        metric_for_best_model="wer",           # <-- ADDED
        greater_is_better=False,               # <-- ADDED (lower WER is better)
    )

    t = Seq2SeqTrainer(
        model=temp_model,
        args=args,
        train_dataset=tune_train,
        eval_dataset=tune_eval,
        data_collator=DataCollatorSpeechSeq2SeqWithPadding(
            processor, temp_model.config.decoder_start_token_id
        ),
        tokenizer=processor,
        compute_metrics=compute_metrics,
        callbacks=[
            EarlyStoppingCallback(
                early_stopping_patience=10,     # <-- ADDED (change if you like)
                early_stopping_threshold=0.0
            )
        ],
    )

    t.train()
    metrics = t.evaluate()
    wer = float(metrics.get("eval_wer", metrics.get("wer", math.inf)))
    return wer

study = optuna.create_study(
    direction="minimize",
    sampler=optuna.samplers.TPESampler(n_startup_trials=4, multivariate=True),
    pruner=optuna.pruners.MedianPruner(n_startup_trials=4),
)
study.optimize(objective, n_trials=10, show_progress_bar=False)

print("[Optuna] Best trial:", study.best_trial.number)
print("[Optuna] Best params:", study.best_trial.params)
print("[Optuna] Best WER:", study.best_trial.value)

# ============================================================
# Attach Best LoRA
# ============================================================
model = prepare_model_for_kbit_training(_fresh_base())
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

best = study.best_trial.params
best_cfg = LoraConfig(
    r=best["r"],
    lora_alpha=best["alpha"],
    lora_dropout=best["dropout"],
    target_modules=best["target_modules"],
    bias="none",
    task_type=TaskType.SEQ_2_SEQ_LM,
    use_rslora=best.get("use_rslora", True),
)
model = get_peft_model(model, best_cfg)
model.print_trainable_parameters()

# ============================================================
# Training Arguments (Final Full Training)
# ============================================================
training_args = Seq2SeqTrainingArguments(
    output_dir="./L01",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=1e-5,
    warmup_steps=500,
    max_steps=1000,
    gradient_checkpointing=True,
    fp16=True,
    eval_strategy="steps",
    per_device_eval_batch_size=4,
    predict_with_generate=True,
    generation_max_length=225,
    seed=91,
    data_seed=91,
    logging_steps=5,
    eval_steps=5,
    save_strategy="steps",
    save_total_limit=2,
    report_to=["none"],
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    push_to_hub=False,
    dataloader_num_workers=8,
    dataloader_prefetch_factor=2,
)

# ============================================================
# Trainer
# ============================================================
trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=vectorized_datasets["train"],
    eval_dataset=vectorized_datasets["test"],
    data_collator=DataCollatorSpeechSeq2SeqWithPadding(
        processor, model.config.decoder_start_token_id
    ),
    tokenizer=processor,
    compute_metrics=compute_metrics,
    callbacks=[
        EarlyStoppingCallback(
            early_stopping_patience=20,   # <-- ADDED (stop if no WER improvement over 20 evals)
            early_stopping_threshold=0.0
        ),
        # CustomCallback(),              # optional; leave commented if not needed
    ],
)

trainer.train()

# ============================================================
# Save Final Model
# ============================================================
trainer.save_model("finetuned_model_L01")

# ============================================================
# Evaluation of Saved Model
# ============================================================
import pandas as pd
from transformers import WhisperForConditionalGeneration

SAVED_MODEL_DIR = "finetuned_model_L01"
CSV_OUT = "test_predictions_L01.csv"

# Reload
eval_model = WhisperForConditionalGeneration.from_pretrained(SAVED_MODEL_DIR)

# Eval-only args
eval_args = Seq2SeqTrainingArguments(
    output_dir="./test_eval_out01",
    per_device_eval_batch_size=4,
    predict_with_generate=True,
    generation_max_length=225,
    fp16=True,
    report_to=["none"],
)

# Eval trainer
eval_trainer = Seq2SeqTrainer(
    args=eval_args,
    model=eval_model,
    eval_dataset=vectorized_datasets["test"],
    data_collator=DataCollatorSpeechSeq2SeqWithPadding(
        processor, eval_model.config.decoder_start_token_id
    ),
    tokenizer=processor,
    compute_metrics=compute_metrics,
)

# Overall test metrics
test_metrics = eval_trainer.evaluate()
print("Test metrics:", {k: float(v) for k, v in test_metrics.items()})

# ============================================================
# Per-sample predictions + CSV Export
# ============================================================
pred = eval_trainer.predict(vectorized_datasets["test"])
pred_ids = pred.predictions
label_ids = pred.label_ids

label_ids[label_ids == -100] = processor.tokenizer.pad_token_id
pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)

wer_metric = evaluate.load("wer")
rows = []
test_raw = raw_datasets["test"]

for i, (hyp, ref) in enumerate(zip(pred_str, label_str)):
    w = 100 * wer_metric.compute(predictions=[hyp], references=[ref])
    rows.append({
        "idx": i,
        "audio": test_raw[i]["audio"]["path"],
        "reference": ref,
        "prediction": hyp,
        "wer": round(w, 2),
    })

df = pd.DataFrame(rows)
df.to_csv(CSV_OUT, index=False, encoding="utf-8")
print("Saved detailed predictions to:", CSV_OUT)