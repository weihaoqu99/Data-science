# =========================================================
# Imports (ORIGINAL)
# =========================================================
from datasets import Dataset, Audio, concatenate_datasets
import glob
import librosa
import os
import numpy as np

# =========================================================
# Convert G.711 audio to PCM (ORIGINAL)
# =========================================================
def convert_g711_to_pcm(pathname):
    collection = []
    with open(pathname, 'rb') as f:
        while (inter := f.read(1)):
            inter = int(ord(inter))
            if inter == 127:
                collection.append(-1)
                continue
            inter = ~inter
            sign = inter & 0x80
            exponent = (inter & 0x70) >> 4
            data = inter & 0x0f
            data |= 0x10
            data <<= 1
            data += 1
            data <<= exponent + 2
            data -= 0x84
            data = data if sign == 0 else -data
            collection.append(data)

    collection = np.array(collection, dtype=np.float32) / 32768
    collection = librosa.resample(collection, orig_sr=8_000, target_sr=16_000, scale=False)
    return collection

# =========================================================
# Collect audio files + transcript cleaner (ORIGINAL)
# =========================================================
audio_files = glob.glob("origAudio/*.mp3")

def retrieve_transcript(input_str: str) -> str:
    tmp = input_str[input_str.find("-")+1:-4]  # strip track number + suffix
    tmp = tmp.replace("_", " ")
    tmp = tmp.replace(".", " ")
    tmp = tmp.replace("shell", "she'll")
    tmp = tmp.replace("dont", "don't")
    tmp = tmp.capitalize()
    tmp = tmp.replace("i ", "I ")
    return tmp

transcripts = [retrieve_transcript(s) for s in audio_files]
num_samples = len(audio_files)

raw_datasets = Dataset.from_dict({
    "audio": audio_files[:num_samples],
    "sentence": transcripts[:num_samples]
}).cast_column("audio", Audio(sampling_rate=16_000))

raw_datasets = raw_datasets.train_test_split(test_size=0.15, seed=91)

# =========================================================
# Processor (ORIGINAL)
# =========================================================
from transformers import WhisperProcessor
whisper_loc = "/appdata/cortex/dev4/shobha/ASR/whisper_copy/distil-large-v2"
processor = WhisperProcessor.from_pretrained(whisper_loc, language="English", task="transcribe")

# =========================================================
# Prepare dataset (ORIGINAL)
# =========================================================
def prepare_dataset(batch):
    # load / resample / convert g711->pcm
    audio = batch["audio"]
    prefix = "/appdata/cortex/dev4/asr_finetuning/audio_for_finetuning_low_qual"
    audio["path"] = os.path.join(prefix, os.path.basename(audio["path"])).replace("mp3", "raw")
    audio["array"] = convert_g711_to_pcm(audio["path"])

    # features
    batch["input_features"] = processor.feature_extractor(
        audio["array"], sampling_rate=audio["sampling_rate"]
    ).input_features[0]

    # labels
    transcription = batch["sentence"]
    batch["labels"] = processor.tokenizer(transcription).input_ids
    return batch

vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1
)

# =========================================================
# Model (ORIGINAL)
# =========================================================
from transformers import WhisperForConditionalGeneration
model = WhisperForConditionalGeneration.from_pretrained(whisper_loc)

model.config.suppress_tokens = []
model.config.use_cache = False
model.config.language = "english"
model.config.task = "transcribe"

# =========================================================
# Data collator (ORIGINAL)
# =========================================================
import torch
from dataclasses import dataclass
from typing import Any, Dict, List, Union

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    decoder_start_token_id: int

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_features": f["input_features"]} for f in features]
        label_features = [{"input_ids": f["labels"]} for f in features]

        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # ignore pad in loss
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        # drop BOS if already prepended
        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(
    processor=processor,
    decoder_start_token_id=model.config.decoder_start_token_id,
)

# =========================================================
# Metrics (ORIGINAL)
# =========================================================
import evaluate
metric = evaluate.load("wer")

def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id

    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    wer = 100 * metric.compute(predictions=pred_str, references=label_str)
    return {"wer": wer}  # Trainer exposes as "eval_wer"

# ===================================================================
# ====================  ADD-ON 1: NO-TOUCH FIX SHIM  =================
# (Does NOT edit your cells. Wraps the model so it ignores stray kwargs
# like `input_ids` that caused your error.)
# ===================================================================
import types

class ForwardKwargSanitizer(torch.nn.Module):
    def __init__(self, base):
        super().__init__()
        self.base = base
        self._allowed = {
            "input_features", "labels",
            "decoder_input_ids", "decoder_attention_mask",
            "return_dict", "output_attentions", "output_hidden_states", "use_cache",
        }
    def forward(self, *args, **kwargs):
        if kwargs:
            kwargs = {k: v for k, v in kwargs.items() if k in self._allowed}
        return self.base(*args, **kwargs)

# Wrap YOUR model (no other code touched)
model = ForwardKwargSanitizer(model)

# Provide a fresh-wrapped base maker for Optuna trials (so you don't edit that cell)
def make_fresh_whisper_base_for_optuna():
    from transformers import WhisperForConditionalGeneration
    from peft import prepare_model_for_kbit_training
    m = WhisperForConditionalGeneration.from_pretrained(whisper_loc)
    m = prepare_model_for_kbit_training(m)
    m.gradient_checkpointing_enable()
    m.enable_input_require_grads()
    m.config.suppress_tokens = []
    m.config.use_cache = False
    m.config.language = "english"
    m.config.task = "transcribe"
    return ForwardKwargSanitizer(m)

# If an Optuna block defines `_fresh_base()`, override it here dynamically
if "_fresh_base" in globals() and isinstance(globals()["_fresh_base"], types.FunctionType):
    globals()["_fresh_base"] = make_fresh_whisper_base_for_optuna

# ===================================================================
# ====================  ADD-ON 2: OPTUNA + LoRA  ====================
# (Finds best LoRA params and attaches them to YOUR model.
#  No edits to your original cells.)
# ===================================================================
import gc, json, math, optuna
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments

# Small, quick slices
TUNE_TRAIN_SAMPLES = min(200, len(vectorized_datasets["train"]))
TUNE_EVAL_SAMPLES  = min(100, len(vectorized_datasets["test"]))
tune_train = vectorized_datasets["train"].select(range(TUNE_TRAIN_SAMPLES))
tune_eval  = vectorized_datasets["test"].select(range(TUNE_EVAL_SAMPLES))

# Fallback fresh base (will be swapped by the shim override above)
def _fresh_base():
    from transformers import WhisperForConditionalGeneration
    m = WhisperForConditionalGeneration.from_pretrained(whisper_loc)
    m = prepare_model_for_kbit_training(m)
    m.gradient_checkpointing_enable()
    m.enable_input_require_grads()
    m.config.suppress_tokens = []
    m.config.use_cache = False
    m.config.language = "english"
    m.config.task = "transcribe"
    return ForwardKwargSanitizer(m)

def objective(trial: optuna.Trial) -> float:
    r           = trial.suggest_categorical("r", [16, 32, 64])
    alpha       = trial.suggest_categorical("alpha", [16, 32, 64])
    dropout     = trial.suggest_float("dropout", 0.01, 0.15, step=0.02)
    use_rslora  = trial.suggest_categorical("use_rslora", [True, False])
    target_mods = trial.suggest_categorical("target_modules", [["q_proj","v_proj"], ["q_proj","k_proj","v_proj"]])

    temp_model = _fresh_base()
    try:
        lcfg = LoraConfig(
            r=r, lora_alpha=alpha, lora_dropout=dropout,
            target_modules=target_mods, bias="none",
            task_type=TaskType.SEQ_2_SEQ_LM, use_rslora=use_rslora,
        )
    except TypeError:
        lcfg = LoraConfig(
            r=r, lora_alpha=alpha, lora_dropout=dropout,
            target_modules=target_mods, bias="none",
            task_type=TaskType.SEQ_2_SEQ_LM,
        )
    temp_model = get_peft_model(temp_model, lcfg)

    trial_args = Seq2SeqTrainingArguments(
        output_dir=f"./optuna_trial_{trial.number}",
        per_device_train_batch_size=2,
        per_device_eval_batch_size=2,
        num_train_epochs=1,
        logging_strategy="steps",
        logging_steps=10,
        evaluation_strategy="steps",
        eval_steps=50,
        save_strategy="no",
        predict_with_generate=True,
        generation_max_length=225,
        fp16=torch.cuda.is_available(),
        report_to=["none"],
        dataloader_num_workers=0,
        # (we intentionally do NOT change your other defaults)
    )

    trial_trainer = Seq2SeqTrainer(
        model=temp_model,
        args=trial_args,
        train_dataset=tune_train,
        eval_dataset=tune_eval,
        data_collator=data_collator,
        tokenizer=processor,
        compute_metrics=compute_metrics,
    )

    trial_trainer.train()
    metrics = trial_trainer.evaluate()
    wer = float(metrics.get("eval_wer", metrics.get("wer", math.inf)))

    del trial_trainer, temp_model
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    trial.report(wer, step=0)
    if trial.should_prune():
        raise optuna.TrialPruned()
    return wer

study = optuna.create_study(
    direction="minimize",
    sampler=optuna.samplers.TPESampler(n_startup_trials=4, multivariate=True),
    pruner=optuna.pruners.MedianPruner(n_startup_trials=4),
)
study.optimize(objective, n_trials=10, show_progress_bar=False)

print("[Optuna] Best params:", study.best_trial.params, "WER:", study.best_trial.value)
os.makedirs("./optuna_results", exist_ok=True)
with open("./optuna_results/best_lora_params.json", "w") as f:
    json.dump(study.best_trial.params, f, indent=2)

# Attach BEST LoRA to YOUR current model
model = prepare_model_for_kbit_training(model)
# model is already wrapped by sanitizer; keep it wrapped:
base = model.base if isinstance(model, ForwardKwargSanitizer) else model
base.gradient_checkpointing_enable()
base.enable_input_require_grads()

best = study.best_trial.params
try:
    best_cfg = LoraConfig(
        r=best["r"], lora_alpha=best["alpha"], lora_dropout=best["dropout"],
        target_modules=best["target_modules"], bias="none",
        task_type=TaskType.SEQ_2_SEQ_LM, use_rslora=best.get("use_rslora", True),
    )
except TypeError:
    best_cfg = LoraConfig(
        r=best["r"], lora_alpha=best["alpha"], lora_dropout=best["dropout"],
        target_modules=best["target_modules"], bias="none",
        task_type=TaskType.SEQ_2_SEQ_LM,
    )

# Replace the base inside the wrapper with the LoRA-wrapped base
base = get_peft_model(base, best_cfg)
model = ForwardKwargSanitizer(base)
model.base.print_trainable_parameters()

# =========================================================
# Training args (ORIGINAL)
# =========================================================
from transformers import Seq2SeqTrainingArguments, TrainerCallback, pipeline

training_args = Seq2SeqTrainingArguments(
    output_dir="./V1",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=1e-5,
    warmup_steps=500,
    max_steps=1000,
    gradient_checkpointing=True,
    fp16=True,
    evaluation_strategy="steps",
    per_device_eval_batch_size=4,
    predict_with_generate=True,
    generation_max_length=225,
    seed=91,
    data_seed=91,
    logging_steps=5,
    eval_steps=5,
    save_strategy="steps",
    save_steps=5,
    save_total_limit=2,
    report_to=["none"],
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    dataloader_num_workers=8,
    dataloader_prefetch_factor=2,
)

# =========================================================
# Trainer + Train (ORIGINAL)
# =========================================================
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=vectorized_datasets["train"],
    eval_dataset=vectorized_datasets["test"],
    data_collator=data_collator,
    tokenizer=processor,
    compute_metrics=compute_metrics,
)
trainer.train()

# =========================================================
# Save (ORIGINAL)
# =========================================================
trainer.save_model("finetuned_model_v1")