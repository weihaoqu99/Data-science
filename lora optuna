# =========================================================
# Imports (original)
# =========================================================
from datasets import Dataset, Audio, concatenate_datasets
import glob
import librosa
import os
import numpy as np

# =========================================================
# Convert G.711 audio to PCM (original)
# =========================================================
def convert_g711_to_pcm(pathname):
    collection = []
    with open(pathname, 'rb') as f:
        while (inter := f.read(1)):
            inter = int(ord(inter))
            if inter == 127:
                collection.append(-1)
                continue
            inter = ~inter
            sign = inter & 0x80
            exponent = (inter & 0x70) >> 4
            data = inter & 0x0f
            data |= 0x10
            data <<= 1
            data += 1
            data <<= exponent + 2
            data -= 0x84
            data = data if sign == 0 else -data
            collection.append(data)

    collection = np.array(collection, dtype=np.float32) / 32768
    collection = librosa.resample(collection, orig_sr=8_000, target_sr=16_000, scale=False)
    return collection

# =========================================================
# Collect audio files + transcript cleaner (original)
# =========================================================
audio_files = glob.glob("origAudio/*.mp3")

def retrieve_transcript(input_str: str) -> str:
    tmp = input_str[input_str.find("-")+1:-4]  # strip track number + suffix
    tmp = tmp.replace("_", " ")
    tmp = tmp.replace(".", " ")
    tmp = tmp.replace("shell", "she'll")
    tmp = tmp.replace("dont", "don't")
    tmp = tmp.capitalize()
    tmp = tmp.replace("i ", "I ")
    return tmp

transcripts = [retrieve_transcript(s) for s in audio_files]
num_samples = len(audio_files)

raw_datasets = Dataset.from_dict({
    "audio": audio_files[:num_samples],
    "sentence": transcripts[:num_samples]
}).cast_column("audio", Audio(sampling_rate=16_000))

raw_datasets = raw_datasets.train_test_split(test_size=0.15, seed=91)

# =========================================================
# Processor (original)
# =========================================================
from transformers import WhisperProcessor
whisper_loc = "/appdata/cortex/dev4/shobha/ASR/whisper_copy/distil-large-v2"
processor = WhisperProcessor.from_pretrained(whisper_loc, language="English", task="transcribe")

# =========================================================
# Prepare dataset (original)
# =========================================================
def prepare_dataset(batch):
    # load / resample / convert g711->pcm (original flow)
    audio = batch["audio"]
    prefix = "/appdata/cortex/dev4/asr_finetuning/audio_for_finetuning_low_qual"
    audio["path"] = os.path.join(prefix, os.path.basename(audio["path"])).replace("mp3", "raw")
    audio["array"] = convert_g711_to_pcm(audio["path"])

    # features
    batch["input_features"] = processor.feature_extractor(
        audio["array"], sampling_rate=audio["sampling_rate"]
    ).input_features[0]

    # labels
    transcription = batch["sentence"]
    batch["labels"] = processor.tokenizer(transcription).input_ids
    return batch

vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets.column_names["train"],
    num_proc=1
)

# =========================================================
# Model (original)
# =========================================================
from transformers import WhisperForConditionalGeneration
model = WhisperForConditionalGeneration.from_pretrained(whisper_loc)

model.config.suppress_tokens = []
model.config.use_cache = False
model.config.language = "english"
model.config.task = "transcribe"

# =========================================================
# Data collator (original)
# =========================================================
import torch
from dataclasses import dataclass
from typing import Any, Dict, List, Union

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    decoder_start_token_id: int

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_features": f["input_features"]} for f in features]
        label_features = [{"input_ids": f["labels"]} for f in features]

        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # ignore pad in loss
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        # drop BOS if already prepended
        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(
    processor=processor,
    decoder_start_token_id=model.config.decoder_start_token_id,
)

# =========================================================
# Metrics (original)
# =========================================================
import evaluate
metric = evaluate.load("wer")

def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id

    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    wer = 100 * metric.compute(predictions=pred_str, references=label_str)
    return {"wer": wer}  # Trainer will expose as "eval_wer" during eval

# =========================================================
# (NEW) >>>>>>>>>>>>>>>>>>>>>>>  OPTUNA + LoRA BLOCK  <<<<<<<<<<<<<<<<<<<<<<<<<
# Runs a quick hyperparam search and then attaches best LoRA to *your* model.
# Placement: after model/processor/datasets/collator/metrics defined, before Trainer().
# =========================================================
import gc, json, math
import optuna
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments

# small, fast slices for search
TUNE_TRAIN_SAMPLES = min(200, len(vectorized_datasets["train"]))
TUNE_EVAL_SAMPLES  = min(100, len(vectorized_datasets["test"]))
tune_train = vectorized_datasets["train"].select(range(TUNE_TRAIN_SAMPLES))
tune_eval  = vectorized_datasets["test"].select(range(TUNE_EVAL_SAMPLES))

def _fresh_base():
    m = WhisperForConditionalGeneration.from_pretrained(whisper_loc)
    m = prepare_model_for_kbit_training(m)  # safe even if not using 8-bit
    m.gradient_checkpointing_enable()
    m.enable_input_require_grads()
    m.config.suppress_tokens = []
    m.config.use_cache = False
    m.config.language = "english"
    m.config.task = "transcribe"
    return m

def objective(trial: optuna.Trial) -> float:
    # search space
    r           = trial.suggest_categorical("r", [16, 32, 64])
    alpha       = trial.suggest_categorical("alpha", [16, 32, 64])
    dropout     = trial.suggest_float("dropout", 0.01, 0.15, step=0.02)
    use_rslora  = trial.suggest_categorical("use_rslora", [True, False])
    target_mods = trial.suggest_categorical("target_modules", [["q_proj","v_proj"], ["q_proj","k_proj","v_proj"]])

    temp_model = _fresh_base()

    # build LoRA config, RS-LoRA if supported
    try:
        lcfg = LoraConfig(
            r=r,
            lora_alpha=alpha,
            lora_dropout=dropout,
            target_modules=target_mods,
            bias="none",
            task_type=TaskType.SEQ_2_SEQ_LM,
            use_rslora=use_rslora,
        )
    except TypeError:
        lcfg = LoraConfig(
            r=r,
            lora_alpha=alpha,
            lora_dropout=dropout,
            target_modules=target_mods,
            bias="none",
            task_type=TaskType.SEQ_2_SEQ_LM,
        )

    temp_model = get_peft_model(temp_model, lcfg)

    args = Seq2SeqTrainingArguments(
        output_dir=f"./optuna_trial_{trial.number}",
        per_device_train_batch_size=2,
        per_device_eval_batch_size=2,
        num_train_epochs=1,
        logging_strategy="steps",
        logging_steps=10,
        evaluation_strategy="steps",
        eval_steps=50,
        save_strategy="no",
        predict_with_generate=True,
        generation_max_length=225,
        fp16=torch.cuda.is_available(),
        report_to=["none"],
        dataloader_num_workers=0,
    )

    t = Seq2SeqTrainer(
        model=temp_model,
        args=args,
        train_dataset=tune_train,
        eval_dataset=tune_eval,
        data_collator=data_collator,
        tokenizer=processor,
        compute_metrics=compute_metrics,
    )

    t.train()
    metrics = t.evaluate()
    # prefer "eval_wer" if Trainer prefixes; otherwise fall back to "wer"
    wer = float(metrics.get("eval_wer", metrics.get("wer", math.inf)))

    # cleanup
    del t, temp_model
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    trial.report(wer, step=0)
    if trial.should_prune():
        raise optuna.TrialPruned()
    return wer

study = optuna.create_study(
    direction="minimize",
    sampler=optuna.samplers.TPESampler(n_startup_trials=4, multivariate=True),
    pruner=optuna.pruners.MedianPruner(n_startup_trials=4),
)
study.optimize(objective, n_trials=10, show_progress_bar=False)

print("[Optuna] Best trial:", study.best_trial.number)
print("[Optuna] Best params:", study.best_trial.params)
print("[Optuna] Best WER:", study.best_trial.value)

os.makedirs("./optuna_results", exist_ok=True)
with open("./optuna_results/best_lora_params.json", "w") as f:
    json.dump(study.best_trial.params, f, indent=2)

# attach BEST LoRA to your in-memory model for real training
model = prepare_model_for_kbit_training(model)
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

best = study.best_trial.params
try:
    best_cfg = LoraConfig(
        r=best["r"],
        lora_alpha=best["alpha"],
        lora_dropout=best["dropout"],
        target_modules=best["target_modules"],
        bias="none",
        task_type=TaskType.SEQ_2_SEQ_LM,
        use_rslora=best.get("use_rslora", True),
    )
except TypeError:
    best_cfg = LoraConfig(
        r=best["r"],
        lora_alpha=best["alpha"],
        lora_dropout=best["dropout"],
        target_modules=best["target_modules"],
        bias="none",
        task_type=TaskType.SEQ_2_SEQ_LM,
    )

model = get_peft_model(model, best_cfg)
model.print_trainable_parameters()
# =====================  END OPTUNA BLOCK  =====================================

# =========================================================
# Training args (original)
# =========================================================
from transformers import Seq2SeqTrainingArguments, TrainerCallback, pipeline

training_args = Seq2SeqTrainingArguments(
    output_dir="./V1",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=1e-5,
    warmup_steps=500,
    max_steps=1000,
    gradient_checkpointing=True,
    fp16=True,
    evaluation_strategy="steps",
    per_device_eval_batch_size=4,
    predict_with_generate=True,
    generation_max_length=225,
    seed=91,
    data_seed=91,
    logging_steps=5,
    eval_steps=5,
    save_strategy="steps",
    save_steps=5,
    save_total_limit=2,
    report_to=["none"],
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    dataloader_num_workers=8,
    dataloader_prefetch_factor=2,
)

# =========================================================
# Trainer + Train (original)
# =========================================================
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=vectorized_datasets["train"],
    eval_dataset=vectorized_datasets["test"],
    data_collator=data_collator,
    tokenizer=processor,
    compute_metrics=compute_metrics,
    # callbacks=[CustomCallback()],  # you had this commented; keep as-is
)

trainer.train()

# =========================================================
# Save (original)
# =========================================================
trainer.save_model("finetuned_model_v1")