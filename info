# ========= Save full Whisper-style generation_config.json (and optionally merge LoRA) =========
import os, json
from pathlib import Path

from transformers import WhisperForConditionalGeneration, WhisperProcessor, GenerationConfig

# --- set these ---
MODEL_DIR  = "finetuned_model_OL1"         # your saved folder (full model OR adapter)
DO_MERGE   = False                         # set True to also create a merged full model folder
BASE_MODEL = "openai/whisper-large-v2"     # base model to use if MODEL_DIR is adapter-only (or leave None to auto-read from adapter_config.json)

Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)

def load_full_model_from_dir(dir_path: str):
    """Try to load a full Whisper model from a directory."""
    return WhisperForConditionalGeneration.from_pretrained(dir_path)

def ensure_processor(dir_path: str, base_or_loaded: str):
    """Make sure tokenizer/processor files exist."""
    try:
        proc = WhisperProcessor.from_pretrained(dir_path if base_or_loaded is None else base_or_loaded)
        proc.save_pretrained(dir_path)
    except Exception as e:
        print(f"ℹ️ Could not write processor files: {e}")

def save_full_generation_config(model, out_dir: str):
    """Save the *full* Whisper generation_config.json (includes lang_to_id, suppress_tokens, etc.)."""
    # If you want to enforce your preferred defaults, set them before saving:
    # (Uncomment if you want: these won't remove the big token lists.)
    # model.generation_config.max_length = 225
    # model.generation_config.num_beams  = 1
    # model.generation_config.do_sample  = False
    # model.generation_config.task       = "transcribe"
    # model.generation_config.language   = "english"

    model.generation_config.save_pretrained(out_dir)
    print(f"✅ generation_config.json saved to: {os.path.join(out_dir, 'generation_config.json')}")

def maybe_merge_adapter(adapter_dir: str, base_model_hint: str = None):
    """If adapter folder, optionally merge into a standalone full model and return merged dir."""
    from peft import PeftModel

    # Find base from adapter_config.json if available
    base_from_cfg = None
    acfg = Path(adapter_dir) / "adapter_config.json"
    if acfg.exists():
        with open(acfg, "r") as f:
            try:
                base_from_cfg = json.load(f).get("base_model_name_or_path")
            except Exception:
                pass

    base_to_use = base_from_cfg or base_model_hint
    if base_to_use is None:
        raise ValueError("Base model not provided and not found in adapter_config.json.")

    print(f"ℹ️ Loading base model: {base_to_use}")
    base_model = WhisperForConditionalGeneration.from_pretrained(base_to_use)

    print(f"ℹ️ Loading adapter from: {adapter_dir}")
    peft_model = PeftModel.from_pretrained(base_model, adapter_dir)

    print("ℹ️ Merging LoRA adapter into base weights...")
    merged = peft_model.merge_and_unload()

    merged_dir = f"{adapter_dir}_merged"
    Path(merged_dir).mkdir(parents=True, exist_ok=True)
    merged.save_pretrained(merged_dir)
    print(f"✅ Merged full model saved to: {merged_dir}")

    # Save processor alongside merged model (nice to have)
    try:
        ensure_processor(merged_dir, base_to_use)
    except Exception as e:
        print(f"ℹ️ Could not save processor to merged dir: {e}")

    return merged_dir

# ----------------- main logic -----------------
is_full_model = any(Path(MODEL_DIR).joinpath(x).exists() for x in ("pytorch_model.bin", "model.safetensors"))
is_adapter    = Path(MODEL_DIR).joinpath("adapter_model.safetensors").exists()

if is_full_model:
    print("ℹ️ Detected FULL Whisper model folder.")
    model = load_full_model_from_dir(MODEL_DIR)

    # Ensure config.json exists (harmless if it already does)
    model.config.save_pretrained(MODEL_DIR)
    ensure_processor(MODEL_DIR, None)

    # Save the full, verbose generation_config.json (with language maps & suppress lists)
    save_full_generation_config(model, MODEL_DIR)

elif is_adapter:
    print("ℹ️ Detected LoRA ADAPTER folder.")
    # You can still save a generation_config.json directly (it’s model-agnostic), but to get the
    # exact Whisper-style one from a model instance, load base+adapter (or merge if requested).
    if DO_MERGE:
        merged_dir = maybe_merge_adapter(MODEL_DIR, BASE_MODEL)
        # Load merged model and save full generation config there
        merged_model = load_full_model_from_dir(merged_dir)
        save_full_generation_config(merged_model, merged_dir)
    else:
        # Load base+adapter in memory only, then save generation_config to the adapter dir
        from peft import PeftModel

        # Choose base
        base_to_use = BASE_MODEL
        acfg = Path(MODEL_DIR) / "adapter_config.json"
        if acfg.exists():
            try:
                base_to_use = json.load(open(acfg)).get("base_model_name_or_path") or base_to_use
            except Exception:
                pass
        if base_to_use is None:
            raise ValueError("Please set BASE_MODEL or include base_model_name_or_path in adapter_config.json.")

        base_model = WhisperForConditionalGeneration.from_pretrained(base_to_use)
        peft_model = PeftModel.from_pretrained(base_model, MODEL_DIR)
        # Save full generation config (has the big Whisper fields) into the adapter dir
        save_full_generation_config(peft_model, MODEL_DIR)
        # Processor files (optional)
        ensure_processor(MODEL_DIR, base_to_use)

else:
    raise FileNotFoundError(
        f"'{MODEL_DIR}' does not look like a full model or a LoRA adapter folder.\n"
        f"Expected one of: pytorch_model.bin/model.safetensors or adapter_model.safetensors"
    )

print("Done.")
# =============================================================================================









from transformers import GenerationConfig
import os
from pathlib import Path

MODEL_DIR = "finetuned_model_OL1"  # your folder

# Fixed generation config
gen_cfg = GenerationConfig(
    max_length=225,
    num_beams=1,        # greedy decoding
    do_sample=False,
    temperature=1.0,
    top_p=1.0,
    top_k=50,
    length_penalty=1.0,
    # ❌ removed early_stopping=True (only valid with beam search)
    task="transcribe",
    language="english"
)

# Save into your model folder
Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)
gen_cfg.save_pretrained(MODEL_DIR)

print(f"✅ generation_config.json saved to {os.path.join(MODEL_DIR, 'generation_config.json')}")



# ========= Create generation_config.json (and optionally merge LoRA) =========
import os, json
from pathlib import Path

MODEL_DIR = "finetuned_model_OL1"        # <-- your saved folder
DO_MERGE  = False                        # set True if you also want to create a merged full model
BASE_MODEL = "openai/whisper-large-v2"   # or your local base path (e.g., whisper_loc)

# ---- 1) Always write generation_config.json for ASR defaults ----
from transformers import GenerationConfig
gen_cfg = GenerationConfig(
    max_length=225,      # match your training generation_max_length
    num_beams=1,         # greedy (typical for ASR)
    do_sample=False,     # deterministic
    temperature=1.0,
    top_p=1.0,
    top_k=50,
    length_penalty=1.0,
    early_stopping=True,
    task="transcribe",   # Whisper-friendly defaults
    language="english"
)
Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)
gen_cfg.save_pretrained(MODEL_DIR)
print(f"✅ Wrote {os.path.join(MODEL_DIR, 'generation_config.json')}")

# ---- 2) If MODEL_DIR already contains a full model, ensure config.json exists (optional) ----
try:
    has_full = any(Path(MODEL_DIR).joinpath(x).exists() for x in ["pytorch_model.bin", "model.safetensors"])
    if has_full:
        from transformers import WhisperForConditionalGeneration, WhisperProcessor
        model = WhisperForConditionalGeneration.from_pretrained(MODEL_DIR)
        # save/refresh config.json in place
        model.config.save_pretrained(MODEL_DIR)
        print(f"✅ Ensured config.json in {MODEL_DIR}")

        # (nice-to-have) ensure processor/tokenizer files present
        proc = WhisperProcessor.from_pretrained(MODEL_DIR)
        proc.save_pretrained(MODEL_DIR)
        print(f"✅ Ensured tokenizer/processor files in {MODEL_DIR}")
except Exception as e:
    print(f"ℹ️ Skipped config/processor refresh: {e}")

# ---- 3) Adapter-only folder? (adapter_model.safetensors present) ----
adapter_path = Path(MODEL_DIR).joinpath("adapter_model.safetensors")
if adapter_path.exists():
    print("ℹ️ Detected LoRA adapter folder.")
    # Still fine: we already saved generation_config.json into MODEL_DIR above.
    # Optionally: merge adapter into base → standalone full model folder.
    if DO_MERGE:
        try:
            from transformers import WhisperForConditionalGeneration
            from peft import PeftModel

            # If adapter_config.json has base path, prefer that
            adapter_cfg_file = Path(MODEL_DIR).joinpath("adapter_config.json")
            base_from_adapter = None
            if adapter_cfg_file.exists():
                with open(adapter_cfg_file, "r") as f:
                    acfg = json.load(f)
                base_from_adapter = acfg.get("base_model_name_or_path")

            base_to_use = base_from_adapter or BASE_MODEL
            print(f"ℹ️ Merging adapter with base: {base_to_use}")

            base_model = WhisperForConditionalGeneration.from_pretrained(base_to_use)
            peft_model = PeftModel.from_pretrained(base_model, MODEL_DIR)
            merged = peft_model.merge_and_unload()

            MERGED_DIR = f"{MODEL_DIR}_merged"
            Path(MERGED_DIR).mkdir(parents=True, exist_ok=True)
            merged.save_pretrained(MERGED_DIR)
            print(f"✅ Saved merged full model to {MERGED_DIR}")

            # write generation_config.json into merged folder, too
            gen_cfg.save_pretrained(MERGED_DIR)
            print(f"✅ Wrote {os.path.join(MERGED_DIR, 'generation_config.json')}")

            # (nice-to-have) save processor
            try:
                from transformers import WhisperProcessor
                proc = WhisperProcessor.from_pretrained(base_to_use)
                proc.save_pretrained(MERGED_DIR)
                print(f"✅ Saved tokenizer/processor into {MERGED_DIR}")
            except Exception as e:
                print(f"ℹ️ Could not save processor into merged dir: {e}")

        except Exception as e:
            print(f"❗ Merge requested but failed: {e}")

print("Done.")
# ======================================================================



#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Create generation_config.json for a fine-tuned Whisper model folder,
optionally verify by running a quick transcription on an audio file.

Usage:
  python make_generation_config.py --model-dir finetuned_model_OL1 \
      --max-length 225 --language english --task transcribe \
      --num-beams 1 --do-sample false --test-audio example.wav
"""

import argparse
import json
import os
import sys

import torch
from transformers import (
    WhisperForConditionalGeneration,
    WhisperProcessor,
    GenerationConfig,
)

def str2bool(v: str) -> bool:
    return str(v).lower() in {"1","true","t","yes","y"}

def parse_args():
    p = argparse.ArgumentParser(description="Create generation_config.json for a fine-tuned Whisper model")
    p.add_argument("--model-dir", required=True, help="Path to your fine-tuned model folder (e.g., finetuned_model_OL1)")
    # Generation config defaults (match your training / common ASR settings)
    p.add_argument("--max-length", type=int, default=225, help="Max generation length")
    p.add_argument("--num-beams", type=int, default=1, help="Beam search beams (1 = greedy)")
    p.add_argument("--do-sample", type=str, default="false", help="Use sampling (true/false). For ASR usually false.")
    p.add_argument("--temperature", type=float, default=1.0, help="Sampling temperature")
    p.add_argument("--top-p", type=float, default=1.0, help="Nucleus sampling p")
    p.add_argument("--top-k", type=int, default=50, help="Top-k sampling")
    p.add_argument("--length-penalty", type=float, default=1.0, help="Length penalty")
    p.add_argument("--early-stopping", type=str, default="true", help="Early stopping (true/false)")
    # Whisper-specific helpful defaults
    p.add_argument("--task", type=str, default="transcribe", help="Whisper task: transcribe or translate")
    p.add_argument("--language", type=str, default="english", help="Language string for Whisper decode")
    # Optional quick test
    p.add_argument("--test-audio", type=str, default=None, help="Optional path to a 16k WAV/MP3/FLAC for a quick transcription test")
    return p.parse_args()

def ensure_config(model_dir: str, model):
    """
    Ensure config.json exists. If not, save it.
    """
    cfg_path = os.path.join(model_dir, "config.json")
    if not os.path.exists(cfg_path):
        model.config.save_pretrained(model_dir)
        print(f"Created missing config.json at: {cfg_path}")

def ensure_processor(model_dir: str, processor):
    """
    Ensure processor/tokenizer files exist (nice to have).
    """
    needed = ["tokenizer.json", "tokenizer_config.json", "preprocessor_config.json", "special_tokens_map.json"]
    missing = [f for f in needed if not os.path.exists(os.path.join(model_dir, f))]
    if missing:
        processor.save_pretrained(model_dir)
        print(f"Saved processor files into {model_dir} (were missing: {missing})")

def write_generation_config(model_dir: str, args):
    gen_cfg = GenerationConfig(
        max_length=args.max_length,
        num_beams=args.num_beams,
        do_sample=str2bool(args.do_sample),
        temperature=args.temperature,
        top_p=args.top_p,
        top_k=args.top_k,
        length_penalty=args.length_penalty,
        early_stopping=str2bool(args.early_stopping),
        # Whisper-friendly extras:
        task=args.task,
        language=args.language,
    )
    gen_cfg.save_pretrained(model_dir)
    print(f"✅ generation_config.json written to: {model_dir}")

def quick_transcribe(model_dir: str, audio_path: str):
    """
    Optional quick transcription to verify everything works and defaults are picked up.
    """
    if not audio_path or not os.path.exists(audio_path):
        print("Skip test: audio not provided or not found.")
        return

    import librosa
    # Load model & processor
    model = WhisperForConditionalGeneration.from_pretrained(model_dir)
    processor = WhisperProcessor.from_pretrained(model_dir)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)

    speech, sr = librosa.load(audio_path, sr=16000)
    inputs = processor.feature_extractor(speech, sampling_rate=16000, return_tensors="pt").input_features.to(device)

    # generate() will automatically pick up generation_config.json now
    with torch.no_grad():
        pred_ids = model.generate(inputs)

    text = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)[0]
    print("\n=== Quick transcription test ===")
    print(f"Audio: {audio_path}")
    print(f"Text : {text}\n")

def main():
    args = parse_args()
    model_dir = args.model_dir

    if not os.path.isdir(model_dir):
        print(f"ERROR: Model directory not found: {model_dir}")
        sys.exit(1)

    # Load model & processor (works even if config.json is already present/missing)
    model = WhisperForConditionalGeneration.from_pretrained(model_dir)
    processor = WhisperProcessor.from_pretrained(model_dir)

    # Ensure config.json & processor files exist (harmless if they already do)
    ensure_config(model_dir, model)
    ensure_processor(model_dir, processor)

    # Write generation_config.json
    write_generation_config(model_dir, args)

    # Optional quick test
    if args.test_audio:
        quick_transcribe(model_dir, args.test_audio)

    print("Done.")

if __name__ == "__main__":
    main()









m

from transformers import WhisperForConditionalGeneration

# Load back your fine-tuned model folder
model = WhisperForConditionalGeneration.from_pretrained("finetuned_model_OL1")

# Force-save the config.json
model.config.save_pretrained("finetuned_model_OL1")
print("✅ config.json created in finetuned_model_OL1/")


from transformers import GenerationConfig

# Build generation config based on your training args
gen_config = GenerationConfig(
    max_length=225,       # you used generation_max_length=225
    num_beams=1,          # greedy decoding
    do_sample=False,      # deterministic
    temperature=1.0,
    top_p=1.0,
    top_k=50,
    length_penalty=1.0,
    early_stopping=True,

    # Whisper-specific defaults (helpful for reloading later)
    task="transcribe",
    language="english"
)

# Save into your fine-tuned model folder
gen_config.save_pretrained("finetuned_model_OL1")

print("✅ generation_config.json created in finetuned_model_OL1/")






1. Integrate the Metrics in Web App

🎯 Goal:
To develop a web-based application for evaluating Spanish–English translation models using BLEU, CHRF, CHRF++, and BERTScore. This allows internal teams to consistently benchmark translation quality and automate evaluation workflows.

📐 Architecture:
Users upload translation datasets through the web interface. The FastAPI backend routes the data to a metrics engine that calculates BLEU, CHRF, CHRF++, and BERTScore. Results are returned as downloadable Excel or JSON.

🧰 Technologies Used:
Python, FastAPI, HuggingFace, Pandas, Excel I/O, JupyterLab
👤 End User: Erica at Bank of America

⸻

✅ 2. Spanish LORA Finetuning with Bias Hyperparameters

🎯 Goal:
Improve translation performance by applying bias-aware hyperparameters during LORA model fine-tuning, tailored to IVR-style Spanish-to-English data.

📐 Architecture:
IVR data is tokenized and passed to a LORA-configured model with bias settings. The model is fine-tuned and evaluated using BLEU/BERTScore to confirm improvements over the base version.

🧰 Technologies Used:
Python, HuggingFace Transformers, PEFT, Optuna
👤 End User: Erica at Bank of America

⸻

✅ 3. Spanish LORA Finetuning with lora_dropout

🎯 Goal:
Enhance model generalization and reduce overfitting by introducing dropout in LORA fine-tuning for Spanish–English translations.

📐 Architecture:
The dataset is prepared and passed through a fine-tuning pipeline with lora_dropout applied. Model outputs are validated against prior versions for accuracy and consistency.

🧰 Technologies Used:
Python, Transformers, PyTorch, PEFT
👤 End User: Erica at Bank of America

⸻

✅ 4. Finetuning the Spanish Model with Hyperparameters

🎯 Goal:
Refine model accuracy by tuning various training parameters such as batch size, learning rate, and training epochs.

📐 Architecture:
The base model is trained using Optuna-generated hyperparameter sets. After training, evaluation scores are compared to select the best-performing configuration.

🧰 Technologies Used:
Python, HuggingFace Transformers, Optuna, Madlad
👤 End User: Erica at Bank of America

⸻

✅ 5. Spanish LORA Finetuning with Warmup Steps & Label Smoothing

🎯 Goal:
Stabilize training and reduce prediction confidence issues by applying warm-up steps and label smoothing to the training process.

📐 Architecture:
Tokenized input is fed into a LORA-based model with warm-up scheduler and label smoothing enabled. The model is fine-tuned and evaluated for improvements in precision and robustness.

🧰 Technologies Used:
Python, Transformers, PEFT, PyTorch
👤 End User: Erica at Bank of America

⸻

✅ 6. Finetuning the Spanish Model

🎯 Goal:
Apply new fine-tuning methods on existing Spanish–English models to achieve improved translation performance for IVR use cases.

📐 Architecture:
A dataset of IVR conversations is fine-tuned using a pre-trained base model. Performance metrics are used to validate the effectiveness of the updated training techniques.

🧰 Technologies Used:
Python, HuggingFace, Madlad, PyTorch
👤 End User: Erica at Bank of America

⸻

✅ 7. Enable Endpoint for Spanish Model

🎯 Goal:
Deploy the fine-tuned Spanish translation model as a REST API for seamless system integration.

📐 Architecture:
FastAPI is used to expose the model via a /translate endpoint. Incoming Spanish text is tokenized and passed to the model. Translated English text is returned as a response.

🧰 Technologies Used:
Python, FastAPI, HuggingFace Transformers
👤 End User: Erica at Bank of America

⸻

✅ 8. Evaluation Library & BERT Score Changes

🎯 Goal:
Upgrade the internal evaluation library with the latest BERTScore and CHRF++ computation logic to ensure alignment with modern Transformer models.

📐 Architecture:
Reference and predicted text are processed using the updated scoring logic. Outputs are written in structured formats (Excel, JSON) for reporting and integration.

🧰 Technologies Used:
Python, BERTScore, evaluate, CHRF++, BLEU
👤 End User: Erica at Bank of America

⸻

✅ 9. Latency / Response Time Evaluation – Madlad vs Helsinki

🎯 Goal:
Determine the better-performing model in terms of latency and translation accuracy between Madlad and Helsinki.

📐 Architecture:
Each model processes the same test dataset. Latency and BLEU/CHRF scores are logged and compared to select the more optimal model for production use.

🧰 Technologies Used:
Python, HuggingFace, BLEU, CHRF++, BERTScore
👤 End User: Erica at Bank of America

⸻

✅ 10. Refine the Training Data of Spanish Model

🎯 Goal:
Enhance the dataset by adding domain-specific terms from the BAC glossary to improve translation accuracy on financial vocabulary.

📐 Architecture:
The training dataset is enriched with glossary terms (e.g., IRA, Zelle). The updated dataset is fine-tuned and evaluated to determine performance improvements.

🧰 Technologies Used:
Python, Pandas, Tokenizer, Madlad
👤 End User: Erica at Bank of America

⸻

✅ 11. CPS MRM Support

🎯 Goal:
Ensure the CPS summarization model adheres to MRM (Model Risk Management) guidelines by analyzing failure logs and accuracy.

📐 Architecture:
Logs are reviewed against ground truth. Root causes for discrepancies are identified, and MRM-compliant reporting is generated for tracking.

🧰 Technologies Used:
Python, Log Analysis, Documentation
👤 End User: Erica at Bank of America

⸻

✅ 12. Merged LORA Model with Tokenizer

🎯 Goal:
Create a production-ready model by merging adapter weights into the base model and bundling it with the tokenizer.

📐 Architecture:
Using merge_and_unload(), the adapter and base model are combined. The tokenizer is saved with the model, ensuring reproducibility and consistency in deployment.

🧰 Technologies Used:
Python, PEFT, Transformers
👤 End User: Erica at Bank of America

⸻

✅ 13. IVR Test Data for Spanish

🎯 Goal:
Validate that Spanish IVR test scripts produce expected translations and understand intent correctly.

📐 Architecture:
Test cases are run through the model. Generated outputs are validated against expected answers to verify model reliability and fluency.

🧰 Technologies Used:
Python, CSV/Excel, Manual Validation
👤 End User: Erica at Bank of America

⸻

✅ 14. CPS Model Evaluation

🎯 Goal:
Evaluate CPS (Call Purpose Summarization) models for accuracy, hallucination control, and one-liner summary quality.

📐 Architecture:
Real customer-agent dialogue turns are summarized by the CPS model. Results are benchmarked for beam accuracy, hallucination rates, and clarity of the generated one-liner summaries.

🧰 Technologies Used:
Python, NLP Metrics, Conversation Data
👤 End User: Erica at Bank of America

⸻

✅ 15. Spanish Model Evaluation

🎯 Goal:
Build a Python library to standardize evaluation of Spanish translation outputs using BLEU, CHRF, and CHRF++ metrics.

📐 Architecture:
The user provides reference and predicted translations. The library computes all metrics and returns structured results. This helps in automated comparison of model performance.

🧰 Technologies Used:
Python, BLEU, CHRF, CHRF++, Pandas
👤 End User: Erica at Bank of America

⸻

✅ 16. Onboarding Document

🎯 Goal:
Update the internal onboarding guide to reflect new workflows, tools, libraries, and configurations introduced across translation and evaluation pipelines.

📐 Architecture:
Legacy documents were reviewed, and revised with new tools, setup steps, and usage guides to ensure smooth onboarding for new team members.

🧰 Technologies Used:
Markdown, Word, Confluence/Wiki
👤 End User: Erica at Bank of America
