Just a quick update: I have completed the planned reviews and shared the old and updated scripts, along with the MRM responses, with g, I also discussed the endpoint comparisons, and we agreed to keep the current setup unchanged since performance is similar across endpoints.

I will be on CTA starting tomorrow and will be back on the 2nd. I will resume work once I return. If anything urgent comes up during this period, I am just a call away.



Based on the sensitivity testing conducted across 1,752 inference-time parameter configurations and 876,000 generated summaries, the CPS Transformer model demonstrates robust and stable performance with respect to inference parameter variations.

Quantitative evaluation using BERTScore indicates that the current production (Original CPS) inference configuration consistently achieves equal or superior performance relative to all alternative deterministic and sampling-based configurations tested. No tested parameter combination resulted in a statistically or materially meaningful improvement over the production baseline.

Deterministic decoding configurations (do_sample = False) exhibited lower variability and more stable BERTScore outcomes, while sampling-based configurations (do_sample = True) introduced increased variability and, in several cases, performance degradation relative to the baseline. Importantly, none of the sampled configurations exceeded the baseline BERTScore observed under the production parameters.

These results confirm that the model’s outputs are not unduly sensitive to inference-time hyperparameter changes, satisfying the robustness expectations outlined in MRM Section 2.7.5. Accordingly, the existing production inference parameters are deemed appropriate, well-calibrated, and fit for continued use, with no identified need for parameter adjustment or remediation.




Rationale for Inference Configuration Choices

Why do_sample = False Configurations Were Included (Deterministic Generation)

The do_sample = False configurations represent deterministic inference behavior, where the model produces the same output for a given input under fixed parameters. These settings are commonly used in production environments that prioritize:
	•	Output consistency and reproducibility
	•	Reduced variance across repeated inferences
	•	Stable behavior for compliance-sensitive applications

By evaluating multiple num_beams and max_length values under deterministic decoding, the analysis assesses whether summary quality and semantic alignment remain stable as beam search depth and output length constraints vary. This ensures the model’s robustness under non-stochastic, controlled inference settings, which are frequently preferred for enterprise deployments.

⸻

Why do_sample = True Configurations Were Included (Stochastic Generation)

The do_sample = True configurations represent sampling-based inference, which introduces controlled randomness into generation. These settings are relevant because:
	•	They are widely used in exploratory and summarization tasks to improve linguistic diversity
	•	Temperature, top-k, and top-p sampling directly influence creativity vs. faithfulness trade-offs
	•	Real-world usage may involve sampling to avoid repetitive or overly generic summaries

Evaluating sampling-based configurations allows the team to assess whether semantic fidelity, as measured by BERTScore, degrades or remains stable under stochastic decoding. This directly addresses inference-time sensitivity to randomness, which is a key concern highlighted in MRM guidance.

⸻

Rationale for Selection of Parameter Combinations

The parameter grid was intentionally designed to balance coverage, realism, and computational feasibility:
	•	Parameter ranges (e.g., temperature 0.1–1.0, top_p 0.5–0.9, num_beams 1–6) reflect industry-standard and production-realistic values
	•	Extremes were avoided to exclude configurations that are known to produce degenerate or unusable outputs
	•	Multiple interacting parameters were varied jointly to capture non-linear interactions between decoding controls
	•	max_length was varied across short and long summaries to ensure robustness across different call durations and content densities

This structured grid ensures broad coverage of the practical inference parameter space, rather than isolated or artificially constrained tests.

⸻

Conclusion

The sensitivity analysis demonstrates that the CPS Transformer model exhibits robust and stable behavior across a wide range of inference-time configurations, including both deterministic and sampling-based decoding strategies.

Across 876,000 generated summaries, BERTScore comparisons against ground truth references show that model performance remains consistent and resilient to reasonable variations in inference parameters. No material degradation or instability was observed that would indicate undue sensitivity to specific parameter choices.

This comprehensive evaluation satisfies the intent of MRM Section 2.7.5 by validating inference-time robustness and provides strong evidence that the model’s production behavior is stable under both controlled and stochastic generation settings.




Overall Annotator Performance Gap = Under the old process, the total average performance difference between annotator 1 and annotator 2 was about 6%, whereas with the new annotation guidelines the difference reduced to less than 1%, indicating significantly improved alignment.




Hallucination Cohen’s kappa = Measures how consistently annotator 1 and annotator 2 classify hallucinations, accounting for agreement that could occur by chance.


Accuracy Weighted Cohen’s kappa – quadratic weighting with accuracy for annotator 1 and 2. Quadratic weighting was chosen to penalize more harshly when values are wider apart, given the small scale of accuracy (0–5) and its ordinal nature. Linear works best on a continuous variable.




To assess the root cause of low inter-annotator consistency and to evaluate whether the new annotation process improves agreement between annotators, the model development team conducted a detailed analysis using the same dataset annotated under both the old and the new annotation guidelines.

The updated annotation process includes:
	•	Removal of model-generated summaries from annotator view to ensure independence.
	•	Refined definitions of Accuracy (0–5 scale) and Hallucination (0/1), with clearer examples.
	•	Calibration sessions to align annotator interpretation across lines of business.

Using these annotations, we evaluated agreement through standard statistical measures:
	1.	Cohen’s Kappa (Hallucination): Measures degree of agreement beyond chance.
	2.	Weighted Cohen’s Kappa (Accuracy): Quadratic weighting penalizes larger scoring differences.
	3.	Accuracy Ties & Hallucination Ties: Count of instances where annotators assigned the exact same score.
	4.	Average Difference & Sum Difference: Reflects how far apart annotators’ scores are when they disagree.

These metrics collectively determine whether annotators are rating summaries consistently using the new process.

⸻

Results:

The comparison between the Old and New annotation methods shows measurable improvement across nearly all consistency Overall, the results indicate greater inter-annotator agreement under the new annotation process, demonstrated by higher Cohen’s kappa scores and increased number of ties. The improvements directly address MRM’s concern regarding the inconsistency of human annotations and provide clear evidence that the updated guidelines and processes produce more reliable, stable annotation quality.



Overall, the results indicate greater inter-annotator agreement under the new annotation process, demonstrated by higher Cohen’s kappa scores and increased number of ties. The improvements directly address MRM’s concern regarding the inconsistency of human annotations and provide clear evidence that the updated guidelines and processes produce more reliable, stable annotation quality.





Response

To address RAI04 regarding sensitivity testing of model inference parameters, the model development team conducted a comprehensive sensitivity analysis on the CPS Transformer model. A random sample of 500 transcripts was selected from four months of production data (May, June, July, and August) to ensure representation across multiple time periods and LOB distributions.

The sensitivity analysis followed the MRM guidance in Section 2.7.5, which recommends validating model robustness against variations in inference-time hyperparameters. Consistent with this guidance, the team evaluated a wide configuration grid covering both deterministic and sampling-based generations.

A total of 1,752 unique inference parameter combinations were generated by varying the following key parameters:
	•	do_sample (True / False)
	•	num_beams
	•	temperature
	•	top_k
	•	top_p
	•	repetition_penalty
	•	no_repeat_ngram_size
	•	max_length

Each of the 1,752 configurations was executed for all 500 production transcripts, producing a total of 876,000 generated summaries. This scale ensures high statistical confidence and complete coverage of the inference parameter space.

⸻

Methodology

1. Random Sampling of Production Data
	•	500 transcripts were randomly selected from production months May, June, July, and August.
	•	Sampling ensured diversity across callers, intents, summary lengths, and accuracy ratings.

2. Configuration Grid

The inference settings included:

Deterministic Configurations (do_sample = False)
	•	num_beams: 1, 2, 4, 6
	•	max_length: 30, 50, 100, 200, 300, 400
	•	Total deterministic configs: 24

Sampling Configurations (do_sample = True)
	•	temperature: 0.1, 0.7, 0.9, 1.0
	•	top_k: 50, 100, 200
	•	top_p: 0.5, 0.8, 0.9
	•	no_repeat_ngram_size: 3, 5
	•	repetition_penalty: 1.0, 1.1, 1.5, 1.8
	•	max_length: 30, 50, 100, 200, 300, 400
	•	Total sampling configs: 1,728

Total Configurations
24 (deterministic) + 1,728 (sampling) = 1,752 configurations
Total Generations
500 samples × 1,752 configs = 876,000 generated summaries

4. Evaluation Metric

All generated summaries were evaluated using BERTScore, following the definition and baseline properties established in prior CPS documentation.

BERTScore was computed for:
	•	Ground Truth 1 vs Generated summary
	•	Ground Truth 2 vs Generated summary (where applicable)
	•	Comparison across parameter settings

The objective was to measure:
	•	semantic stability
	•	robustness to inference hyperparameters
	•	sensitivity to generation randomness













Out of 223 evaluated samples, we identified very few Contact Center – Other non-fluent summaries. Therefore, we combined four months of production data (3004 samples), passed them through the Transformer model (trained in November 2025), and focused on evaluating summaries with intents other than contact_center_other.

From the 3004 evaluated samples, 143 were identified as not contact_center_other, and 20 were classified as non-fluent (13.9%). Among these 20 non-fluent summaries, 18 produced identical intent predictions when compared with their corrected/fluent summaries.


They entered the telephone number themselves
Remove the five-star rating from the Apple page
Received $50 from my son yesterday at Cornell
Enter or reset the dot-password
Pay the service fee before walking to the store
Go back to the branch because I did not receive a callback
Zellatron account action request
I can handle it like I did with the lawyer and the police
Unlock my account so I can go to the store
Access the best features of my account
Donna doesn’t think the other one on 14 is a client
Give me one second — I need more clarity to review this
There is nothing for the 119 games
Okay, I see the $1,060 and $2,000 transaction now
Sean Riley sent it, but the system flagged it as a virus
The money will not be sent
Mohammed Bye’s pickup truck was stored at the Allen Mall


A transcript has multiple intents or topics when the speaker discusses more than one main purpose, task, or issue within the same call.
These are clearly different from each other and are not part of the same flow.



As indicated in TS28, some transcripts in the production data contain multiple topics or intents, making it difficult for the model to generate a single, clear summary. To address this, we plan to introduce a simple annotation process provided by the business team. For each transcript, we will capture two basic indicators:
	1.	Input Intents: Does the sample have multiple topics or intents in the input? (Yes/No)
	2.	Output Intents: Does the model’s summary contain multiple topics or intents? (Yes/No)

These simple flags will help track how often multiple intents occur and ensure that the model’s summaries accurately represent the main topic. The results will be reviewed regularly as part of ongoing model monitoring and improvement.





As indicated in TS28, there are transcripts with multiple intents in the production data, and the model is not always able to create a single-intent summary with the most relevant intent. To address this, we proposed the following enhancements as part of the ongoing monitoring plan: (1) capturing the existence of multi-intents (Yes/No) by labeling each transcript with a binary LOB indicator to identify whether multiple intents are present or only a single intent exists. This helps in tracking the frequency and distribution of multi-intent cases and ensures the model’s predictions capture all relevant intents accurately, improving summarization quality and interpretability. (2) extracting and reviewing intent-bearing utterances from the call transcripts to verify that the model correctly associates each phrase or sentence with the corresponding intent. This step helps identify patterns where the model may miss or merge intents, allowing targeted improvements. (3) establishing ground-truth datasets that clearly mark all identified intents and primary intents for each transcript. These datasets will be validated using a random monthly sample of 100 production calls from each line of business. The results will be analyzed and included in the ongoing monitoring reports to ensure continuous validation, consistent tracking, and refinement of the model’s intent detection performance across CPS models.





# July production data with 200 valid random samples
import random

# Filter out any rows that have missing 'Cumulative Transcript'
valid_samples = July_samples[July_samples['Cumulative Transcript'].notna()].copy()

# Randomly sample exactly 200 valid entries (or fewer if not enough valid rows exist)
July_samples_200 = valid_samples.sample(n=min(200, len(valid_samples)), random_state=42)

print(f"Selected {len(July_samples_200)} valid samples for generation...")

# Run the inference process
result = process_dataset_parallel(
    July_samples_200,
    'Cumulative Transcript',
    configs,
    model='CPS',
    max_workers=35
)

# Save results
result.to_csv('July_all_exp200.csv', index=False)
print("✅ Saved predictions for 200 valid random samples.")








Yesterday, after the refinement call, I connected with Jay and we discussed three key points.

For the first two points — multi-intent versus single-intent and the BERT Score — Jay will be connecting with the MRM team to gather additional details.

For the sensitivity analysis, I’ve started running experiments with different parameter settings — specifically testing do_sample as True and False, max_length values of 200, 300, and 400, and repetition_penalty values of 1.1, 1.2, 1.5, and 1.8.

Right now, I’m running these tests on the first 100 records from the July dataset and evaluating the results using BERT Score metrics. Once the comparison is complete, I’ll review the impact and share the findings.”



Technical Report

Project: Spanish-to-English Neural Machine Translation – LoRA Fine-Tuning, Evaluation, and Deployment
Author: [Your Name / Team]
Date: [Insert Date]

⸻

1. Introduction

This report documents the end-to-end development of a Spanish-to-English neural machine translation (NMT) system using LoRA fine-tuning.
The work focused on improving translation accuracy for domain-specific and conversational text and creating a comprehensive evaluation and deployment pipeline.

The main components of this project are:
	1.	Fine-tuning models using LoRA techniques
	2.	Experimentation with hyperparameters using Optuna
	3.	Automated evaluation using BLEU, chrF/chrF++, and BERTScore
	4.	Development of APIs for translation and evaluation
	5.	Latency benchmarking and deployment readiness

⸻

2. Objectives and Motivation

Motivation
	•	Why improve Spanish-to-English translation?
Many off-the-shelf models perform poorly for specialized domains such as finance, call center conversations, and IVR data. The translations lacked fluency and consistency, which directly impacts downstream tasks.
	•	Goal:
	•	Improve translation accuracy
	•	Provide a consistent automated evaluation pipeline
	•	Build a deployable API-based solution

⸻

3. Models Used

3.1 Helsinki-NLP (MarianMT)

This model served as the primary baseline. It is optimized for high-quality Spanish↔English translations.

3.2 MADLAD 400 3B MT

The MADLAD multilingual model was also fine-tuned.
This model supports multiple languages and was chosen to see if multilingual pretraining offers an advantage.

Both models were downloaded and stored locally (MADLAD saved in a dedicated models folder for use during training and inference).

⸻

4. Methodology

4.1 LoRA Fine-Tuning

LoRA Variants
	•	RA LoRA (Residual Adaptation) – applied to Helsinki model
	•	RS LoRA (Residual Scaling) – applied to MADLAD model

Parameters Explored
	•	Rank: 8, 16, 32, 64, 128, 256, 512
	•	Alpha: 8, 16, 32, 64, 128, 256, 512
	•	Warmup Steps: 100, 500, 1000
	•	Label Smoothing: 0.0, 0.1, 0.2
	•	LoRA dropout and bias configurations also tested.

Automation

The Optuna library was used to systematically search through the parameter space to find the best configurations.

⸻

4.2 Evaluation Metrics

Custom Evaluation Library

A custom library was developed to compute:
	•	R-BLEU
	•	chrF
	•	chrF++

These metrics compare reference translations and candidate translations across different test datasets.

BERTScore
	•	Model: RoBERTa-large from Hugging Face
	•	Library: Hugging Face evaluate library
	•	Metrics:
	•	Precision
	•	Recall
	•	F1-score

The F1-score was the primary metric used to evaluate semantic similarity.

⸻

4.3 Types of Data Evaluated

The evaluation pipeline was applied to:
	1.	1000 transformer consumer records
	2.	749 IVR HL transformer prediction records

This ensured that the model was tested across different types of real-world data.

⸻

4.4 API Development

Two main endpoints were developed:
	1.	Spanish-to-English Translation Endpoint
	•	Provides real-time translation using fine-tuned Helsinki and MADLAD models.
	2.	BERTScore Evaluation Endpoint
	•	Accepts reference and candidate text inputs and returns precision, recall, and F1-score.

These endpoints were integrated into a web application to allow automated evaluation and translation.

⸻

4.5 Latency and GPU Utilization
	•	Latency measurements were carried out for both MADLAD and Helsinki models.
	•	GPU resources:
	•	Multiple GPUs (0, 1, 2, and 3) were used based on configuration.
	•	Results showed that the Helsinki model had lower latency than MADLAD.

⸻

5. Experiments

The experiments can be grouped into these major steps:
	1.	Hyperparameter Sweeps
	•	Conducted systematic experiments for rank and alpha values (8–512).
	2.	Effect of Bias and Dropout
	•	Explored bias parameters and LoRA dropout options.
	3.	Label Smoothing and Warmup Steps
	•	Experimented with different label smoothing values and warmup step configurations.
	4.	Comparative Latency Evaluation
	•	Benchmarked the time taken for translations between MADLAD and Helsinki.
	5.	Dataset-Specific Evaluations
	•	Evaluated all metrics on consumer data and IVR HL data.

⸻

6. Key Findings

Best Hyperparameters
	•	Helsinki:
	•	RA LoRA True
	•	Rank = 256, Alpha = 32
	•	Warmup Steps = 500, Label Smoothing = 0.1
	•	MADLAD:
	•	RS LoRA True
	•	Rank = 256, Alpha = 64

⸻

Evaluation Results (Final Models)

Helsinki:
	•	BLEU: 30+
	•	chrF: 54%
	•	BERTScore (F1): 0.88 (Precision and Recall also calculated)

MADLAD:
	•	BLEU: +8 over baseline
	•	chrF: 50%
	•	BERTScore (F1): 0.86

Latency:
	•	Helsinki model is faster and more suitable for deployment.

⸻

7. Contributions
	•	Designed and fine-tuned RA and RS LoRA models for Spanish-to-English translation.
	•	Built a translation evaluation library for BLEU, chrF, and chrF++ metrics.
	•	Implemented BERTScore API endpoint using RoBERTa-large for semantic evaluation.
	•	Created a Spanish-to-English translation API endpoint.
	•	Integrated these endpoints in a web app for automated evaluation.
	•	Conducted latency benchmarking and GPU resource optimization.
	•	Performed large-scale evaluation on 1000 consumer records and 749 IVR HL prediction records.

⸻

8. Outcome and Future Work
	•	The Helsinki RA LoRA fine-tuned model has been selected for production deployment due to superior accuracy and speed.
	•	Future enhancements:
	•	Model ensembling (combining Helsinki and MADLAD models).
	•	Active learning and incremental updates with new data.
	•	Adding COMET metric and human evaluation.

⸻

9. Conclusion

This project delivered:
	•	Improved translation accuracy through fine-tuning with LoRA
	•	Automated evaluation pipelines for translation quality
	•	Web-based API endpoints for translation and evaluation
	•	Latency-optimized, GPU-enabled deployment

The combination of high-quality evaluation, hyperparameter optimization, and API deployment ensures the system is robust and production-ready