# =========================
# YOUR ORIGINAL CODE (unchanged)
# =========================
import os
os.environ['CUDA_VISIBLE_DEVICES'] = "1"

from datasets import Dataset, Audio
import glob

audio_files = glob.glob("/appdata/cortex/dev1/origAudio/*.mp3")

def retrieve_transcript(input_str: str) -> str:
    tmp = input_str[input_str.find("-")+1:-4]  # strip out track number and .mp3 suffix
    tmp = tmp.replace('_', ' ')
    tmp = tmp.replace('.', ' ')
    tmp = tmp.replace("shell", "she'll")
    tmp = tmp.replace("dont", "don't")
    tmp = tmp.capitalize()
    tmp = tmp.replace(' i ', ' I ')
    return tmp

transcripts = [retrieve_transcript(s) for s in audio_files]

# Need to adjust sampling rate if necessary
num_samples = len(audio_files)

raw_datasets = Dataset.from_dict({
    "audio": audio_files[:num_samples],
    "sentence": transcripts[:num_samples]
}).cast_column("audio", Audio(sampling_rate=16_000))

raw_datasets = raw_datasets.train_test_split(test_size=0.15, seed=91)
raw_datasets
# =========================
# END OF YOUR ORIGINAL CODE
# =========================


# >>> NEW CHANGE (OPTUNA): installs/imports and utility setup for hyperparameter search
# If Optuna isn't installed in your env, uncomment the next two lines once:
# import sys, subprocess
# subprocess.check_call([sys.executable, "-m", "pip", "install", "optuna>=3.6.0"])

import math
import gc
import torch
import evaluate
import optuna
from optuna.samplers import TPESampler
from optuna.pruners import MedianPruner

from dataclasses import dataclass
from typing import Any, Dict, List, Union

from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
)

# Reuse your dataset split above; just wire in processor/model/collator/metrics.
whisper_loc = "/appdata/cortex/dev1/whisper_copy/distil-large-v2"
processor = WhisperProcessor.from_pretrained(whisper_loc, language="English", task="transcribe")

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    decoder_start_token_id: int
    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # inputs
        input_features = [{"input_features": f["input_features"]} for f in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
        # labels
        label_features = [{"input_ids": f["labels"]} for f in features]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
            labels = labels[:, 1:]
        batch["labels"] = labels
        return batch

def prepare_dataset(batch):
    audio = batch["audio"]
    batch["input_features"] = processor.feature_extractor(
        audio["array"], sampling_rate=audio["sampling_rate"]
    ).input_features[0]
    transcription = batch["sentence"]
    batch["labels"] = processor.tokenizer(transcription).input_ids
    return batch

# Vectorize once (shared for all trials)
vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets["train"].column_names,
    num_proc=8
)

wer_metric = evaluate.load("wer")

def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id
    pred_str  = processor.tokenizer.batch_decode(pred_ids,  skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)
    wer = 100 * wer_metric.compute(predictions=pred_str, references=label_str)
    return {"wer": wer}

# For reproducibility in the search
GLOBAL_SEED = 91
torch.manual_seed(GLOBAL_SEED)

# Objective function for Optuna
def objective(trial: optuna.trial.Trial) -> float:
    # ---- Hyperparameter search space
    lr = trial.suggest_float("learning_rate", 5e-6, 5e-5, log=True)
    grad_accum = trial.suggest_categorical("gradient_accumulation_steps", [4, 8, 16])
    train_bs = trial.suggest_categorical("per_device_train_batch_size", [2, 4])
    eval_bs = trial.suggest_categorical("per_device_eval_batch_size", [2, 4])
    warmup_steps = trial.suggest_int("warmup_steps", 100, 1000, step=100)
    max_steps = trial.suggest_int("max_steps", 200, 1200, step=100)
    gen_max_len = trial.suggest_int("generation_max_length", 128, 256, step=16)

    # ---- Model per trial
    model = WhisperForConditionalGeneration.from_pretrained(whisper_loc)
    model.config.forced_decoder_ids = None
    model.config.suppress_tokens = []
    model.config.use_cache = False  # compatible with grad checkpointing
    model.config.language = "english"
    model.config.task = "transcribe"

    data_collator = DataCollatorSpeechSeq2SeqWithPadding(
        processor=processor,
        decoder_start_token_id=model.config.decoder_start_token_id
    )

    # ---- Training args per trial
    training_args = Seq2SeqTrainingArguments(
        output_dir=f"./optuna_trial_{trial.number}",
        per_device_train_batch_size=train_bs,
        per_device_eval_batch_size=eval_bs,
        gradient_accumulation_steps=grad_accum,
        learning_rate=lr,
        warmup_steps=warmup_steps,
        max_steps=max_steps,
        gradient_checkpointing=True,
        fp16=True,
        evaluation_strategy="steps",
        eval_steps=max(50, max_steps // 10),
        logging_steps=max(25, max_steps // 20),
        save_strategy="no",            # we evaluate on-the-fly; avoid heavy disk writes during search
        report_to=["none"],
        predict_with_generate=True,
        generation_max_length=gen_max_len,
        seed=GLOBAL_SEED,
        load_best_model_at_end=False,  # keep search fast; we evaluate current model
        metric_for_best_model="wer",
        greater_is_better=False,
        dataloader_num_workers=8,
        dataloader_prefetch_factor=2,
    )

    # >>> NEW CHANGE (CHECKPOINT PATCH): avoid optimizer/scheduler writes if saving is enabled later
    # (During search we set save_strategy="no", but this keeps you safe if you flip it to "steps".)
    try:
        training_args.save_only_model = True
    except Exception:
        pass
    try:
        if not getattr(training_args, "save_only_model", False):
            from transformers.trainer import Trainer as _BaseTrainer
            def _no_save_optimizer_and_scheduler(self, output_dir):
                return
            _BaseTrainer._save_optimizer_and_scheduler = _no_save_optimizer_and_scheduler
    except Exception as _e:
        print("Non-fatal: checkpoint patch failed:", _e)
    # <<< NEW CHANGE (CHECKPOINT PATCH)

    trainer = Seq2SeqTrainer(
        args=training_args,
        model=model,
        train_dataset=vectorized_datasets["train"],
        eval_dataset=vectorized_datasets["test"],
        data_collator=data_collator,
        compute_metrics=compute_metrics,
        tokenizer=processor,
    )

    # ---- Training with pruning callback: stop bad trials early
    trainer.train()

    # ---- Evaluate WER on the eval split
    eval_metrics = trainer.evaluate()
    wer = float(eval_metrics.get("eval_wer", math.inf))

    # Free GPU memory between trials
    del trainer, model
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # Report to Optuna (smaller WER is better)
    return wer

# Create the Optuna study
study = optuna.create_study(
    direction="minimize",
    sampler=TPESampler(seed=GLOBAL_SEED),
    pruner=MedianPruner(n_startup_trials=4, n_warmup_steps=1),
)

# Run search â€” adjust n_trials to taste
N_TRIALS = 10  # try 10 first; increase when you confirm the loop runs smoothly
study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)

print("\n===== OPTUNA RESULTS =====")
print("Best trial number:", study.best_trial.number)
print("Best WER:", study.best_value)
print("Best params:")
for k, v in study.best_trial.params.items():
    print(f"  {k}: {v}")
# <<< NEW CHANGE (OPTUNA)