# =========================================================
# 8X
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1"

from datasets import Dataset, Audio
import glob
import math
import torch
import torch.nn as nn

# =========================================================
# Dataset Preparation
audio_files = glob.glob("/appdata/cortex/dev1/origAudio/*.mp3")

def retrieve_transcript(input_str: str) -> str:
    tmp = input_str[input_str.rfind("/")+1:-4]
    tmp = tmp.replace("im","i'm")
    tmp = tmp.replace("shell","she'll")
    tmp = tmp.replace("dont","don't")
    tmp = tmp.capitalize()
    tmp = tmp.replace("i ","I ")
    return tmp

transcripts = [retrieve_transcript(s) for s in audio_files]
num_samples = len(audio_files)

raw_datasets = Dataset.from_dict({
    "audio": audio_files[:num_samples],
    "sentence": transcripts[:num_samples],
}).cast_column("audio", Audio(sampling_rate=16000))

raw_datasets = raw_datasets.train_test_split(test_size=0.15, seed=91)

# =========================================================
# Processor
from transformers import WhisperProcessor
whisper_loc = "/appdata/cortex/dev1/whisper_copy/distil-large-v2"
processor = WhisperProcessor.from_pretrained(whisper_loc, language="english", task="transcribe")

# vectorize
def prepare_dataset(batch):
    audio = batch["audio"]
    batch["input_features"] = processor.feature_extractor(
        audio["array"], sampling_rate=audio["sampling_rate"]
    )["input_features"][0]
    transcription = batch["sentence"]
    batch["labels"] = processor.tokenizer(transcription).input_ids
    return batch

vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets["train"].column_names,
    num_proc=8,
)

# =========================================================
# Model
from transformers import WhisperForConditionalGeneration
model = WhisperForConditionalGeneration.from_pretrained(whisper_loc)

# your config tweaks
model.config.forced_decoder_ids = None
model.config.suppress_tokens = []
model.config.use_cache = False
model.config.language = "english"
model.config.task = "transcribe"
# avoid tying conflicts just in case
model.config.tie_word_embeddings = False

# =========================================================
# LoRA-like implementation (no peft)
class LoRALinear(nn.Module):
    def __init__(self, orig_linear: nn.Linear, r: int = 64, alpha: int = 32, dropout: float = 0.1):
        super().__init__()
        self.orig_linear = orig_linear
        self.r = r
        self.alpha = alpha
        self.scaling = alpha / r
        self.dropout = nn.Dropout(dropout)

        # LoRA adapters
        self.lora_A = nn.Linear(orig_linear.in_features, r, bias=False)
        self.lora_B = nn.Linear(r, orig_linear.out_features, bias=False)

        # init
        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B.weight)

        # freeze original
        for p in self.orig_linear.parameters():
            p.requires_grad = False

    def forward(self, x):
        return self.orig_linear(x) + self.lora_B(self.lora_A(self.dropout(x))) * self.scaling


def add_lora_to_whisper(model: nn.Module, r=64, alpha=32, dropout=0.1):
    """
    ONLY wrap safe transformer blocks; skip embeddings and lm_head/proj_out.
    """
    TARGET_KEYWORDS = (
        "self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj", "self_attn.out_proj",
        "encoder_attn.q_proj", "encoder_attn.k_proj", "encoder_attn.v_proj", "encoder_attn.out_proj",
        "fc1", "fc2", "mlp.fc1", "mlp.fc2"
    )
    SKIP_SUBSTRINGS = ("embed", "lm_head", "proj_out", "project_out", "token")

    # Helper: set attribute by dotted path
    def _set_module(root, path, new_module):
        parts = path.split(".")
        parent = root
        for p in parts[:-1]:
            parent = getattr(parent, p)
        setattr(parent, parts[-1], new_module)

    for name, module in list(model.named_modules()):
        if isinstance(module, nn.Linear):
            # fast skip by name
            if any(s in name for s in SKIP_SUBSTRINGS):
                continue
            if not any(k in name for k in TARGET_KEYWORDS):
                continue
            # replace this Linear with LoRALinear
            lora_layer = LoRALinear(module, r=r, alpha=alpha, dropout=dropout)
            _set_module(model, name, lora_layer)

    return model

# Apply LoRA adapters
model = add_lora_to_whisper(model, r=64, alpha=32, dropout=0.1)

# quick sanity on trainable %
trainable, total = 0, 0
for n, p in model.named_parameters():
    num = p.numel()
    total += num
    if p.requires_grad:
        trainable += num
print(f"Trainable params: {trainable} / {total} ({100*trainable/total:.2f}%)")

# =========================================================
# Data Collator
from dataclasses import dataclass
from typing import Any, Dict, List, Union

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    decoder_start_token_id: int

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_features": f["input_features"]} for f in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        label_features = [{"input_ids": f["labels"]} for f in features]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
        if (labels[:, 0] != self.decoder_start_token_id).all():
            labels = labels[:, 1:]
        batch["labels"] = labels
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(
    processor=processor,
    decoder_start_token_id=model.config.decoder_start_token_id,
)

# =========================================================
# Metrics
import evaluate
metric = evaluate.load("wer")

def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id
    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)
    wer = 100 * metric.compute(predictions=pred_str, references=label_str)
    return {"wer": wer}

# =========================================================
# Training Setup
from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer

training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper-lora-like",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=16,   # increase by 2x for every 2x decrease in batch size
    learning_rate=4.083705401062e-05,
    warmup_steps=400,
    max_steps=1000,
    gradient_checkpointing=True,
    fp16=True,
    evaluation_strategy="steps",
    per_device_eval_batch_size=4,
    predict_with_generate=True,
    generation_max_length=176,
    seed=91,
    logging_steps=5,
    eval_steps=5,
    save_steps=5,
    save_strategy="steps",
    save_total_limit=2,
    report_to=["none"],
    load_best_model_at_end=True,     # IMPORTANT
    metric_for_best_model="wer",
    greater_is_better=False,
    dataloader_num_workers=8,
    dataloader_prefetch_factor=2,
    remove_unused_columns=False,
    # >>> This avoids the crash you saw <<<
    save_safetensors=False,
)

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=vectorized_datasets["train"],
    eval_dataset=vectorized_datasets["test"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor,
)

# =========================================================
# Train
trainer.train()