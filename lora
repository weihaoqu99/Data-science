# ============================================================
# Whisper Test/Eval — normalized WER + CSV (works with LoRA or full model)
# ============================================================

import os, glob, re, unicodedata
from dataclasses import dataclass
from typing import Any, Dict, List, Union

import torch
import pandas as pd
import evaluate
from datasets import Dataset, Audio
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
)

# --------------------
# PATHS / TOGGLES (edit these only)
# --------------------
AUDIO_GLOB      = "/appdata/cortex/dev1/origAudio/*.mp3"  # same audio used for test split
# Base model dir you trained from:
# BASE_MODEL_DIR = "/appdata/cortex/dev1/aptaiModels/whisper-large-v2"
BASE_MODEL_DIR  = "/appdata/cortex/dev1/whisper_copy/distil-large-v2"

# If you saved **LoRA adapter only** with PEFT:
USE_ADAPTER     = True
ADAPTER_DIR     = "/appdata/cortex/dev1/shob/ASR_models/asr_finetuning/whisper_lora_adapter"

# If you saved a **full/merged model dir** instead, set:
# USE_ADAPTER   = False
# FULL_MODEL_DIR = "/appdata/cortex/dev1/shob/ASR_models/asr_finetuning/whisper_finetuning_best_hyp"

CSV_OUT         = "/appdata/cortex/dev1/shob/ASR_models/asr_finetuning/test_predictions_normalized.csv"
CUDA_VISIBLE    = "1"  # pick your GPU
os.environ["CUDA_VISIBLE_DEVICES"] = CUDA_VISIBLE

# --------------------
# Rebuild dataset (same logic as training)
# --------------------
audio_files = glob.glob(AUDIO_GLOB)

def retrieve_transcript(input_str: str) -> str:
    tmp = input_str[input_str.find("-")+1:-4]  # strip track prefix & ".mp3"
    tmp = tmp.replace("_", " ").replace(".", " ")
    tmp = tmp.replace("shell", "she'll").replace("dont", "don't")
    tmp = tmp.capitalize().replace("i ", "I ")
    return tmp

transcripts = [retrieve_transcript(s) for s in audio_files]

raw_datasets = Dataset.from_dict({
    "audio": audio_files,
    "sentence": transcripts
}).cast_column("audio", Audio(sampling_rate=16_000))

# Use **the same split** as training
raw_datasets = raw_datasets.train_test_split(test_size=0.15, seed=91)

# --------------------
# Processor & feature prep (same as training)
# --------------------
processor = WhisperProcessor.from_pretrained(BASE_MODEL_DIR, language="English", task="transcribe")

def prepare_dataset(batch):
    a = batch["audio"]
    batch["input_features"] = processor.feature_extractor(
        a["array"], sampling_rate=a["sampling_rate"]
    ).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets["train"].column_names,
    num_proc=8
)

# --------------------
# Collator (same as training)
# --------------------
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    decoder_start_token_id: int
    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # inputs
        inp = [{"input_features": f["input_features"]} for f in features]
        batch = self.processor.feature_extractor.pad(inp, return_tensors="pt")
        # labels -> pad -> mask PAD -> -100; drop SOT if present
        lab = [{"input_ids": f["labels"]} for f in features]
        lab_batch = self.processor.tokenizer.pad(lab, return_tensors="pt")
        labels = lab_batch["input_ids"].masked_fill(lab_batch["attention_mask"].ne(1), -100)
        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
            labels = labels[:, 1:]
        batch["labels"] = labels
        return batch

# --------------------
# Load model for evaluation (LoRA adapter or full model)
# --------------------
if USE_ADAPTER:
    from peft import PeftModel
    base = WhisperForConditionalGeneration.from_pretrained(BASE_MODEL_DIR)
    model = PeftModel.from_pretrained(base, ADAPTER_DIR)
else:
    model = WhisperForConditionalGeneration.from_pretrained(FULL_MODEL_DIR)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
model.eval()

# Match training-time tokenizer/generation settings
processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id
model.config.pad_token_id = processor.tokenizer.pad_token_id
if getattr(processor.tokenizer, "sot_token_id", None) is not None:
    model.config.decoder_start_token_id = processor.tokenizer.sot_token_id
model.config.forced_decoder_ids = None
model.generation_config.forced_decoder_ids = None
model.config.suppress_tokens = []
model.generation_config.suppress_tokens = []
model.config.use_cache = False
model.config.language = "english"
model.config.task = "transcribe"

data_collator = DataCollatorSpeechSeq2SeqWithPadding(
    processor=processor,
    decoder_start_token_id=model.config.decoder_start_token_id,
)

# --------------------
# Whisper-style text normalization BEFORE WER
# --------------------
def normalize_text(s: str) -> str:
    if s is None:
        return ""
    s = unicodedata.normalize("NFKC", s)
    s = s.replace("’", "'").replace("`", "'")
    s = s.replace("–", "-").replace("—", "-")
    s = s.lower()
    s = re.sub(r"[^a-z0-9' ]+", " ", s)  # keep letters/digits/apostrophes/spaces
    s = re.sub(r"\s+", " ", s).strip()
    return s

# --------------------
# Lightweight eval trainer (predict only)
# --------------------
eval_args = Seq2SeqTrainingArguments(
    output_dir="./_eval_tmp",
    per_device_eval_batch_size=4,
    remove_unused_columns=False,
    label_names=["labels"],
    predict_with_generate=True,
    generation_max_length=225,
    fp16=True,
    report_to=[],
)

eval_trainer = Seq2SeqTrainer(
    args=eval_args,
    model=model,
    eval_dataset=vectorized_datasets["test"],
    data_collator=data_collator,
    tokenizer=processor,
)

# --------------------
# Predict on test split
# --------------------
pred_output = eval_trainer.predict(vectorized_datasets["test"])

pred_ids = pred_output.predictions[0] if isinstance(pred_output.predictions, (tuple, list)) else pred_output.predictions
label_ids = torch.tensor(pred_output.label_ids)
label_ids[label_ids == -100] = processor.tokenizer.pad_token_id
label_ids = label_ids.cpu().numpy()

pred_text = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
ref_text  = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)

# Normalize both sides BEFORE WER
pred_norm = [normalize_text(s) for s in pred_text]
ref_norm  = [normalize_text(s) for s in ref_text]

# --------------------
# Overall and per-utterance WER
# --------------------
wer_metric = evaluate.load("wer")
overall_wer = 100.0 * wer_metric.compute(predictions=pred_norm, references=ref_norm)
print(f"Overall test WER (normalized): {overall_wer:.2f}")

rows = []
for i, (hyp, ref, hyp_raw, ref_raw) in enumerate(zip(pred_norm, ref_norm, pred_text, ref_text)):
    w = 100.0 * wer_metric.compute(predictions=[hyp], references=[ref])
    rows.append({
        "idx": i,
        "audio": raw_datasets["test"][i]["audio"]["path"],
        "reference": ref_raw,
        "prediction": hyp_raw,
        "wer": round(w, 2),
    })

df = pd.DataFrame(rows)
df.to_csv(CSV_OUT, index=False, encoding="utf-8")
print("Saved detailed predictions to:", CSV_OUT)