# --- LoRA (RS-LoRA True, r=64, alpha=32) -----------------------
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    PeftModel,
    TaskType,
)

# (optional but safe even if you’re not using 8-bit) – keeps layer norms in fp32, etc.
model = prepare_model_for_kbit_training(model)

# your notebook already uses this style; keep it
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

# Prefer the enum for compatibility with older PEFTs (avoids the "task_type" error)
try:
    lora_cfg = LoraConfig(
        r=64,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules=["q_proj", "v_proj"],
        bias="none",
        task_type=TaskType.SEQ_2_SEQ_LM,
        use_rslora=True,               # RS-LoRA = True
    )
except TypeError:
    # If your installed PEFT is older and doesn't know `use_rslora`,
    # fall back to standard LoRA but keep r/alpha the same.
    lora_cfg = LoraConfig(
        r=64,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules=["q_proj", "v_proj"],
        bias="none",
        task_type=TaskType.SEQ_2_SEQ_LM,
    )

model = get_peft_model(model, lora_cfg)
model.print_trainable_parameters()  # sanity check: only LoRA params should be trainable
# --------------------------------------------------------------