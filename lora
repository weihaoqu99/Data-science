# ------------------------- YOUR ORIGINAL CODE (unchanged) -------------------------
import os
os.environ['CUDA_VISIBLE_DEVICES'] = "1"

from datasets import Dataset, Audio
import glob

audio_files = glob.glob("/appdata/cortex/dev1/origAudio/*.mp3")

def retrieve_transcript(input_str: str) -> str:
    tmp = input_str[input_str.find("-")+1:-4]  # strip out track number and .mp3 suffix
    tmp = tmp.replace('_', ' ')
    tmp = tmp.replace('.', ' ')
    tmp = tmp.replace("shell", "she'll")
    tmp = tmp.replace("dont", "don't")
    tmp = tmp.capitalize()
    tmp = tmp.replace(' i ', ' I ')
    return tmp

transcripts = [retrieve_transcript(s) for s in audio_files]

# Need to adjust sampling rate if necessary
num_samples = len(audio_files)

raw_datasets = Dataset.from_dict({
    "audio": audio_files[:num_samples],
    "sentence": transcripts[:num_samples]
}).cast_column("audio", Audio(sampling_rate=16_000))

raw_datasets = raw_datasets.train_test_split(test_size=0.15, seed=91)
raw_datasets
# ----------------------------------------------------------------------------------


# ============================== NEW: LoRA + training ==============================

# NEW: flags & LoRA hyperparams you asked for
USE_LORA = True            # <--- NEW
LORA_R = 64                # <--- NEW
LORA_ALPHA = 32            # <--- NEW
LORA_DROPOUT = 0.05        # <--- NEW

# NEW: core imports for ASR fine-tuning with LoRA
import torch                                  # <--- NEW
from transformers import (                    # <--- NEW
    AutoProcessor,
    AutoModelForSpeechSeq2Seq,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments
)
# NEW: data collator (use library if available; otherwise a tiny fallback)
try:                                          # <--- NEW
    from transformers import DataCollatorSpeechSeq2SeqWithPadding
    _HAS_DC = True
except Exception:
    _HAS_DC = False

# NEW: PEFT (LoRA)
from peft import LoraConfig, get_peft_model    # <--- NEW

# NEW: pick a Whisper-style checkpoint (you can swap to your usual model)
# If you typically use Distil-Whisper large v2 + LoRA, this matches your setup.
MODEL_ID = "distil-whisper/distil-large-v2"    # <--- NEW (change if you use a different ckpt)

# NEW: processor & model
processor = AutoProcessor.from_pretrained(MODEL_ID)   # <--- NEW
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    MODEL_ID,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True
).to("cuda")                                           # <--- NEW

# NEW: helpful Whisper settings so training isn’t constrained
model.config.forced_decoder_ids = None                 # <--- NEW
model.config.suppress_tokens = []                      # <--- NEW

# NEW: apply LoRA if requested
if USE_LORA:                                           # <--- NEW
    lora_cfg = LoraConfig(
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        lora_dropout=LORA_DROPOUT,
        bias="none",
        task_type="SEQ_2_SEQ_LM",
        # common attention projections for Whisper/seq2seq
        target_modules=["q_proj", "v_proj"]
    )
    model = get_peft_model(model, lora_cfg)
    model.print_trainable_parameters()  # sanity check in logs

# NEW: prepare features/labels from your raw_datasets (no changes to your original construction)
def _prepare_batch(batch):                              # <--- NEW
    # batch["audio"] is a dict from datasets Audio() with "array" and "sampling_rate"
    audio = batch["audio"]
    inputs = processor(
        audio["array"],
        sampling_rate=audio["sampling_rate"],
        return_tensors="pt"
    )
    with processor.as_target_tokenizer():
        labels = processor.tokenizer(batch["sentence"]).input_ids

    batch["input_features"] = inputs.input_features[0]
    batch["labels"] = labels
    return batch

# Map to add input_features + labels; keep your original columns intact until we remove them for Trainer
train_ds = raw_datasets["train"].map(                  # <--- NEW
    _prepare_batch,
    remove_columns=[],
    desc="Preparing train features"
)
eval_ds = raw_datasets["test"].map(                    # <--- NEW
    _prepare_batch,
    remove_columns=[],
    desc="Preparing eval features"
)

# NEW: data collator (library or minimal fallback)
if _HAS_DC:                                            # <--- NEW
    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
else:
    # Minimal padding for input_features & labels if the class isn’t available
    from torch.nn.utils.rnn import pad_sequence
    def data_collator(features):
        # input_features: list of FloatTensor [feature_len, feature_dim]
        input_features = [torch.tensor(f["input_features"]) for f in features]
        # pad to max length along time dimension
        input_features = pad_sequence(input_features, batch_first=True)
        # labels are int lists
        labels = [torch.tensor(f["labels"], dtype=torch.long) for f in features]
        labels = pad_sequence(labels, batch_first=True, padding_value=processor.tokenizer.pad_token_id)
        return {
            "input_features": input_features,
            "labels": labels
        }

# NEW: training args (tune to your infra/paths)
output_dir = "/appdata/cortex/dev1/ASR_model/lora_distil_large_v2_r64a32"  # <--- NEW
training_args = Seq2SeqTrainingArguments(                                  # <--- NEW
    output_dir=output_dir,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=2,
    learning_rate=1e-4,
    warmup_steps=500,
    num_train_epochs=3,
    fp16=True,
    logging_steps=50,
    evaluation_strategy="steps",
    eval_steps=500,
    save_steps=500,
    save_total_limit=2,
    predict_with_generate=False,
    report_to="none",
    gradient_checkpointing=True
)

# NEW: build trainer
trainer = Seq2SeqTrainer(                              # <--- NEW
    args=training_args,
    model=model,
    train_dataset=train_ds.remove_columns(["audio", "sentence"]),
    eval_dataset=eval_ds.remove_columns(["audio", "sentence"]),
    tokenizer=processor.tokenizer,
    data_collator=data_collator
)

# NEW: kick off training
# (comment this out if you only wanted the config objects)
trainer.train()                                        # <--- NEW

# ================================================================================