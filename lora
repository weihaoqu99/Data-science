from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# prepare model for LoRA training
model = prepare_model_for_kbit_training(model)
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

# LoRA config (RS-LoRA enabled)
lora_cfg = LoraConfig(
    r=64,
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules=["q_proj", "v_proj"],
    bias="none",
    task_type="SEQ_2_SEQ_LM",   # required for Whisper
    use_rslora=True             # enable RS-LoRA
)

# apply LoRA
model = get_peft_model(model, lora_cfg)