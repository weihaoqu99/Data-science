# ===== LoRA ONLY: r=64, alpha=32, rsLoRA=True, FAST (top-k decoder layers) =====
from peft import LoraConfig, get_peft_model
import re

# keeps your current memory settings
model.gradient_checkpointing_enable()
model.enable_input_require_grads()
model.config.use_cache = False

# build LoRA config (version-safe: no task_type string)
lora_cfg = LoraConfig(
    r=64,
    lora_alpha=32,
    lora_dropout=0.05,
    use_rslora=True,
    target_modules=["q_proj", "v_proj"],
    bias="none",
)

# apply LoRA
model = get_peft_model(model, lora_cfg)

# -------- SPEED-UP: train LoRA only on the LAST K decoder layers ----------
K = 6  # <-- tweak if you want even faster (e.g., 4) or a bit slower/better (e.g., 8)

# figure out number of decoder layers across HF/PEFT variants
try:
    num_layers = len(model.base_model.model.decoder.layers)
except Exception:
    num_layers = len(model.model.decoder.layers)
keep_from = max(0, num_layers - K)

for name, p in model.named_parameters():
    if "lora_" not in name:
        continue
    # never train encoder LoRA
    if ".encoder." in name:
        p.requires_grad = False
        continue
    # freeze decoder LoRA below the keep range
    m = re.search(r"decoder\.layers\.(\d+)\.", name)
    if m:
        layer_idx = int(m.group(1))
        if layer_idx < keep_from:
            p.requires_grad = False

# optional: see how many params will train
try:
    model.print_trainable_parameters()
except Exception:
    pass
# ===== END LoRA ONLY =====