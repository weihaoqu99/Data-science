# ============================================================
# Whisper fine-tuning — your original pipeline + PEFT LoRA
# - Version-safe: no unsupported TrainingArguments (num_beams, etc.)
# - Modern 8-bit path via BitsAndBytesConfig with fallbacks
# - Fixes 100-WER pitfalls (SOT, no forced IDs, safe padding)
# ============================================================

# --- imports
import os, glob
from dataclasses import dataclass
from typing import Any, Dict, List, Union

import torch
import evaluate
from datasets import Dataset, Audio
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
)

# --- PEFT (LoRA)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# =============== ENV / PATHS ===============
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # e.g. "0" or "0,1,2,3"
AUDIO_GLOB = "/appdata/cortex/dev1/origAudio/*.mp3"

# Base model (as in your notebook)
# whisper_loc = "/appdata/cortex/dev1/aptaiModels/whisper-large-v2"
whisper_loc = "/appdata/cortex/dev1/whisper_copy/distil-large-v2"

# =============== DATASET (your original style) ===============
audio_files = glob.glob(AUDIO_GLOB)

def retrieve_transcript(input_str: str) -> str:
    # strip leading "{track}-" and trailing ".mp3"
    tmp = input_str[input_str.find("-")+1:-4]
    tmp = tmp.replace("_", " ").replace(".", " ")
    tmp = tmp.replace("shell", "she'll").replace("dont", "don't")
    tmp = tmp.capitalize().replace("i ", "I ")
    return tmp

transcripts = [retrieve_transcript(s) for s in audio_files]
num_samples = len(audio_files)

raw_datasets = Dataset.from_dict({
    "audio": audio_files[:num_samples],
    "sentence": transcripts[:num_samples],
}).cast_column("audio", Audio(sampling_rate=16_000))

raw_datasets = raw_datasets.train_test_split(test_size=0.15, seed=91)

# =============== PROCESSOR & FEATURE PREP ===============
processor = WhisperProcessor.from_pretrained(whisper_loc, language="English", task="transcribe")

def prepare_dataset(batch):
    audio = batch["audio"]
    batch["input_features"] = processor.feature_extractor(
        audio["array"], sampling_rate=audio["sampling_rate"]
    ).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets["train"].column_names,
    num_proc=8
)

# =============== LOAD MODEL (8-bit if available; no deprecation warnings) ===============
eight_bit = False
try:
    # Preferred path on newer Transformers (avoids load_in_8bit deprecation)
    from transformers import BitsAndBytesConfig
    quant_cfg = BitsAndBytesConfig(load_in_8bit=True)
    model = WhisperForConditionalGeneration.from_pretrained(
        whisper_loc, quantization_config=quant_cfg, device_map="auto"
    )
    eight_bit = True
except Exception:
    # Fallback to older API or fp16 if bitsandbytes not present
    try:
        model = WhisperForConditionalGeneration.from_pretrained(
            whisper_loc, load_in_8bit=True, device_map="auto"
        )
        eight_bit = True
    except Exception:
        model = WhisperForConditionalGeneration.from_pretrained(whisper_loc)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model.to(device)

# ---- critical Whisper config (prevents 100 WER) ----
processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id
model.config.pad_token_id = processor.tokenizer.pad_token_id

# Decoder start (SOT) for English transcription
if getattr(processor.tokenizer, "sot_token_id", None) is not None:
    model.config.decoder_start_token_id = processor.tokenizer.sot_token_id

# Avoid prefix conflicts — let generation handle prompts naturally
model.config.forced_decoder_ids = None
model.generation_config.forced_decoder_ids = None
model.config.suppress_tokens = []
model.generation_config.suppress_tokens = []

# Training niceties
model.config.use_cache = False      # enables gradient checkpointing
model.config.language = "english"
model.config.task = "transcribe"

# =============== APPLY PEFT LoRA (AWS/HF-style) ===============
if eight_bit:
    model = prepare_model_for_kbit_training(model)

# LoRA configuration — matches common AWS/HF examples
peft_kwargs = dict(
    r=32,
    lora_alpha=64,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
)

# Try RS-LoRA if your PEFT supports it; otherwise standard LoRA
try:
    lora_cfg = LoraConfig(use_rslora=True, **peft_kwargs)
except TypeError:
    lora_cfg = LoraConfig(**peft_kwargs)

model = get_peft_model(model, lora_cfg)
model.print_trainable_parameters()

# =============== COLLATOR (original style, with safe masking) ===============
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    decoder_start_token_id: int

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # inputs
        input_features = [{"input_features": f["input_features"]} for f in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        # labels (pad → mask → -100)
        label_features = [{"input_ids": f["labels"]} for f in features]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        labels = labels_batch["input_ids"]
        lamask = labels_batch["attention_mask"]

        # ignore PAD in loss
        labels = labels.masked_fill(lamask.ne(1), -100)

        # drop SOT/BOS if present at position 0
        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(
    processor=processor,
    decoder_start_token_id=model.config.decoder_start_token_id,
)

# =============== METRIC (WER) ===============
metric = evaluate.load("wer")

def compute_metrics(pred):
    # pred.predictions may be a tuple; handle both
    pred_ids = pred.predictions[0] if isinstance(pred.predictions, (tuple, list)) else pred.predictions
    label_ids = torch.tensor(pred.label_ids)
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id
    label_ids = label_ids.cpu().numpy()

    pred_str  = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)
    wer = 100.0 * metric.compute(predictions=pred_str, references=label_str)
    return {"wer": wer}

# =============== TRAINING ARGS (close to your original; version-safe) ===============
training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper_peft_lora_out",
    remove_unused_columns=False,     # keep "input_features"
    label_names=["labels"],
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=1e-5,
    warmup_steps=200,
    max_steps=1000,                  # you used steps; keep it
    evaluation_strategy="steps",
    eval_steps=5,
    logging_steps=5,
    save_strategy="steps",
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    predict_with_generate=True,
    generation_max_length=225,
    # IMPORTANT: Do not pass unsupported args like `num_beams` or `do_sample` here.
    # We'll stick with defaults for wide version compatibility.
    fp16=not eight_bit,
    gradient_checkpointing=True,
    dataloader_num_workers=8,
    # Avoid uncommon/older-incompatible args:
    # dataloader_prefetch_factor is omitted for compatibility.
    report_to=[],                    # no TB/W&B
)

# =============== TRAINER (original flow) ===============
trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=vectorized_datasets["train"],
    eval_dataset=vectorized_datasets["test"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor,
)

# =============== (Optional) QUICK SMOKE TEST ===============
if True:
    from torch.utils.data import DataLoader
    tmp_loader = DataLoader(vectorized_datasets["test"], batch_size=2, collate_fn=data_collator)
    batch = next(iter(tmp_loader))
    model.eval()
    with torch.no_grad():
        gen = model.generate(
            input_features=batch["input_features"].to(model.device),
            max_new_tokens=64,  # keep short
        )
    pred = processor.tokenizer.batch_decode(gen, skip_special_tokens=True)
    gold = processor.tokenizer.batch_decode(
        batch["labels"].masked_fill(batch["labels"] == -100, processor.tokenizer.pad_token_id),
        skip_special_tokens=True
    )
    print("SMOKE TEST — PRED:", pred)
    print("SMOKE TEST — GOLD:", gold[:len(pred)])

# =============== TRAIN & SAVE ===============
trainer.train()

# Save ONLY the LoRA adapter (PEFT best practice)
adapter_dir = "/appdata/cortex/dev1/shob/ASR_models/asr_finetuning/whisper_peft_lora_adapter"
model.save_pretrained(adapter_dir)
processor.save_pretrained(adapter_dir)
print(f"Saved LoRA adapter + processor to: {adapter_dir}")