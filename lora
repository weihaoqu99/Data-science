# ==========================================
# 3) Model config and custom data collator
# ==========================================
from transformers import WhisperForConditionalGeneration
import torch
from dataclasses import dataclass
from typing import Any, Dict, List, Union

model = WhisperForConditionalGeneration.from_pretrained(whisper_loc)
model.config.forced_decoder_ids = None
model.config.suppress_tokens = []
model.config.use_cache = False  # incompatible with gradient checkpointing (memory savings)
model.config.language = "english"
model.config.task = "transcribe"

# ---------- NEW CHANGE (LoRA fix): speech-aware task type + safe forward patch ----------
# The TaskType guard uses SPEECH_SEQ_2_SEQ if your PEFT has it; otherwise falls back.
from peft import LoraConfig, get_peft_model
try:
    from peft import TaskType
    _task_type = getattr(TaskType, "SPEECH_SEQ_2_SEQ", TaskType.SEQ_2_SEQ_LM)
except Exception:
    _task_type = "SEQ_2_SEQ_LM"  # very old PEFT fallback

lora_cfg = LoraConfig(
    r=64,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type=_task_type,
    target_modules=["q_proj", "k_proj", "v_proj", "out_proj"],  # attention projections
)

model = get_peft_model(model, lora_cfg)

# SAFETY: Some PEFT/Trainer combos may still pass `input_ids`.
# Strip it if present so Whisper receives only `input_features` + `labels`.
_model_forward_orig = model.forward
def _forward_strip_input_ids(*args, **kwargs):
    kwargs.pop("input_ids", None)
    return _model_forward_orig(*args, **kwargs)
model.forward = _forward_strip_input_ids
# ------------------------------------------------------------------------

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    decoder_start_token_id: int

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # input features and labels; they have to be of different lengths and need different padding
        input_features = [{"input_features": f["input_features"]} for f in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        # get the tokenized Label sequences
        label_features = [{"input_ids": f["labels"]} for f in features]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        # if bos token is appended in previous tokenization step, cut it
        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels
        return batch