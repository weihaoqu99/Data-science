# %%
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1"

from datasets import Dataset, Audio
import glob
import math
import torch
import torch.nn as nn

# ==========================================================
# Dataset Preparation
# ==========================================================
audio_files = glob.glob("/appdata/cortex/dev1/origAudio/*.mp3")

def retrieve_transcript(input_str: str) -> str:
    tmp = input_str[input_str.find("-")+1:-4]
    tmp = tmp.replace("_", " ")
    tmp = tmp.replace(".", " ")
    tmp = tmp.replace("shell", "she'll")
    tmp = tmp.replace("dont", "don't")
    tmp = tmp.capitalize()
    tmp = tmp.replace("i ", "I ")
    return tmp

transcripts = [retrieve_transcript(s) for s in audio_files]
num_samples = len(audio_files)

raw_datasets = Dataset.from_dict({
    "audio": audio_files[:num_samples],
    "sentence": transcripts[:num_samples]
}).cast_column("audio", Audio(sampling_rate=16_000))

raw_datasets = raw_datasets.train_test_split(test_size=0.15, seed=91)

# %%
from transformers import WhisperProcessor
whisper_loc = "/appdata/cortex/dev1/whisper_copy/distil-large-v2"
processor = WhisperProcessor.from_pretrained(whisper_loc, language="English", task="transcribe")

# %%
def prepare_dataset(batch):
    audio = batch["audio"]
    batch["input_features"] = processor.feature_extractor(
        audio["array"], sampling_rate=audio["sampling_rate"]
    ).input_features[0]
    transcription = batch["sentence"]
    batch["labels"] = processor.tokenizer(transcription).input_ids
    return batch

vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets["train"].column_names,
    num_proc=8
)

# %%
from transformers import WhisperForConditionalGeneration
model = WhisperForConditionalGeneration.from_pretrained(whisper_loc)
model.config.forced_decoder_ids = None
model.config.suppress_tokens = []
model.config.use_cache = False
model.config.language = "english"
model.config.task = "transcribe"

# ==========================================================
# LoRA-like Implementation (no peft required)
# ==========================================================
class LoRALinear(nn.Module):
    def __init__(self, orig_linear: nn.Linear, r: int = 64, alpha: int = 32, dropout: float = 0.1):
        super().__init__()
        self.orig_linear = orig_linear
        self.r = r
        self.alpha = alpha
        self.scaling = alpha / r
        self.dropout = nn.Dropout(dropout)

        # LoRA adapters
        self.lora_A = nn.Linear(orig_linear.in_features, r, bias=False)
        self.lora_B = nn.Linear(r, orig_linear.out_features, bias=False)

        # init
        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B.weight)

        # freeze original
        for param in self.orig_linear.parameters():
            param.requires_grad = False

    def forward(self, x):
        return self.orig_linear(x) + self.lora_B(self.lora_A(self.dropout(x))) * self.scaling


def add_lora_to_whisper(model, r=64, alpha=32, dropout=0.1):
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            new_layer = LoRALinear(module, r=r, alpha=alpha, dropout=dropout)
            parent = model
            *path, last = name.split(".")
            for p in path:
                parent = getattr(parent, p)
            setattr(parent, last, new_layer)
    return model

# Apply LoRA adapters
model = add_lora_to_whisper(model, r=64, alpha=32, dropout=0.1)

# check trainable %
trainable, total = 0, 0
for n, p in model.named_parameters():
    total += p.numel()
    if p.requires_grad:
        trainable += p.numel()
print(f"Trainable params: {trainable} / {total} ({100*trainable/total:.2f}%)")

# ==========================================================
# Data Collator
# ==========================================================
from dataclasses import dataclass
from typing import Any, Dict, List, Union

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    decoder_start_token_id: int

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_features": feature["input_features"]} for feature in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
            labels = labels[:, 1:]
        batch["labels"] = labels
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(
    processor=processor,
    decoder_start_token_id=model.config.decoder_start_token_id,
)

# ==========================================================
# Metrics
# ==========================================================
import evaluate
metric = evaluate.load("wer")

def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id
    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)
    wer = 100 * metric.compute(predictions=pred_str, references=label_str)
    return {"wer": wer}

# ==========================================================
# Training Setup
# ==========================================================
from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper-lora-like",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=1e-5,
    warmup_steps=500,
    max_steps=1000,
    gradient_checkpointing=True,
    fp16=True,
    evaluation_strategy="steps",
    per_device_eval_batch_size=4,
    predict_with_generate=True,
    generation_max_length=225,
    seed=91,
    data_seed=91,
    logging_steps=5,
    eval_steps=5,
    save_strategy="steps",
    save_total_limit=2,
    report_to=["none"],
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    push_to_hub=False,
    dataloader_num_workers=8,
    dataloader_prefetch_factor=2,
)

# ==========================================================
# Trainer
# ==========================================================
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=vectorized_datasets["train"],
    eval_dataset=vectorized_datasets["test"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor,
)

# ==========================================================
# Train
# ==========================================================
trainer.train()