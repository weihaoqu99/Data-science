# ============================================================
# Whisper fine-tuning — your original pipeline + PEFT LoRA
# - Keeps your filename→transcript dataset builder
# - Adds LoRA (q/v projections) with AWS/HF-style params
# - Fixes WER=100 pitfalls (SOT token, no forced IDs, safe padding)
# - Tries 8-bit loading; falls back to fp16 automatically
# ============================================================

# --- imports
import os, glob, math
from dataclasses import dataclass
from typing import Any, Dict, List, Union

import torch
import torch.nn as nn
import evaluate
from datasets import Dataset, Audio
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
)

# --- PEFT (LoRA)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# =============== ENV / PATHS ===============
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # or "1" or "0,1,2,3"
AUDIO_GLOB = "/appdata/cortex/dev1/origAudio/*.mp3"

# Choose your base model path (as in your notebook)
# whisper_loc = "/appdata/cortex/dev1/aptaiModels/whisper-large-v2"
whisper_loc = "/appdata/cortex/dev1/whisper_copy/distil-large-v2"

# =============== DATASET (your original style) ===============
audio_files = glob.glob(AUDIO_GLOB)

def retrieve_transcript(input_str: str) -> str:
    # strip leading "track-" (or similar) and trailing ".mp3"
    tmp = input_str[input_str.find("-")+1:-4]
    tmp = tmp.replace("_", " ").replace(".", " ")
    tmp = tmp.replace("shell", "she'll").replace("dont", "don't")
    tmp = tmp.capitalize().replace("i ", "I ")
    return tmp

transcripts = [retrieve_transcript(s) for s in audio_files]
num_samples = len(audio_files)

raw_datasets = Dataset.from_dict({
    "audio": audio_files[:num_samples],
    "sentence": transcripts[:num_samples],
}).cast_column("audio", Audio(sampling_rate=16_000))

raw_datasets = raw_datasets.train_test_split(test_size=0.15, seed=91)

# =============== PROCESSOR & FEATURE PREP ===============
processor = WhisperProcessor.from_pretrained(whisper_loc, language="English", task="transcribe")

def prepare_dataset(batch):
    # log-Mel from audio
    audio = batch["audio"]
    batch["input_features"] = processor.feature_extractor(
        audio["array"], sampling_rate=audio["sampling_rate"]
    ).input_features[0]
    # tokenize label ids
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

vectorized_datasets = raw_datasets.map(
    prepare_dataset,
    remove_columns=raw_datasets["train"].column_names,
    num_proc=8
)

# =============== LOAD MODEL (8-bit if available) ===============
eight_bit = False
try:
    model = WhisperForConditionalGeneration.from_pretrained(
        whisper_loc,
        load_in_8bit=True,      # memory saver if bitsandbytes installed
        device_map="auto",
    )
    eight_bit = True
except Exception:
    model = WhisperForConditionalGeneration.from_pretrained(whisper_loc)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)

# ---- critical Whisper config (prevents 100 WER) ----
# PAD=EOS (Whisper convention) — set on both tokenizer and model
processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id
model.config.pad_token_id = processor.tokenizer.pad_token_id

# Decoder start (SOT) for English transcription
if getattr(processor.tokenizer, "sot_token_id", None) is not None:
    model.config.decoder_start_token_id = processor.tokenizer.sot_token_id

# Avoid prefix conflicts — let Trainer/generate handle prompts
model.config.forced_decoder_ids = None
model.generation_config.forced_decoder_ids = None
model.config.suppress_tokens = []
model.generation_config.suppress_tokens = []

# Training niceties
model.config.use_cache = False      # enables gradient checkpointing
model.config.language = "english"
model.config.task = "transcribe"

# =============== APPLY PEFT LoRA (AWS/HF-style) ===============
# Prepare for k-bit if we loaded in 8-bit
if eight_bit:
    model = prepare_model_for_kbit_training(model)

# LoRA configuration (you can tweak r/alpha/dropout)
# Matches common AWS/HF examples: r=32, alpha=64, q/v only
peft_kwargs = dict(
    r=32,
    lora_alpha=64,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
)

# If your PEFT version supports RS-LoRA, enable it (safe try/except)
try:
    lora_cfg = LoraConfig(use_rslora=True, **peft_kwargs)
except TypeError:
    lora_cfg = LoraConfig(**peft_kwargs)

model = get_peft_model(model, lora_cfg)
model.print_trainable_parameters()  # sanity check: only LoRA params should train

# =============== COLLATOR (your original style, with safe masking) ===============
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    decoder_start_token_id: int

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # inputs
        input_features = [{"input_features": f["input_features"]} for f in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        # labels (pad → mask → -100)
        label_features = [{"input_ids": f["labels"]} for f in features]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        labels = labels_batch["input_ids"]
        lamask = labels_batch["attention_mask"]

        # ignore PAD in loss
        labels = labels.masked_fill(lamask.ne(1), -100)

        # drop SOT/BOS if present at position 0
        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels
        return batch

data_collator = DataCollatorSpeechSeq2SeqWithPadding(
    processor=processor,
    decoder_start_token_id=model.config.decoder_start_token_id,
)

# =============== METRIC (WER) ===============
metric = evaluate.load("wer")

def compute_metrics(pred):
    pred_ids = pred.predictions[0] if isinstance(pred.predictions, (tuple, list)) else pred.predictions
    label_ids = torch.tensor(pred.label_ids)
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id
    label_ids = label_ids.cpu().numpy()

    pred_str  = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)
    wer = 100.0 * metric.compute(predictions=pred_str, references=label_str)
    return {"wer": wer}

# =============== TRAINING ARGS (close to your original) ===============
training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper_peft_lora_out",
    remove_unused_columns=False,     # keep "input_features"
    label_names=["labels"],
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=8,   # like your earlier setup
    learning_rate=1e-5,              # safe for LoRA
    warmup_steps=200,
    max_steps=1000,                   # you were using steps; keep it
    evaluation_strategy="steps",
    eval_steps=5,
    logging_steps=5,
    save_strategy="steps",
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    predict_with_generate=True,
    generation_max_length=225,       # you used 225
    num_beams=1,
    do_sample=False,
    fp16=not eight_bit,
    gradient_checkpointing=True,
    dataloader_num_workers=8,
    dataloader_prefetch_factor=2,
    report_to=["none"],
)

# =============== TRAINER (original flow) ===============
trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=vectorized_datasets["train"],
    eval_dataset=vectorized_datasets["test"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor,
)

# =============== QUICK SMOKE TEST (sanity before long training) ===============
if True:
    from torch.utils.data import DataLoader
    tmp_loader = DataLoader(vectorized_datasets["test"], batch_size=2, collate_fn=data_collator)
    batch = next(iter(tmp_loader))
    model.eval()
    with torch.no_grad():
        gen = model.generate(
            input_features=batch["input_features"].to(model.device),
            max_new_tokens=64,
            do_sample=False,
            num_beams=1,
        )
    pred = processor.tokenizer.batch_decode(gen, skip_special_tokens=True)
    gold = processor.tokenizer.batch_decode(
        batch["labels"].masked_fill(batch["labels"] == -100, processor.tokenizer.pad_token_id),
        skip_special_tokens=True
    )
    print("SMOKE TEST — PRED:", pred)
    print("SMOKE TEST — GOLD:", gold[:len(pred)])

# =============== TRAIN & SAVE ===============
trainer.train()

# Save ONLY the LoRA adapter (PEFT best practice)
adapter_dir = "/appdata/cortex/dev1/shob/ASR_models/asr_finetuning/whisper_peft_lora_adapter"
model.save_pretrained(adapter_dir)
processor.save_pretrained(adapter_dir)
print(f"Saved LoRA adapter + processor to: {adapter_dir}")

# (Optional) Merge adapter into base for export:
# from peft import PeftModel
# base = WhisperForConditionalGeneration.from_pretrained(whisper_loc)
# merged = get_peft_model(base, lora_cfg)  # or PeftModel.from_pretrained(base, adapter_dir)
# merged.load_adapter(adapter_dir)
# merged = merged.merge_and_unload()
# merged.save_pretrained("/path/to/merged_model")