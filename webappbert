#!/usr/bin/env python
import os
import sys
import argparse
import warnings
from pathlib import Path
from typing import Union, List, Optional

import pandas as pd
from sacrebleu import sentence_bleu, sentence_chrf
from sacrebleu.metrics import CHRF

# ─── Set up offline HuggingFace & point to your local cache ────────────────────
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_HUB_OFFLINE"]       = "1"
os.environ["TRANSFORMERS_CACHE"]   = r"C:/Users/ZKC7HOU/Documents/4. BERT score/roberta-large"
warnings.simplefilter("ignore")

# ─── We’ll load the tokenizer & model from your local cache only ──────────────
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel


class TranslationEvaluator:
    """
    Load data
    Compute sentence-level BLEU, ChrF, ChrF++, and BERTScore (P/R/F1)
    Compute macro-average model metrics
    Write Detailed + Model metrics to Excel
    """
    def __init__(
        self,
        output_file: Union[str, Path] = "Metrics_output.xlsx",
        baseline_tsv: str = r"C:/Users/ZKC7HOU/Documents/4. BERT score/roberta-large/en/roberta-large.tsv"
    ):
        self._data = None
        self._detailed_results = None
        self._model_metrics = None
        self._output_file = Path(output_file)
        self._chrf = CHRF(word_order=2)

        # load RoBERTa‐large purely from your cache, offline
        self.device = torch.device("cuda" if os.getenv("CUDA_VISIBLE_DEVICES") else "cpu")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                os.environ["TRANSFORMERS_CACHE"], local_files_only=True
            )
            self.model = AutoModel.from_pretrained(
                os.environ["TRANSFORMERS_CACHE"], local_files_only=True
            ).to(self.device)
            self.model.eval()
        except Exception as e:
            print("ERROR: loading local RoBERTa-large model/tokenizer failed:", e, file=sys.stderr)
            sys.exit(1)

        # store baseline for rescaling later
        self.baseline_tsv = baseline_tsv

    def load_data(self, file_path: Union[str, Path]) -> None:
        fp = Path(file_path)
        if fp.suffix.lower() == ".csv":
            self._data = pd.read_csv(fp)
        elif fp.suffix.lower() in (".xls", ".xlsx"):
            self._data = pd.read_excel(fp)
        else:
            raise ValueError(f"Unsupported file type: {fp.suffix}")

    def evaluate(
        self,
        prediction_cols: List[str] = ["candidate"],
        reference_col: str = "reference",
        metrics: List[str] = ["BLEU", "ChrF", "ChrF++", "BERTScore"],
        keep_cols: Optional[List[str]] = None
    ) -> None:
        if self._data is None:
            raise ValueError("No data loaded. Call load_data() first.")
        keep = keep_cols[:] if keep_cols else []
        if reference_col not in keep:
            keep.insert(0, reference_col)

        self._validate_columns(reference_col, prediction_cols, keep)
        self._compute_detailed_metrics(reference_col, prediction_cols, metrics, keep)
        self._compute_model_metrics(metrics, prediction_cols)

    def _validate_columns(
        self,
        reference_col: str,
        prediction_cols: List[str],
        keep_cols: List[str]
    ) -> None:
        missing = [
            c for c in [reference_col] + prediction_cols + keep_cols
            if c not in self._data.columns
        ]
        if missing:
            raise ValueError(f"Columns not found in data: {missing}")

    def _compute_detailed_metrics(
        self,
        reference_col: str,
        prediction_cols: List[str],
        metrics: List[str],
        keep_cols: List[str]
    ) -> None:
        df = self._data.copy().astype(str)

        # 1) BLEU, ChrF, ChrF++
        for p in prediction_cols:
            if "BLEU" in metrics:
                df[f"{p} BLEU"] = df.apply(
                    lambda r: sentence_bleu(r[p], references=[r[reference_col]]).score,
                    axis=1
                )
            if "ChrF" in metrics:
                df[f"{p} ChrF"] = df.apply(
                    lambda r: sentence_chrf(r[p], references=[r[reference_col]]).score,
                    axis=1
                )
            if "ChrF++" in metrics:
                df[f"{p} ChrF++"] = df.apply(
                    lambda r: self._chrf.sentence_score(r[p], references=[r[reference_col]]).score,
                    axis=1
                )

        # 2) Local offline BERTScore
        if "BERTScore" in metrics:
            def score_pair(cand: str, ref: str):
                # encode
                ec = self.tokenizer(cand, return_tensors="pt", truncation=True, max_length=512)
                er = self.tokenizer(ref,  return_tensors="pt", truncation=True, max_length=512)
                ec = {k: v.to(self.device) for k, v in ec.items()}
                er = {k: v.to(self.device) for k, v in er.items()}
                # embed
                with torch.no_grad():
                    oc = self.model(**ec).last_hidden_state.squeeze(0)  # [T1,d]
                    or_ = self.model(**er).last_hidden_state.squeeze(0)  # [T2,d]
                ec_norm = F.normalize(oc, dim=1)   # [T1,d]
                er_norm = F.normalize(or_, dim=1)  # [T2,d]
                sim = ec_norm @ er_norm.T          # [T1,T2]
                p = sim.max(dim=1).values.mean().item()
                r = sim.max(dim=0).values.mean().item()
                f = 2 * p * r / (p + r) if p + r > 0 else 0.0
                return p, r, f

            for p in prediction_cols:
                bert_p, bert_r, bert_f = zip(*df.apply(
                    lambda r: score_pair(r[p], r[reference_col]),
                    axis=1
                ))
                df[f"{p} BERT-P"]  = bert_p
                df[f"{p} BERT-R"]  = bert_r
                df[f"{p} BERT-F1"] = bert_f

        # 3) pick columns
        cols = keep[:]
        for p in prediction_cols:
            cols.append(p)
            if "BLEU" in metrics:    cols.append(f"{p} BLEU")
            if "ChrF" in metrics:    cols.append(f"{p} ChrF")
            if "ChrF++" in metrics:  cols.append(f"{p} ChrF++")
            if "BERTScore" in metrics:
                cols += [f"{p} BERT-P", f"{p} BERT-R", f"{p} BERT-F1"]

        self._detailed_results = df[cols]

    def _compute_model_metrics(
        self,
        metrics: List[str],
        prediction_cols: List[str]
    ) -> None:
        rows = []
        for p in prediction_cols:
            row = {"Model": p}
            if "BLEU" in metrics:
                row["BLEU"]    = self._detailed_results[f"{p} BLEU"].mean()
            if "ChrF" in metrics:
                row["ChrF"]    = self._detailed_results[f"{p} ChrF"].mean()
            if "ChrF++" in metrics:
                row["ChrF++"]  = self._detailed_results[f"{p} ChrF++"].mean()
            if "BERTScore" in metrics:
                row["BERT-P"]  = pd.Series(self._detailed_results[f"{p} BERT-P"]).mean()
                row["BERT-R"]  = pd.Series(self._detailed_results[f"{p} BERT-R"]).mean()
                row["BERT-F1"] = pd.Series(self._detailed_results[f"{p} BERT-F1"]).mean()
            rows.append(row)

        self._model_metrics = pd.DataFrame(rows)

    def generate_report(self) -> None:
        self._output_file.parent.mkdir(exist_ok=True, parents=True)
        with pd.ExcelWriter(self._output_file, engine="xlsxwriter") as writer:
            self._detailed_results.to_excel(writer, sheet_name="Detailed metrics", index=False)
            self._model_metrics .to_excel(writer, sheet_name="Model metrics",    index=False)
            ws = writer.sheets["Model metrics"]
            max_row, max_col = self._model_metrics.shape
            headers = [{"header": c} for c in self._model_metrics.columns]
            ws.add_table(0, 0, max_row, max_col-1, {
                "columns": headers,
                "style":   "Table Style Medium 9",
                "name":    "ModelMetrics"
            })
            # auto-fit columns
            for idx, col in enumerate(self._model_metrics.columns):
                w = max(self._model_metrics[col].astype(str).map(len).max(), len(col)) + 2
                ws.set_column(idx, idx, w)

        print(f"Report saved to: {self._output_file.resolve()}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run metrics with TranslationEvaluator")
    parser.add_argument(
        "--output_dir", type=Path, required=True,
        help="Directory in which to write the Excel report"
    )
    parser.add_argument(
        "files", nargs="+",
        help="One or more .csv/.xls/.xlsx files to process"
    )
    args = parser.parse_args()

    out_dir   = args.output_dir
    out_dir.mkdir(exist_ok=True, parents=True)
    output_fp = out_dir / "Metrics_output.xlsx"

    evaluator = TranslationEvaluator(output_file=output_fp)
    for f in args.files:
        print(f"▶ Loading data from {f}")
        evaluator.load_data(f)

    evaluator.evaluate(
        prediction_cols=["candidate"],
        reference_col="reference",
        metrics=["BLEU", "ChrF", "ChrF++", "BERTScore"]
    )
    evaluator.generate_report()
    print("✅ All done!")