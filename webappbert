import os
import sys
import argparse
import warnings
from pathlib import Path
from typing import Union, List, Optional

import pandas as pd
from sacrebleu import sentence_bleu, sentence_chrf
from sacrebleu.metrics import CHRF
import evaluate  # pip install evaluate

class TranslationEvaluator:
    """
    Load data
    Compute sentence-level BLEU, ChrF, ChrF++, and BERTScore (P/R/F1)
    via a local bertscore.py metric
    Write Detailed + Model metrics to Excel
    """
    def __init__(
        self,
        output_file: Union[str, Path] = "Metrics_output.xlsx",
        model_path: str = r"C:/Users/ZKC7HOU/Documents/4. BERT score/roberta-large",
        baseline_tsv: str = r"C:/Users/ZKC7HOU/Documents/4. BERT score/roberta-large/Versions-delete/roberta-large.tsv"
    ):
        # Point BERTScore metric at your local cache & baseline:
        os.environ["BERT"]     = model_path
        os.environ["BASELINE"] = baseline_tsv
        # Avoid HF Hub entirely
        os.environ["TRANSFORMERS_OFFLINE"] = "1"
        os.environ["HF_HUB_OFFLINE"]       = "1"

        warnings.simplefilter("ignore")
        self._data             = None
        self._detailed_results = None
        self._model_metrics    = None
        self._output_file      = Path(output_file)
        self._chrf             = CHRF(word_order=2)

        # Load your **local** bertscore metric by path:
        metric_path = Path(__file__).parent / "Automation_Scripts" / "bertscore.py"
        if not metric_path.exists():
            print(f"ERROR: could not find local bertscore.py at {metric_path}", file=sys.stderr)
            sys.exit(1)

        try:
            self._bertscorer = evaluate.load(str(metric_path))
        except Exception as e:
            print("ERROR: failed to load local bertscore metric:", e, file=sys.stderr)
            sys.exit(1)

    def load_data(self, file_path: Union[str, Path]) -> None:
        fp = Path(file_path)
        if fp.suffix == ".csv":
            self._data = pd.read_csv(fp)
        elif fp.suffix in (".xls", ".xlsx"):
            self._data = pd.read_excel(fp)
        else:
            raise ValueError(f"Unsupported file type: {fp.suffix}")

    def evaluate(
        self,
        prediction_cols: List[str] = ["candidate"],
        reference_col: str = "reference",
        metrics: List[str] = ["BLEU", "ChrF", "ChrF++", "BERTScore"],
        keep_cols: Optional[List[str]] = None
    ) -> None:
        if self._data is None:
            raise ValueError("No data loaded. Call load_data() first.")
        keep = keep_cols or []
        if reference_col not in keep:
            keep.insert(0, reference_col)

        self._validate_columns(reference_col, prediction_cols, keep)
        self._compute_detailed_metrics(reference_col, prediction_cols, metrics, keep)
        self._compute_model_metrics(metrics, prediction_cols)

    def _validate_columns(
        self,
        reference_col: str,
        prediction_cols: List[str],
        keep_cols: List[str]
    ) -> None:
        missing = [
            c for c in [reference_col] + prediction_cols + keep_cols
            if c not in self._data.columns
        ]
        if missing:
            raise ValueError(f"Columns not found in data: {missing}")

    def _compute_detailed_metrics(
        self,
        reference_col: str,
        prediction_cols: List[str],
        metrics: List[str],
        keep_cols: List[str]
    ) -> None:
        df = self._data.copy().astype(str)

        # BLEU / ChrF / ChrF++
        for p in prediction_cols:
            if "BLEU" in metrics:
                df[f"{p} BLEU"] = df.apply(
                    lambda r: sentence_bleu(r[p], [r[reference_col]]).score,
                    axis=1
                )
            if "ChrF" in metrics:
                df[f"{p} ChrF"] = df.apply(
                    lambda r: sentence_chrf(r[p], [r[reference_col]]).score,
                    axis=1
                )
            if "ChrF++" in metrics:
                df[f"{p} ChrF++"] = df.apply(
                    lambda r: self._chrf.sentence_score(r[p], [r[reference_col]]).score,
                    axis=1
                )

        # BERTScore via local metric
        if "BERTScore" in metrics:
            preds = df[prediction_cols[0]].tolist()
            refs  = [[r] for r in df[reference_col].tolist()]
            out   = self._bertscorer.compute(
                predictions=preds,
                references=refs,
                model_type=os.environ["BERT"],
                num_layers=24,
                device="cuda" if os.getenv("CUDA_VISIBLE_DEVICES") else "cpu",
                rescale_with_baseline=True,
                baseline_path=os.environ["BASELINE"],
                lang="en"
            )
            df[f"{prediction_cols[0]} BERT-P"]  = out["precision"]
            df[f"{prediction_cols[0]} BERT-R"]  = out["recall"]
            df[f"{prediction_cols[0]} BERT-F1"] = out["f1"]

        # collect columns
        cols = keep.copy()
        for p in prediction_cols:
            cols.append(p)
            if "BLEU" in metrics:    cols.append(f"{p} BLEU")
            if "ChrF" in metrics:    cols.append(f"{p} ChrF")
            if "ChrF++" in metrics:  cols.append(f"{p} ChrF++")
            if "BERTScore" in metrics:
                cols += [f"{p} BERT-P", f"{p} BERT-R", f"{p} BERT-F1"]

        self._detailed_results = df[cols]

    def _compute_model_metrics(
        self,
        metrics: List[str],
        prediction_cols: List[str]
    ) -> None:
        rows = []
        for p in prediction_cols:
            row = {"Model": p}
            if "BLEU" in metrics:
                row["BLEU"]   = self._detailed_results[f"{p} BLEU"].mean()
            if "ChrF" in metrics:
                row["ChrF"]   = self._detailed_results[f"{p} ChrF"].mean()
            if "ChrF++" in metrics:
                row["ChrF++"] = self._detailed_results[f"{p} ChrF++"].mean()
            if "BERTScore" in metrics:
                row["BERT-P"]  = pd.Series(self._detailed_results[f"{p} BERT-P"]).mean()
                row["BERT-R"]  = pd.Series(self._detailed_results[f"{p} BERT-R"]).mean()
                row["BERT-F1"] = pd.Series(self._detailed_results[f"{p} BERT-F1"]).mean()
            rows.append(row)
        self._model_metrics = pd.DataFrame(rows)

    def generate_report(self) -> None:
        self._output_file.parent.mkdir(parents=True, exist_ok=True)
        with pd.ExcelWriter(self._output_file, engine="xlsxwriter") as writer:
            self._detailed_results.to_excel(writer, sheet_name="Detailed metrics", index=False)
            self._model_metrics.to_excel(writer, sheet_name="Model metrics", index=False)
            wb = writer.book
            ws = writer.sheets["Model metrics"]
            max_row, max_col = self._model_metrics.shape
            headers = [{"header": c} for c in self._model_metrics.columns]
            ws.add_table(0, 0, max_row, max_col - 1, {
                "columns": headers,
                "style": "Table Style Medium 9",
                "name": "ModelMetrics"
            })
            for idx, col in enumerate(self._model_metrics.columns):
                width = max(self._model_metrics[col].astype(str).map(len).max(), len(col)) + 2
                ws.set_column(idx, idx, width)

        print(f"Report saved to: {self._output_file.resolve()}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run metrics with TranslationEvaluator")
    parser.add_argument(
        "--output_dir", type=Path, required=True,
        help="Directory to write Excel report"
    )
    parser.add_argument(
        "files", nargs="+", help="Input .csv/.xls/.xlsx files"
    )
    args = parser.parse_args()

    out_dir    = args.output_dir
    out_dir.mkdir(parents=True, exist_ok=True)
    output_fp  = out_dir / "Metrics_output.xlsx"

    evaluator = TranslationEvaluator(output_file=output_fp)
    for f in args.files:
        print(f"▶ Loading {f}")
        evaluator.load_data(f)

    evaluator.evaluate(
        prediction_cols=["candidate"],
        reference_col="reference",
        metrics=["BLEU", "ChrF", "ChrF++", "BERTScore"],
        keep_cols=None
    )
    evaluator.generate_report()
    print("✅ All done!")