#!/usr/bin/env python3
import os
import sys
import warnings
from pathlib import Path
import pandas as pd
import numpy as np
import evaluate

class BertScoreEvaluator:
    """
    Load data, compute sentence‚Äêlevel BERTScore (P/R/F1), write
    both detailed and macro‚Äêaverage results to Excel.
    """
    def __init__(
        self,
        output_file: Path,
        model_type: str = "roberta-large",
        num_layers: int = None,
        device: str = None,
    ):
        # allow user to override where HF cache lives
        # e.g. os.environ["TRANSFORMERS_CACHE"] = "/path/to/cache"
        warnings.simplefilter("ignore", UserWarning)

        self.scorer = evaluate.load("bertscore")
        self._data = None
        self._detailed = None
        self._summary = None

        self.output_file = Path(output_file)
        self.model_type = model_type
        self.num_layers = num_layers
        self.device = device or ("cuda" if os.getenv("CUDA_VISIBLE_DEVICES") else "cpu")

    def load_data(self, file_path: Path):
        file_path = Path(file_path)
        if file_path.suffix.lower() == ".csv":
            self._data = pd.read_csv(file_path)
        elif file_path.suffix.lower() in {".xls", ".xlsx"}:
            self._data = pd.read_excel(file_path)
        else:
            raise ValueError(f"Unsupported file type: {file_path.suffix!r}")

    def evaluate(self, prediction_col="candidate", reference_col="reference"):
        if self._data is None:
            raise ValueError("No data loaded. Call load_data() first.")

        df = self._data.copy()
        # ensure strings
        preds = df[prediction_col].astype(str).tolist()
        refs  = df[reference_col].astype(str).tolist()

        # compute batched BERTScore
        res = self.scorer.compute(
            predictions=preds,
            references=refs,
            model_type=self.model_type,
            num_layers=self.num_layers,
            device=self.device,
            lang="en",
        )

        # attach to df
        df["precision"] = res["precision"]
        df["recall"]    = res["recall"]
        df["f1"]        = res["f1"]

        # store detailed + summary
        self._detailed = df[[reference_col, prediction_col, "precision","recall","f1"]]
        mean_p = np.mean(res["precision"])
        mean_r = np.mean(res["recall"])
        mean_f = np.mean(res["f1"])
        self._summary = pd.DataFrame([{
            "Model":          self.model_type,
            "Precision_mean": mean_p,
            "Recall_mean":    mean_r,
            "F1_mean":        mean_f,
        }])

    def generate_report(self):
        # ensure parent exists
        self.output_file.parent.mkdir(parents=True, exist_ok=True)
        with pd.ExcelWriter(self.output_file, engine="xlsxwriter") as writer:
            # 1) detailed per‚Äêsentence
            self._detailed.to_excel(
                writer, sheet_name="Detailed metrics", index=False
            )
            # 2) macro‚Äêaverage summary
            self._summary.to_excel(
                writer, sheet_name="Model metrics", index=False
            )
        print(f"‚úÖ Report saved to: {self.output_file.resolve()}")



if __name__ == "__main__":
    import argparse

    p = argparse.ArgumentParser(
        description="Run BERTScore evaluation on one or more files"
    )
    p.add_argument(
        "--output_dir",
        type=Path,
        required=True,
        help="Directory in which to write the Excel report"
    )
    p.add_argument(
        "--model_type",
        type=str,
        default="roberta-large",
        help="HuggingFace model to use for BERTScore"
    )
    p.add_argument(
        "--num_layers",
        type=int,
        default=None,
        help="How many transformer layers to use (default: all)"
    )
    p.add_argument(
        "--device",
        type=str,
        default=None,
        help="Device for scoring (cuda or cpu)"
    )
    p.add_argument(
        "files",
        nargs="+",
        help="One or more input CSV/XLS/XLSX files"
    )
    args = p.parse_args()

    out_dir = args.output_dir
    out_dir.mkdir(parents=True, exist_ok=True)
    output_path = out_dir / "Bertscore_output.xlsx"

    evaluator = BertScoreEvaluator(
        output_file=output_path,
        model_type=args.model_type,
        num_layers=args.num_layers,
        device=args.device,
    )

    for f in args.files:
        print(f"üì• Loading data from {f}")
        evaluator.load_data(f)

    print("‚ñ∂Ô∏è Computing BERTScore...")
    evaluator.evaluate()
    evaluator.generate_report()