#!/usr/bin/env python3
"""
Compute BERTScore for one or more Excel files with 'reference' and 'candidate' columns,
and write out a two-sheet Excel report (Detailed_metrics + Model_metrics).
"""

import os
import sys
import argparse
from pathlib import Path

import pandas as pd
import evaluate

# ──────────────────────────────────────────────────────────────────────────────
# hard-code your local roberta-large checkout so evaluate.load("bertscore")
# will pick it up without touching server.py or your JSON config:
MODEL_CACHE = r"C:\Users\ZKC7HOU\Documents\4. BERT score\roberta-large"
# ──────────────────────────────────────────────────────────────────────────────

def compute_bertscore(file_paths, output_dir: Path):
    # point HF to your local model before loading
    os.environ["TRANSFORMERS_CACHE"] = MODEL_CACHE

    try:
        scorer = evaluate.load("bertscore")
        print(f"✅ Loaded BERTScore metric (cache={MODEL_CACHE})")
    except Exception as e:
        print(f"❌ Failed to load BERTScore metric: {e}", file=sys.stderr)
        sys.exit(1)

    all_dfs = []
    summary = []

    for fp in file_paths:
        df = pd.read_excel(fp)
        if not {"reference", "candidate"}.issubset(df.columns):
            print(f"❌ {fp} is missing a required column", file=sys.stderr)
            sys.exit(1)

        # compute per-sentence BERTScore
        out = scorer.compute(
            predictions=df["candidate"].astype(str).tolist(),
            references=df["reference"].astype(str).tolist(),
            model_type=MODEL_CACHE,
            device="cuda" if os.environ.get("CUDA_VISIBLE_DEVICES") else "cpu",
        )

        # attach back to a copy of the original
        df2 = df.copy()
        df2["precision"] = out["precision"]
        df2["recall"]    = out["recall"]
        df2["f1"]        = out["f1"]
        all_dfs.append(df2)

        # macro-average for this file
        summary.append({
            "file":      Path(fp).name,
            "precision": sum(out["precision"]) / len(out["precision"]),
            "recall":    sum(out["recall"])    / len(out["recall"]),
            "f1":        sum(out["f1"])        / len(out["f1"]),
        })

    # concat detailed + build summary DF
    detailed_df = pd.concat(all_dfs, ignore_index=True)
    summary_df  = pd.DataFrame(summary)

    # write to Excel
    report_path = output_dir / "Bertscore_output.xlsx"
    with pd.ExcelWriter(report_path, engine="xlsxwriter") as writer:
        detailed_df.to_excel(writer, sheet_name="Detailed_metrics", index=False)
        summary_df.to_excel(writer,  sheet_name="Model_metrics",    index=False)

    print(f"\n✅ All done! Report saved to:\n   {report_path.resolve()}")


def parse_args():
    p = argparse.ArgumentParser(description="Run BERTScore over Excel files")
    p.add_argument(
        "--output_dir",
        type=Path,
        required=True,
        help="Directory in which to write Bertscore_output.xlsx"
    )
    p.add_argument(
        "files",
        nargs="+",
        help="One or more input Excel files (.xls/.xlsx) to process"
    )
    return p.parse_args()


if __name__ == "__main__":
    args = parse_args()
    args.output_dir.mkdir(parents=True, exist_ok=True)
    compute_bertscore(args.files, args.output_dir)