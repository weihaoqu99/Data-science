#!/usr/bin/env python
import os
import sys
import argparse
from pathlib import Path

import pandas as pd
import torch
import torch.nn.functional as F
from transformers import RobertaTokenizer, RobertaModel

def main():
    parser = argparse.ArgumentParser(
        description="Compute BERTScore offline with local RoBERTa-large"
    )
    parser.add_argument(
        "--output_dir",
        type=Path,
        required=True,
        help="Directory to write Bertscore_output.xlsx"
    )
    parser.add_argument(
        "files",
        nargs="+",
        help="One or more .csv/.xls/.xlsx files with 'reference' & 'candidate' columns"
    )
    args = parser.parse_args()

    # ─── Setup local model cache (no Hub) ────────────────────────────────────
    MODEL_DIR = Path(r"C:/Users/ZKC7HOU/Documents/4. BERT score/roberta-large")
    if not MODEL_DIR.is_dir():
        print(f"ERROR: cannot find model folder at {MODEL_DIR}", file=sys.stderr)
        sys.exit(1)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    try:
        tokenizer = RobertaTokenizer.from_pretrained(
            str(MODEL_DIR),
            local_files_only=True,
            use_fast=False
        )
        model = RobertaModel.from_pretrained(
            str(MODEL_DIR),
            local_files_only=True
        ).to(device).eval()
    except Exception as e:
        print("ERROR: loading local RoBERTa-large model/tokenizer failed:", e, file=sys.stderr)
        sys.exit(1)
    # ────────────────────────────────────────────────────────────────────────

    # ─── Read & concatenate all inputs ──────────────────────────────────────
    dfs = []
    for fn in args.files:
        p = Path(fn)
        if p.suffix.lower() in (".xls", ".xlsx"):
            dfs.append(pd.read_excel(p))
        elif p.suffix.lower() == ".csv":
            dfs.append(pd.read_csv(p))
        else:
            print(f"ERROR: unsupported file type {p.suffix}", file=sys.stderr)
            sys.exit(1)
    if not dfs:
        print("ERROR: no data loaded", file=sys.stderr)
        sys.exit(1)
    df = pd.concat(dfs, ignore_index=True)
    # ────────────────────────────────────────────────────────────────────────

    # ─── Verify required columns ────────────────────────────────────────────
    for col in ("reference", "candidate"):
        if col not in df.columns:
            print(f"ERROR: missing required column '{col}'", file=sys.stderr)
            sys.exit(1)
    # ────────────────────────────────────────────────────────────────────────

    # ─── Compute BERTScore sentence by sentence ─────────────────────────────
    precisions, recalls, f1s = [], [], []
    for cand, ref in zip(df["candidate"].astype(str), df["reference"].astype(str)):
        # tokenize
        ec = tokenizer(cand, return_tensors="pt", truncation=True, max_length=512).to(device)
        er = tokenizer(ref,  return_tensors="pt", truncation=True, max_length=512).to(device)
        # embed
        with torch.no_grad():
            oc = model(**ec).last_hidden_state.squeeze(0)   # [T1,d]
            or_ = model(**er).last_hidden_state.squeeze(0)  # [T2,d]
        # normalize & similarity
        nc = F.normalize(oc,  dim=1)  # [T1,d]
        nr = F.normalize(or_, dim=1)  # [T2,d]
        sim = nc @ nr.T               # [T1,T2]
        p = sim.max(dim=1).values.mean().item()
        r = sim.max(dim=0).values.mean().item()
        f = 2 * p * r / (p + r) if (p + r) > 0 else 0.0

        precisions.append(round(p, 6))
        recalls.append(round(r, 6))
        f1s.append(round(f, 6))
    # ────────────────────────────────────────────────────────────────────────

    # ─── Attach scores & write out ─────────────────────────────────────────
    df["precision"] = precisions
    df["recall"]    = recalls
    df["f1"]        = f1s

    out_dir = args.output_dir
    out_dir.mkdir(parents=True, exist_ok=True)
    out_fp = out_dir / "Bertscore_output.xlsx"
    df.to_excel(out_fp, index=False)

    print(f"✔️  Report saved to: {out_fp.resolve()}")

if __name__ == "__main__":
    main()