#!/usr/bin/env python
import sys
import os
import argparse
import warnings
from pathlib import Path

import pandas as pd
from bert_score import BERTScorer


class BertScoreEvaluator:
    """
    Load data
    Compute sentence-level BERTScore (P, R, F1)
    Compute macro-average model metrics
    Write Detailed + Model metrics to Excel
    """
    def __init__(self, output_file: Path):
        # === optional: set your transformers cache/model path if needed ===
        # os.environ["TRANSFORMERS_CACHE"] = r"C:/Users/…/roberta-large"
        warnings.simplefilter("ignore")

        self._data = None
        self._detailed = None
        self._summary = None
        self._out = Path(output_file)
        # instantiate a scorer (English, rescale with baseline, using roberta-large)
        self._scorer = BERTScorer(
            lang="en",
            rescale_with_baseline=True,
            model_type="roberta-large"
        )

    def load_data(self, fp: Path):
        if fp.suffix in (".xls", ".xlsx"):
            self._data = pd.read_excel(fp)
        elif fp.suffix == ".csv":
            self._data = pd.read_csv(fp)
        else:
            raise ValueError(f"Unsupported file type {fp.suffix}")

    def evaluate(
        self,
        prediction_cols: list = ["candidate"],
        reference_col: str = "reference",
        keep_cols: list = None
    ):
        if self._data is None:
            raise ValueError("No data loaded – call load_data() first")

        keep = keep_cols or []
        if reference_col not in keep:
            keep = [reference_col] + keep

        # validate columns
        missing = [c for c in [reference_col] + prediction_cols + keep if c not in self._data.columns]
        if missing:
            raise ValueError(f"Columns not found in data: {missing}")

        df = self._data.copy()
        # for each prediction column, compute P/R/F1 lists
        for p in prediction_cols:
            preds = df[p].astype(str).tolist()
            refs  = [[r] for r in df[reference_col].astype(str).tolist()]
            P, R, F1 = self._scorer.score(preds, refs)
            df[f"{p} BERT-P"]  = P.tolist()
            df[f"{p} BERT-R"]  = R.tolist()
            df[f"{p} BERT-F1"] = F1.tolist()

        # assemble detailed DataFrame
        cols = keep.copy()
        for p in prediction_cols:
            cols.append(p)
            cols += [f"{p} BERT-P", f"{p} BERT-R", f"{p} BERT-F1"]
        self._detailed = df[cols]

        # macro-average summary
        rows = []
        for p in prediction_cols:
            rows.append({
                "Model": p,
                "BERT-P": self._detailed[f"{p} BERT-P"].mean(),
                "BERT-R": self._detailed[f"{p} BERT-R"].mean(),
                "BERT-F1": self._detailed[f"{p} BERT-F1"].mean()
            })
        self._summary = pd.DataFrame(rows)

    def generate_report(self):
        # ensure output dir exists
        self._out.parent.mkdir(parents=True, exist_ok=True)
        with pd.ExcelWriter(self._out, engine="xlsxwriter") as writer:
            # detailed per-sentence (optional; you only read “Model metrics” for the webapp)
            self._detailed.to_excel(writer, sheet_name="Detailed metrics", index=False)
            # macro-avg summary
            self._summary.to_excel(writer, sheet_name="Model metrics", index=False)

            # format the summary sheet as a nice table
            wb = writer.book
            ws = writer.sheets["Model metrics"]
            max_row, max_col = self._summary.shape
            headers = [{"header": c} for c in self._summary.columns]
            ws.add_table(0, 0, max_row, max_col - 1, {
                "columns": headers,
                "style": "Table Style Medium 9",
                "name": "BertScoreSummary"
            })
            # auto-fit columns
            for idx, col in enumerate(self._summary.columns):
                width = max(self._summary[col].astype(str).map(len).max(), len(col)) + 2
                ws.set_column(idx, idx, width)

        print(f"✔️ Report saved to {self._out.resolve()}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Compute BERTScore metrics")
    parser.add_argument(
        "--output_dir",
        type=Path,
        required=True,
        help="Directory in which to write Bertscore_output.xlsx"
    )
    parser.add_argument(
        "files",
        nargs="+",
        help="One or more input .csv/.xls/.xlsx files"
    )
    args = parser.parse_args()

    out_dir = args.output_dir
    out_dir.mkdir(parents=True, exist_ok=True)
    output_path = out_dir / "Bertscore_output.xlsx"

    evaluator = BertScoreEvaluator(output_file=output_path)
    for f in args.files:
        print(f"▶ Loading {f}")
        evaluator.load_data(Path(f))

    evaluator.evaluate(
        prediction_cols=["candidate"],
        reference_col="reference",
        keep_cols=None
    )
    evaluator.generate_report()
    print("✅ All done!")
    
    