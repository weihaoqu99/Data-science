#!/usr/bin/env python
import os
import sys
import argparse
from pathlib import Path

import pandas as pd
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel

# ─── 0) PATCH OUT HF VALIDATION & FORCE OFFLINE ────────────────────────────────
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_HUB_OFFLINE"]       = "1"

# Monkey‐patch the hub validator so Windows paths aren’t treated as repo IDs
try:
    import huggingface_hub.utils.validators as _validators
    _validators.validate_repo_id = lambda *args, **kwargs: None
except ImportError:
    pass
# ────────────────────────────────────────────────────────────────────────────────

# ─── 1) Your exact local model + baseline paths ───────────────────────────────
MODEL_PATH    = Path(r"C:/Users/ZKC7HOU/Documents/4. BERT score/roberta-large")
BASELINE_PATH = Path(r"C:/Users/ZKC7HOU/Documents/4. BERT score/roberta-large/Versions-delete/roberta-large.tsv")
DEVICE        = torch.device("cuda" if os.environ.get("CUDA_VISIBLE_DEVICES") else "cpu")
# ────────────────────────────────────────────────────────────────────────────────

def main():
    # ─── 2) CLI parsing ─────────────────────────────────────────────────────────
    parser = argparse.ArgumentParser(description="Compute BERTScore metrics offline")
    parser.add_argument(
        "--output_dir", type=Path, required=True,
        help="Directory to write Bertscore_output.xlsx"
    )
    parser.add_argument(
        "files", nargs="+",
        help="One or more .xls/.xlsx/.csv files with 'reference' & 'candidate' columns"
    )
    args = parser.parse_args()
    # ────────────────────────────────────────────────────────────────────────────────

    # ─── 3) Load tokenizer & model from local disk ───────────────────────────────
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)
        model     = AutoModel.from_pretrained(MODEL_PATH,   local_files_only=True).to(DEVICE)
        model.eval()
    except Exception as e:
        print(f"ERROR: loading model/tokenizer from {MODEL_PATH!r} failed:", e, file=sys.stderr)
        sys.exit(1)
    # ────────────────────────────────────────────────────────────────────────────────

    # ─── 4) Read + concat input files ──────────────────────────────────────────────
    dfs = []
    for fn in args.files:
        p = Path(fn)
        if not p.exists():
            print(f"ERROR: file not found: {p}", file=sys.stderr); sys.exit(1)
        if p.suffix.lower() in (".xls", ".xlsx"):
            dfs.append(pd.read_excel(p))
        elif p.suffix.lower() == ".csv":
            dfs.append(pd.read_csv(p))
        else:
            print(f"ERROR: unsupported file type {p.suffix}", file=sys.stderr); sys.exit(1)
    if not dfs:
        print("ERROR: no data loaded", file=sys.stderr); sys.exit(1)
    df = pd.concat(dfs, ignore_index=True)
    # ────────────────────────────────────────────────────────────────────────────────

    # ─── 5) Validate columns ──────────────────────────────────────────────────────
    for col in ("reference", "candidate"):
        if col not in df.columns:
            print(f"ERROR: missing required column '{col}'", file=sys.stderr)
            sys.exit(1)
    # ────────────────────────────────────────────────────────────────────────────────

    # ─── 6) Compute sentence-level BERTScore ──────────────────────────────────────
    precisions, recalls, f1s = [], [], []
    for _, row in df.iterrows():
        cand, ref = str(row["candidate"]), str(row["reference"])
        ec = tokenizer(cand, return_tensors="pt", truncation=True, max_length=512)
        er = tokenizer(ref,  return_tensors="pt", truncation=True, max_length=512)
        with torch.no_grad():
            oc = model(**{k: v.to(DEVICE) for k, v in ec.items()})
            or_ = model(**{k: v.to(DEVICE) for k, v in er.items()})
        emb_c = F.normalize(oc.last_hidden_state.squeeze(0), dim=1)
        emb_r = F.normalize(or_.last_hidden_state.squeeze(0), dim=1)
        sim   = emb_c @ emb_r.T
        p     = sim.max(dim=1).values.mean().item()
        r     = sim.max(dim=0).values.mean().item()
        f     = 2 * p * r / (p + r) if (p + r) > 0 else 0.0

        precisions.append(p)
        recalls.append(r)
        f1s.append(f)
    # ────────────────────────────────────────────────────────────────────────────────

    # ─── 7) Attach scores & write Excel ───────────────────────────────────────────
    df["precision"] = [round(min(max(x, 0.0), 1.0), 6) for x in precisions]
    df["recall"]    = [round(min(max(x, 0.0), 1.0), 6) for x in recalls]
    df["f1"]        = [round(min(max(x, 0.0), 1.0), 6) for x in f1s]

    out_dir = args.output_dir
    out_dir.mkdir(parents=True, exist_ok=True)
    output_fp = out_dir / "Bertscore_output.xlsx"
    df.to_excel(output_fp, index=False)
    # ────────────────────────────────────────────────────────────────────────────────

    # ─── 8) Success line ──────────────────────────────────────────────────────────
    print(f"✔️ Report saved to: {output_fp.resolve()}")

if __name__ == "__main__":
    main()